{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-14 16:59:12.966571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7234, 2)\n",
      "                                                 review  label\n",
      "0     @dianalaa32 Es una escena de uno de los docume...      2\n",
      "1     Qué feo es tener que terminar con alguien; y m...      0\n",
      "2     Oído en McDonalds \"el mejor mannequin challeng...      0\n",
      "3     Tengo que aceptar que me esta hundiendo el con...      1\n",
      "4     Mmm no quiero hacer spoiler pero hoy va a ver ...      1\n",
      "...                                                 ...    ...\n",
      "7229  @sebatramp Acá también, Seba ???? Para peor el...      0\n",
      "7230  @Phoyu_Agustina no soy hack pero es imposible ...      1\n",
      "7231  Nadie te vende un The Last of Us Remastered po...      0\n",
      "7232  Me propuse dejar las redes, las salidas &amp; ...      1\n",
      "7233  @irenichus siii! Voy como en media hora. Me va...      2\n",
      "\n",
      "[7234 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, AutoModelForMaskedLM, AdamW\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras \n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "MAX_LEN = 38\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "df = pd.read_csv('/Users/nfanlo/Desktop/Python/DataAnalysis/Datasets TFG/TFG/Task1-train-dev/train_dev/train_dev_all.csv')\n",
    "print(df.shape)\n",
    "df.isnull().sum()\n",
    "df.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df['review'] = df['text']\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "df['label'] = df['sentiment']\n",
    "df.drop('sentiment', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Environment stopwords\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['review']=df['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 5787\n",
      "Validation set length : 1447\n"
     ]
    }
   ],
   "source": [
    "review = df['review']\n",
    "label = df['label']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(review, label, stratify=label, test_size=0.2)\n",
    "\n",
    "# Report datasets lenghts\n",
    "print('Training set length : {}'.format(len(X_train)))\n",
    "print('Validation set length : {}'.format(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 38\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bertin-project/bertin-roberta-base-spanish\",\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=MAX_LEN,\n",
    "                   truncation=True ,pad_to_max_length=True,\n",
    "                   return_token_type_ids = False,\n",
    "                   return_attention_mask = True,)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_val_inputs, X_val_masks = preprocessing(X_val)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "batch_size = 16\n",
    "\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "y_val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks, y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bertin-project/bertin-roberta-base-spanish were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at bertin-project/bertin-roberta-base-spanish and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el modelo + optimizador + definimos EPOCHS + Scheduler\n",
    "#Modelo\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"bertin-project/bertin-roberta-base-spanish\", num_labels=3,\n",
    " output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 4e-5, eps = 1e-6)\n",
    "\n",
    "epochs=5\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50262, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion para formatear el tiempo y otra para calcular la exactitud\n",
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, label):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = label.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Training--------------------\n",
      "\n",
      "======= Epoch 1 / 5 =======\n",
      "batch loss: 1.0931745767593384 | avg loss: 1.0931745767593384\n",
      "batch loss: 1.1036325693130493 | avg loss: 1.0984035730361938\n",
      "batch loss: 1.1152548789978027 | avg loss: 1.1040206750233967\n",
      "batch loss: 1.0797587633132935 | avg loss: 1.097955197095871\n",
      "batch loss: 1.0668110847473145 | avg loss: 1.0917263746261596\n",
      "batch loss: 1.070284366607666 | avg loss: 1.0881527066230774\n",
      "batch loss: 1.0704345703125 | avg loss: 1.0856215442929948\n",
      "batch loss: 1.112610101699829 | avg loss: 1.0889951139688492\n",
      "batch loss: 1.1122556924819946 | avg loss: 1.091579622692532\n",
      "batch loss: 1.083343744277954 | avg loss: 1.0907560348510743\n",
      "batch loss: 1.0776851177215576 | avg loss: 1.0895677696574817\n",
      "batch loss: 1.1090052127838135 | avg loss: 1.091187556584676\n",
      "batch loss: 1.0300532579421997 | avg loss: 1.0864849182275624\n",
      "batch loss: 1.0893163681030273 | avg loss: 1.0866871646472387\n",
      "batch loss: 1.1571669578552246 | avg loss: 1.091385817527771\n",
      "batch loss: 1.112537145614624 | avg loss: 1.0927077755331993\n",
      "batch loss: 1.0819158554077148 | avg loss: 1.0920729567022884\n",
      "batch loss: 1.1063462495803833 | avg loss: 1.0928659174177382\n",
      "batch loss: 1.0955339670181274 | avg loss: 1.0930063410809165\n",
      "batch loss: 1.0451101064682007 | avg loss: 1.0906115293502807\n",
      "batch loss: 1.1365203857421875 | avg loss: 1.092797665368943\n",
      "batch loss: 1.1281681060791016 | avg loss: 1.0944054126739502\n",
      "batch loss: 1.1151131391525269 | avg loss: 1.0953057486078013\n",
      "batch loss: 1.107433557510376 | avg loss: 1.095811073978742\n",
      "batch loss: 1.087689757347107 | avg loss: 1.0954862213134766\n",
      "batch loss: 1.052829384803772 | avg loss: 1.093845573755411\n",
      "batch loss: 1.0764801502227783 | avg loss: 1.093202409920869\n",
      "batch loss: 1.0677034854888916 | avg loss: 1.0922917340482985\n",
      "batch loss: 1.1163147687911987 | avg loss: 1.0931201145566742\n",
      "batch loss: 1.0740721225738525 | avg loss: 1.0924851814905803\n",
      "batch loss: 1.0349762439727783 | avg loss: 1.090630054473877\n",
      "batch loss: 1.0612062215805054 | avg loss: 1.089710559695959\n",
      "batch loss: 1.1287165880203247 | avg loss: 1.0908925605542732\n",
      "batch loss: 1.1325303316116333 | avg loss: 1.0921172008794897\n",
      "batch loss: 1.0605738162994385 | avg loss: 1.0912159613200596\n",
      "batch loss: 1.0259498357772827 | avg loss: 1.089403013388316\n",
      "batch loss: 1.1060987710952759 | avg loss: 1.0898542500830986\n",
      "batch loss: 1.0814765691757202 | avg loss: 1.0896337847960622\n",
      "batch loss: 1.052878499031067 | avg loss: 1.0886913415713189\n",
      "batch loss: 1.1438686847686768 | avg loss: 1.0900707751512528\n",
      "batch loss: 1.058875322341919 | avg loss: 1.0893099104485862\n",
      "batch loss: 1.1061023473739624 | avg loss: 1.0897097303753807\n",
      "batch loss: 1.0364112854003906 | avg loss: 1.088470231655032\n",
      "batch loss: 1.1251194477081299 | avg loss: 1.0893031683835117\n",
      "batch loss: 1.0737179517745972 | avg loss: 1.0889568302366468\n",
      "batch loss: 1.1044327020645142 | avg loss: 1.0892932622329048\n",
      "batch loss: 1.0235856771469116 | avg loss: 1.0878952285076708\n",
      "batch loss: 1.0942943096160889 | avg loss: 1.0880285426974297\n",
      "batch loss: 1.089512586593628 | avg loss: 1.0880588293075562\n",
      "batch loss: 1.0895341634750366 | avg loss: 1.0880883359909057\n",
      "batch loss: 1.0683163404464722 | avg loss: 1.08770064980376\n",
      "batch loss: 1.1578692197799683 | avg loss: 1.0890500453802257\n",
      "batch loss: 1.0943326950073242 | avg loss: 1.0891497180146992\n",
      "batch loss: 1.1198246479034424 | avg loss: 1.089717772271898\n",
      "batch loss: 1.1290034055709839 | avg loss: 1.0904320565136996\n",
      "batch loss: 1.088681697845459 | avg loss: 1.0904008001089096\n",
      "batch loss: 1.0664939880371094 | avg loss: 1.089981382353264\n",
      "batch loss: 1.0448837280273438 | avg loss: 1.089203836589024\n",
      "batch loss: 1.0792688131332397 | avg loss: 1.08903544636096\n",
      "batch loss: 1.1588279008865356 | avg loss: 1.0901986539363862\n",
      "batch loss: 1.079704999923706 | avg loss: 1.090026626821424\n",
      "batch loss: 1.102335810661316 | avg loss: 1.0902251620446481\n",
      "batch loss: 1.1131361722946167 | avg loss: 1.0905888288740129\n",
      "batch loss: 1.1289293766021729 | avg loss: 1.0911878999322653\n",
      "batch loss: 1.1194987297058105 | avg loss: 1.0916234511595506\n",
      "batch loss: 1.088645100593567 | avg loss: 1.0915783246358235\n",
      "batch loss: 1.052567481994629 | avg loss: 1.0909960732531192\n",
      "batch loss: 1.0672791004180908 | avg loss: 1.0906472942408394\n",
      "batch loss: 1.0641664266586304 | avg loss: 1.0902635135512422\n",
      "batch loss: 1.071666955947876 | avg loss: 1.0899978484426225\n",
      "batch loss: 1.0974183082580566 | avg loss: 1.0901023619611498\n",
      "batch loss: 1.1061404943466187 | avg loss: 1.0903251137998369\n",
      "batch loss: 1.0609164237976074 | avg loss: 1.089922255032683\n",
      "batch loss: 1.1110743284225464 | avg loss: 1.0902080938622758\n",
      "batch loss: 1.1411454677581787 | avg loss: 1.0908872588475544\n",
      "batch loss: 1.0305150747299194 | avg loss: 1.0900928880039014\n",
      "batch loss: 1.112773060798645 | avg loss: 1.0903874357025345\n",
      "batch loss: 1.12266206741333 | avg loss: 1.09080121303216\n",
      "batch loss: 1.1431200504302979 | avg loss: 1.0914634767966935\n",
      "batch loss: 1.1149697303771973 | avg loss: 1.0917573049664497\n",
      "batch loss: 1.0978288650512695 | avg loss: 1.0918322624983612\n",
      "batch loss: 1.0813311338424683 | avg loss: 1.091704199953777\n",
      "batch loss: 1.0749449729919434 | avg loss: 1.0915022815566464\n",
      "batch loss: 1.0328161716461182 | avg loss: 1.090803637391045\n",
      "batch loss: 1.0847585201263428 | avg loss: 1.0907325183644014\n",
      "batch loss: 1.1277003288269043 | avg loss: 1.0911623766255933\n",
      "batch loss: 1.111487865447998 | avg loss: 1.0913960029338967\n",
      "batch loss: 1.1477928161621094 | avg loss: 1.0920368758114902\n",
      "batch loss: 1.106619954109192 | avg loss: 1.092200730623824\n",
      "batch loss: 1.1058030128479004 | avg loss: 1.0923518670929804\n",
      "batch loss: 1.1090826988220215 | avg loss: 1.092535722386706\n",
      "batch loss: 1.1041728258132935 | avg loss: 1.0926622126413428\n",
      "batch loss: 1.1324657201766968 | avg loss: 1.093090207346024\n",
      "batch loss: 1.0679917335510254 | avg loss: 1.0928232023056517\n",
      "batch loss: 1.0617207288742065 | avg loss: 1.0924958078484786\n",
      "batch loss: 1.1094310283660889 | avg loss: 1.0926722163955371\n",
      "batch loss: 1.1226845979690552 | avg loss: 1.0929816223911404\n",
      "batch loss: 1.074652910232544 | avg loss: 1.0927945947160527\n",
      "batch loss: 1.0841532945632935 | avg loss: 1.0927073088559238\n",
      "batch loss: 1.093172550201416 | avg loss: 1.0927119612693788\n",
      "batch loss: 1.0780311822891235 | avg loss: 1.0925666070220494\n",
      "batch loss: 1.131009817123413 | avg loss: 1.0929435012387294\n",
      "batch loss: 1.0751121044158936 | avg loss: 1.0927703808812261\n",
      "batch loss: 1.0731360912322998 | avg loss: 1.0925815896346018\n",
      "batch loss: 1.072922945022583 | avg loss: 1.0923943644478207\n",
      "batch loss: 1.0836817026138306 | avg loss: 1.0923121695248585\n",
      "batch loss: 1.1277682781219482 | avg loss: 1.092643535025766\n",
      "batch loss: 1.0662083625793457 | avg loss: 1.0923987649105213\n",
      "batch loss: 1.1411876678466797 | avg loss: 1.0928463695246144\n",
      "batch loss: 1.0787935256958008 | avg loss: 1.092718616398898\n",
      "batch loss: 1.067889928817749 | avg loss: 1.0924949345287975\n",
      "batch loss: 1.026946783065796 | avg loss: 1.0919096831764494\n",
      "batch loss: 1.1287169456481934 | avg loss: 1.0922354111629249\n",
      "batch loss: 1.058279275894165 | avg loss: 1.0919375503272342\n",
      "batch loss: 1.150134801864624 | avg loss: 1.092443613384081\n",
      "batch loss: 1.1493452787399292 | avg loss: 1.0929341449819763\n",
      "batch loss: 1.0773485898971558 | avg loss: 1.0928009351094563\n",
      "batch loss: 1.1148977279663086 | avg loss: 1.0929881960658703\n",
      "batch loss: 1.0588209629058838 | avg loss: 1.092701076459484\n",
      "batch loss: 1.1525999307632446 | avg loss: 1.093200233578682\n",
      "batch loss: 1.0955045223236084 | avg loss: 1.0932192772873177\n",
      "batch loss: 1.093977689743042 | avg loss: 1.0932254937828565\n",
      "batch loss: 1.0860100984573364 | avg loss: 1.0931668320322425\n",
      "batch loss: 1.0955009460449219 | avg loss: 1.0931856555323447\n",
      "batch loss: 1.0611780881881714 | avg loss: 1.0929295949935913\n",
      "batch loss: 1.0969603061676025 | avg loss: 1.0929615847648135\n",
      "batch loss: 1.0815805196762085 | avg loss: 1.0928719700790765\n",
      "batch loss: 1.0572192668914795 | avg loss: 1.0925934333354235\n",
      "batch loss: 1.0422148704528809 | avg loss: 1.09220290184021\n",
      "batch loss: 1.1075433492660522 | avg loss: 1.0923209052819471\n",
      "batch loss: 1.0975438356399536 | avg loss: 1.0923607749793365\n",
      "batch loss: 1.0727310180664062 | avg loss: 1.0922120646996931\n",
      "batch loss: 1.1412060260772705 | avg loss: 1.0925804403491486\n",
      "batch loss: 1.0547447204589844 | avg loss: 1.0922980842305654\n",
      "batch loss: 1.0783941745758057 | avg loss: 1.0921950923071968\n",
      "batch loss: 1.0929324626922607 | avg loss: 1.0922005141482634\n",
      "batch loss: 1.0360554456710815 | avg loss: 1.0917906961301818\n",
      "batch loss: 1.1058764457702637 | avg loss: 1.0918927667797476\n",
      "batch loss: 1.0836448669433594 | avg loss: 1.0918334293708527\n",
      "batch loss: 1.1640973091125488 | avg loss: 1.0923495999404362\n",
      "batch loss: 1.1161974668502808 | avg loss: 1.0925187337483075\n",
      "batch loss: 1.1833103895187378 | avg loss: 1.0931581116058458\n",
      "batch loss: 1.1359007358551025 | avg loss: 1.0934570110761201\n",
      "batch loss: 1.1376376152038574 | avg loss: 1.0937638208270073\n",
      "batch loss: 1.115082859992981 | avg loss: 1.0939108486833244\n",
      "batch loss: 1.1132302284240723 | avg loss: 1.0940431732020965\n",
      "batch loss: 1.095654010772705 | avg loss: 1.0940541312808083\n",
      "batch loss: 1.099027156829834 | avg loss: 1.0940877328047882\n",
      "batch loss: 1.1076045036315918 | avg loss: 1.0941784493875184\n",
      "batch loss: 1.1110796928405762 | avg loss: 1.094291124343872\n",
      "batch loss: 1.1000999212265015 | avg loss: 1.0943295931973993\n",
      "batch loss: 1.0954138040542603 | avg loss: 1.094336726163563\n",
      "batch loss: 1.103421688079834 | avg loss: 1.094396104999617\n",
      "batch loss: 1.1096725463867188 | avg loss: 1.0944953026709618\n",
      "batch loss: 1.1020426750183105 | avg loss: 1.0945439953957834\n",
      "batch loss: 1.1123156547546387 | avg loss: 1.0946579162891095\n",
      "batch loss: 1.0952643156051636 | avg loss: 1.0946617787051354\n",
      "batch loss: 1.0967952013015747 | avg loss: 1.0946752813797962\n",
      "batch loss: 1.0915722846984863 | avg loss: 1.094655765677398\n",
      "batch loss: 1.0880634784698486 | avg loss: 1.094614563882351\n",
      "batch loss: 1.1249371767044067 | avg loss: 1.0948029030924258\n",
      "batch loss: 1.1008048057556152 | avg loss: 1.0948399518742973\n",
      "batch loss: 1.0975509881973267 | avg loss: 1.0948565839989786\n",
      "batch loss: 1.1221463680267334 | avg loss: 1.0950229851210989\n",
      "batch loss: 1.0762650966644287 | avg loss: 1.0949093009486344\n",
      "batch loss: 1.12009859085083 | avg loss: 1.0950610436588886\n",
      "batch loss: 1.076192021369934 | avg loss: 1.0949480555014697\n",
      "batch loss: 1.0807485580444336 | avg loss: 1.094863534683273\n",
      "batch loss: 1.0675883293151855 | avg loss: 1.0947021429355328\n",
      "batch loss: 1.0519238710403442 | avg loss: 1.0944505060420318\n",
      "batch loss: 1.137885570526123 | avg loss: 1.0947045122670849\n",
      "batch loss: 1.075809121131897 | avg loss: 1.0945946553418802\n",
      "batch loss: 1.0922266244888306 | avg loss: 1.0945809673022673\n",
      "batch loss: 1.1719422340393066 | avg loss: 1.0950255722835147\n",
      "batch loss: 1.0335617065429688 | avg loss: 1.0946743501935687\n",
      "batch loss: 1.1566047668457031 | avg loss: 1.0950262275609104\n",
      "batch loss: 1.071807622909546 | avg loss: 1.094895049003558\n",
      "batch loss: 1.0572867393493652 | avg loss: 1.0946837663650513\n",
      "batch loss: 1.0206488370895386 | avg loss: 1.0942701634082048\n",
      "batch loss: 1.1205099821090698 | avg loss: 1.0944159401787652\n",
      "batch loss: 1.1132866144180298 | avg loss: 1.0945201980474903\n",
      "batch loss: 1.1391056776046753 | avg loss: 1.0947651732098924\n",
      "batch loss: 1.083283543586731 | avg loss: 1.0947024320644108\n",
      "batch loss: 1.1768659353256226 | avg loss: 1.0951489728430044\n",
      "batch loss: 1.1891008615493774 | avg loss: 1.0956568208900659\n",
      "batch loss: 1.0758017301559448 | avg loss: 1.09555007309042\n",
      "batch loss: 1.0304803848266602 | avg loss: 1.0952021068430202\n",
      "batch loss: 1.071373701095581 | avg loss: 1.095075360003938\n",
      "batch loss: 1.0570820569992065 | avg loss: 1.0948743372367173\n",
      "batch loss: 1.0218656063079834 | avg loss: 1.094490080758145\n",
      "batch loss: 1.032987356185913 | avg loss: 1.0941680769645732\n",
      "batch loss: 1.1339280605316162 | avg loss: 1.0943751602123182\n",
      "batch loss: 1.1262001991271973 | avg loss: 1.0945400567870065\n",
      "batch loss: 1.0734977722167969 | avg loss: 1.0944315914026241\n",
      "batch loss: 1.0042794942855835 | avg loss: 1.09396927295587\n",
      "batch loss: 1.1058515310287476 | avg loss: 1.094029896721548\n",
      "batch loss: 0.9994005560874939 | avg loss: 1.093549544738634\n",
      "batch loss: 1.0096689462661743 | avg loss: 1.0931259053524094\n",
      "batch loss: 1.1263115406036377 | avg loss: 1.0932926673385965\n",
      "batch loss: 1.0891045331954956 | avg loss: 1.093271726667881\n",
      "batch loss: 1.0467222929000854 | avg loss: 1.0930401374451557\n",
      "batch loss: 0.911853015422821 | avg loss: 1.09214317149455\n",
      "batch loss: 1.1734294891357422 | avg loss: 1.092543596704605\n",
      "batch loss: 1.0905216932296753 | avg loss: 1.0925336854130614\n",
      "batch loss: 1.0926285982131958 | avg loss: 1.0925341484023303\n",
      "batch loss: 1.099082112312317 | avg loss: 1.092565934634903\n",
      "batch loss: 1.0284887552261353 | avg loss: 1.0922563830435563\n",
      "batch loss: 1.0687549114227295 | avg loss: 1.0921433951992254\n",
      "batch loss: 1.1280430555343628 | avg loss: 1.092315163908963\n",
      "batch loss: 1.0696028470993042 | avg loss: 1.0922070100193932\n",
      "batch loss: 1.018441081047058 | avg loss: 1.0918574084602826\n",
      "batch loss: 1.1276284456253052 | avg loss: 1.0920261397676647\n",
      "batch loss: 0.9740716814994812 | avg loss: 1.0914723629682836\n",
      "batch loss: 1.0081217288970947 | avg loss: 1.0910828740240257\n",
      "batch loss: 0.9614356756210327 | avg loss: 1.0904798637988955\n",
      "batch loss: 1.0180370807647705 | avg loss: 1.0901444805441078\n",
      "batch loss: 1.0021394491195679 | avg loss: 1.0897389274039027\n",
      "batch loss: 0.9084692597389221 | avg loss: 1.0889074151669074\n",
      "batch loss: 1.096056342124939 | avg loss: 1.0889400586689988\n",
      "batch loss: 1.203607439994812 | avg loss: 1.0894612740386616\n",
      "batch loss: 1.325154185295105 | avg loss: 1.090527757800003\n",
      "batch loss: 1.0109621286392212 | avg loss: 1.0901693540650446\n",
      "batch loss: 1.004801869392395 | avg loss: 1.0897865402324318\n",
      "batch loss: 1.115485429763794 | avg loss: 1.0899012674178397\n",
      "batch loss: 0.9719793796539307 | avg loss: 1.0893771701388888\n",
      "batch loss: 1.118694543838501 | avg loss: 1.089506893031365\n",
      "batch loss: 1.0965702533721924 | avg loss: 1.0895380091562146\n",
      "batch loss: 1.0947628021240234 | avg loss: 1.0895609249148452\n",
      "batch loss: 1.134795069694519 | avg loss: 1.0897584539313503\n",
      "batch loss: 1.118326187133789 | avg loss: 1.0898826614670132\n",
      "batch loss: 1.0862371921539307 | avg loss: 1.0898668802145757\n",
      "batch loss: 1.091048240661621 | avg loss: 1.089871972285468\n",
      "batch loss: 1.0774309635162354 | avg loss: 1.0898185773980464\n",
      "batch loss: 1.1018580198287964 | avg loss: 1.0898700280067248\n",
      "batch loss: 1.0920361280441284 | avg loss: 1.0898792454536925\n",
      "batch loss: 1.1164124011993408 | avg loss: 1.0899916740797333\n",
      "batch loss: 1.120069146156311 | avg loss: 1.0901185832446134\n",
      "batch loss: 1.085632562637329 | avg loss: 1.0900997344185324\n",
      "batch loss: 1.1086068153381348 | avg loss: 1.0901771699035516\n",
      "batch loss: 1.0990569591522217 | avg loss: 1.0902141690254212\n",
      "batch loss: 1.0795146226882935 | avg loss: 1.0901697725675907\n",
      "batch loss: 1.1049954891204834 | avg loss: 1.0902310358591316\n",
      "batch loss: 1.0834745168685913 | avg loss: 1.0902032312542322\n",
      "batch loss: 1.0994956493377686 | avg loss: 1.0902413149349024\n",
      "batch loss: 1.096483588218689 | avg loss: 1.090266793601367\n",
      "batch loss: 1.1203083992004395 | avg loss: 1.0903889139493306\n",
      "batch loss: 1.108827829360962 | avg loss: 1.09046356542873\n",
      "batch loss: 1.1109559535980225 | avg loss: 1.0905461960261869\n",
      "batch loss: 1.0916670560836792 | avg loss: 1.0905506974722008\n",
      "batch loss: 1.1035157442092896 | avg loss: 1.0906025576591492\n",
      "batch loss: 1.1022604703903198 | avg loss: 1.090649003526604\n",
      "batch loss: 1.0846340656280518 | avg loss: 1.0906251347254192\n",
      "batch loss: 1.1013729572296143 | avg loss: 1.0906676162372937\n",
      "batch loss: 1.0961723327636719 | avg loss: 1.0906892883496022\n",
      "batch loss: 1.0920984745025635 | avg loss: 1.0906948145698099\n",
      "batch loss: 1.0932573080062866 | avg loss: 1.090704824309796\n",
      "batch loss: 1.0833497047424316 | avg loss: 1.0906762051675107\n",
      "batch loss: 1.0895909070968628 | avg loss: 1.0906719985858415\n",
      "batch loss: 1.0604888200759888 | avg loss: 1.0905554612170776\n",
      "batch loss: 1.1020021438598633 | avg loss: 1.0905994869195499\n",
      "batch loss: 1.1138079166412354 | avg loss: 1.090688408106223\n",
      "batch loss: 1.1116704940795898 | avg loss: 1.0907684924038312\n",
      "batch loss: 1.0748871564865112 | avg loss: 1.0907081070961608\n",
      "batch loss: 1.062799334526062 | avg loss: 1.0906023920485468\n",
      "batch loss: 1.0923181772232056 | avg loss: 1.0906088667095832\n",
      "batch loss: 1.0825228691101074 | avg loss: 1.0905784682223671\n",
      "batch loss: 1.0894070863723755 | avg loss: 1.090574081024427\n",
      "batch loss: 1.076772928237915 | avg loss: 1.0905225841856714\n",
      "batch loss: 1.0952768325805664 | avg loss: 1.090540257971526\n",
      "batch loss: 1.0954023599624634 | avg loss: 1.0905582657566777\n",
      "batch loss: 1.0625057220458984 | avg loss: 1.0904547508352358\n",
      "batch loss: 1.1018474102020264 | avg loss: 1.0904966356123196\n",
      "batch loss: 1.071528434753418 | avg loss: 1.0904271550230928\n",
      "batch loss: 1.1217913627624512 | avg loss: 1.0905416229345504\n",
      "batch loss: 1.1034743785858154 | avg loss: 1.0905886511369185\n",
      "batch loss: 1.062577486038208 | avg loss: 1.0904871614083\n",
      "batch loss: 1.1332669258117676 | avg loss: 1.090641600990984\n",
      "batch loss: 1.1487987041473389 | avg loss: 1.0908507992037766\n",
      "batch loss: 1.0927653312683105 | avg loss: 1.0908576613258718\n",
      "batch loss: 1.1281466484069824 | avg loss: 1.090990836279733\n",
      "batch loss: 1.1043665409088135 | avg loss: 1.0910384366520782\n",
      "batch loss: 1.1152827739715576 | avg loss: 1.0911244094794523\n",
      "batch loss: 1.0961638689041138 | avg loss: 1.0911422167565714\n",
      "batch loss: 1.045657753944397 | avg loss: 1.0909820601973734\n",
      "batch loss: 1.0892164707183838 | avg loss: 1.09097586514657\n",
      "batch loss: 1.0914355516433716 | avg loss: 1.0909774724420134\n",
      "batch loss: 1.1127172708511353 | avg loss: 1.091053220868526\n",
      "batch loss: 1.0460759401321411 | avg loss: 1.090897049754858\n",
      "batch loss: 1.1014461517333984 | avg loss: 1.0909335518378287\n",
      "batch loss: 1.0728764533996582 | avg loss: 1.0908712859811454\n",
      "batch loss: 1.1346435546875 | avg loss: 1.0910217061485212\n",
      "batch loss: 1.0909693241119385 | avg loss: 1.091021526757985\n",
      "batch loss: 1.0763217210769653 | avg loss: 1.0909713567727255\n",
      "batch loss: 1.122495174407959 | avg loss: 1.0910785806422332\n",
      "batch loss: 1.1091057062149048 | avg loss: 1.0911396895424794\n",
      "batch loss: 1.0802816152572632 | avg loss: 1.0911030068590835\n",
      "batch loss: 1.0950037240982056 | avg loss: 1.0911161405871612\n",
      "batch loss: 1.04032564163208 | avg loss: 1.0909457026712046\n",
      "batch loss: 1.109128475189209 | avg loss: 1.0910065146194254\n",
      "batch loss: 1.1105351448059082 | avg loss: 1.0910716100533804\n",
      "batch loss: 1.0548378229141235 | avg loss: 1.0909512320230172\n",
      "batch loss: 1.0820131301879883 | avg loss: 1.0909216356593252\n",
      "batch loss: 1.1350224018096924 | avg loss: 1.0910671827423297\n",
      "batch loss: 1.1083853244781494 | avg loss: 1.0911241503138291\n",
      "batch loss: 1.0948854684829712 | avg loss: 1.0911364825045475\n",
      "batch loss: 1.1350586414337158 | avg loss: 1.0912800189716365\n",
      "batch loss: 1.0935842990875244 | avg loss: 1.0912875247700595\n",
      "batch loss: 1.0958280563354492 | avg loss: 1.0913022667556613\n",
      "batch loss: 1.0594282150268555 | avg loss: 1.0911991144846944\n",
      "batch loss: 1.1208235025405884 | avg loss: 1.0912946770268102\n",
      "batch loss: 1.1064093112945557 | avg loss: 1.091343277136996\n",
      "batch loss: 1.093563199043274 | avg loss: 1.0913503922713108\n",
      "batch loss: 1.0961918830871582 | avg loss: 1.0913658602930867\n",
      "batch loss: 1.073103427886963 | avg loss: 1.0913076996803284\n",
      "batch loss: 1.062124252319336 | avg loss: 1.0912150538156904\n",
      "batch loss: 1.0948035717010498 | avg loss: 1.0912264098849478\n",
      "batch loss: 1.1208291053771973 | avg loss: 1.0913197937823997\n",
      "batch loss: 1.106856346130371 | avg loss: 1.091368650865255\n",
      "batch loss: 1.0949316024780273 | avg loss: 1.0913798199925677\n",
      "batch loss: 1.1054855585098267 | avg loss: 1.091423900425434\n",
      "batch loss: 1.0455673933029175 | avg loss: 1.0912810452630586\n",
      "batch loss: 1.0764938592910767 | avg loss: 1.0912351223252574\n",
      "batch loss: 1.1276861429214478 | avg loss: 1.0913479737822116\n",
      "batch loss: 1.1218581199645996 | avg loss: 1.0914421409000585\n",
      "batch loss: 1.0472453832626343 | avg loss: 1.0913061508765587\n",
      "batch loss: 1.1086938381195068 | avg loss: 1.0913594873404941\n",
      "batch loss: 1.1051983833312988 | avg loss: 1.0914018081233408\n",
      "batch loss: 1.0874797105789185 | avg loss: 1.091389850508876\n",
      "batch loss: 1.0577014684677124 | avg loss: 1.0912874542108786\n",
      "batch loss: 1.1138687133789062 | avg loss: 1.0913558822689635\n",
      "batch loss: 1.1190614700317383 | avg loss: 1.091439584951026\n",
      "batch loss: 1.0952450037002563 | avg loss: 1.0914510470556926\n",
      "batch loss: 1.0860291719436646 | avg loss: 1.0914347651484493\n",
      "batch loss: 1.0854437351226807 | avg loss: 1.0914168279328031\n",
      "batch loss: 1.0952963829040527 | avg loss: 1.0914284086939114\n",
      "batch loss: 1.0684410333633423 | avg loss: 1.09135999388638\n",
      "batch loss: 1.118729829788208 | avg loss: 1.0914412100166524\n",
      "batch loss: 1.0343832969665527 | avg loss: 1.0912723996230131\n",
      "batch loss: 1.015344500541687 | avg loss: 1.0910484235195284\n",
      "batch loss: 1.129029631614685 | avg loss: 1.0911601329551024\n",
      "batch loss: 1.0953782796859741 | avg loss: 1.0911725028868644\n",
      "batch loss: 1.0803852081298828 | avg loss: 1.0911409610893295\n",
      "batch loss: 1.0747942924499512 | avg loss: 1.091093303163267\n",
      "batch loss: 1.0546866655349731 | avg loss: 1.0909874699143476\n",
      "batch loss: 1.0853981971740723 | avg loss: 1.090971269123796\n",
      "batch loss: 1.1119669675827026 | avg loss: 1.0910319503332149\n",
      "batch loss: 1.0730246305465698 | avg loss: 1.0909800560398817\n",
      "batch loss: 1.1066508293151855 | avg loss: 1.0910250869975693\n",
      "batch loss: 1.089524745941162 | avg loss: 1.091020788026061\n",
      "batch loss: 1.1040865182876587 | avg loss: 1.0910581186839512\n",
      "batch loss: 1.050571084022522 | avg loss: 1.090942771006853\n",
      "batch loss: 1.0428714752197266 | avg loss: 1.0908062048256397\n",
      "batch loss: 1.034862995147705 | avg loss: 1.0906477254781102\n",
      "batch loss: 1.1345601081848145 | avg loss: 1.0907717717569427\n",
      "batch loss: 1.0709338188171387 | avg loss: 1.0907158901993657\n",
      "batch loss: 1.1024285554885864 | avg loss: 1.0907487909445601\n",
      "batch loss: 1.0165992975234985 | avg loss: 1.0905410892823164\n",
      "batch loss: 1.0536680221557617 | avg loss: 1.090438091888108\n",
      "batch loss: 1.0932860374450684 | avg loss: 1.0904460248840884\n",
      "batch loss: 1.0475802421569824 | avg loss: 1.090326953265402\n",
      "batch loss: 1.0437147617340088 | avg loss: 1.0901978336212708\n",
      "batch loss: 1.1164737939834595 | avg loss: 1.090270419147133\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 0:27:50\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.39\n",
      "  Validation took: 0:01:48\n",
      "\n",
      "======= Epoch 2 / 5 =======\n",
      "batch loss: 1.1119760274887085 | avg loss: 1.1119760274887085\n",
      "batch loss: 1.0915162563323975 | avg loss: 1.101746141910553\n",
      "batch loss: 1.0785776376724243 | avg loss: 1.09402330716451\n",
      "batch loss: 1.0559109449386597 | avg loss: 1.0844952166080475\n",
      "batch loss: 1.0370209217071533 | avg loss: 1.0750003576278686\n",
      "batch loss: 1.061493992805481 | avg loss: 1.0727492968241374\n",
      "batch loss: 1.0470114946365356 | avg loss: 1.0690724679401942\n",
      "batch loss: 1.1136282682418823 | avg loss: 1.0746419429779053\n",
      "batch loss: 1.1212637424468994 | avg loss: 1.0798221429189045\n",
      "batch loss: 1.0799791812896729 | avg loss: 1.0798378467559815\n",
      "batch loss: 1.0657672882080078 | avg loss: 1.078558705069802\n",
      "batch loss: 1.1225770711898804 | avg loss: 1.0822269022464752\n",
      "batch loss: 1.0015666484832764 | avg loss: 1.0760222673416138\n",
      "batch loss: 1.0941303968429565 | avg loss: 1.0773157051631383\n",
      "batch loss: 1.168358325958252 | avg loss: 1.083385213216146\n",
      "batch loss: 1.1173497438430786 | avg loss: 1.0855079963803291\n",
      "batch loss: 1.0801384449005127 | avg loss: 1.0851921404109282\n",
      "batch loss: 1.1158767938613892 | avg loss: 1.0868968433803983\n",
      "batch loss: 1.1011897325515747 | avg loss: 1.087649100705197\n",
      "batch loss: 1.0239359140396118 | avg loss: 1.0844634413719176\n",
      "batch loss: 1.1574254035949707 | avg loss: 1.0879378205253964\n",
      "batch loss: 1.1489025354385376 | avg loss: 1.0907089439305393\n",
      "batch loss: 1.1278365850448608 | avg loss: 1.0923231891963794\n",
      "batch loss: 1.1196351051330566 | avg loss: 1.0934611856937408\n",
      "batch loss: 1.0870168209075928 | avg loss: 1.093203411102295\n",
      "batch loss: 1.0297198295593262 | avg loss: 1.0907617348891039\n",
      "batch loss: 1.0687222480773926 | avg loss: 1.0899454575997811\n",
      "batch loss: 1.0556434392929077 | avg loss: 1.0887203855173928\n",
      "batch loss: 1.1352250576019287 | avg loss: 1.0903239948996182\n",
      "batch loss: 1.0682216882705688 | avg loss: 1.0895872513453166\n",
      "batch loss: 1.0081361532211304 | avg loss: 1.086959796567117\n",
      "batch loss: 1.0559674501419067 | avg loss: 1.0859912857413292\n",
      "batch loss: 1.1430845260620117 | avg loss: 1.087721383932865\n",
      "batch loss: 1.1487209796905518 | avg loss: 1.089515489690444\n",
      "batch loss: 1.0631465911865234 | avg loss: 1.088762092590332\n",
      "batch loss: 1.013565182685852 | avg loss: 1.0866732895374298\n",
      "batch loss: 1.0925241708755493 | avg loss: 1.086831421465487\n",
      "batch loss: 1.0680105686187744 | avg loss: 1.0863361358642578\n",
      "batch loss: 1.0427502393722534 | avg loss: 1.0852185487747192\n",
      "batch loss: 1.1353849172592163 | avg loss: 1.0864727079868317\n",
      "batch loss: 1.0631457567214966 | avg loss: 1.0859037579559698\n",
      "batch loss: 1.1042062044143677 | avg loss: 1.0863395304906935\n",
      "batch loss: 1.0390490293502808 | avg loss: 1.085239751394405\n",
      "batch loss: 1.1242324113845825 | avg loss: 1.0861259482123635\n",
      "batch loss: 1.0739730596542358 | avg loss: 1.0858558840221828\n",
      "batch loss: 1.1039329767227173 | avg loss: 1.0862488642982815\n",
      "batch loss: 1.0239101648330688 | avg loss: 1.084922508990511\n",
      "batch loss: 1.0946096181869507 | avg loss: 1.0851243237654369\n",
      "batch loss: 1.0899841785430908 | avg loss: 1.0852235044751848\n",
      "batch loss: 1.0884697437286377 | avg loss: 1.0852884292602538\n",
      "batch loss: 1.0689892768859863 | avg loss: 1.084968838037229\n",
      "batch loss: 1.1558960676193237 | avg loss: 1.0863328232215002\n",
      "batch loss: 1.094520092010498 | avg loss: 1.086487299991104\n",
      "batch loss: 1.1198776960372925 | avg loss: 1.087105640658626\n",
      "batch loss: 1.1293047666549683 | avg loss: 1.087872897494923\n",
      "batch loss: 1.0884202718734741 | avg loss: 1.087882672037397\n",
      "batch loss: 1.0654785633087158 | avg loss: 1.0874896174982975\n",
      "batch loss: 1.0418535470962524 | avg loss: 1.086702788698262\n",
      "batch loss: 1.0789066553115845 | avg loss: 1.0865706508442508\n",
      "batch loss: 1.1652389764785767 | avg loss: 1.0878817896048227\n",
      "batch loss: 1.0790562629699707 | avg loss: 1.087737108840317\n",
      "batch loss: 1.103778600692749 | avg loss: 1.0879958425798724\n",
      "batch loss: 1.1164271831512451 | avg loss: 1.088447133700053\n",
      "batch loss: 1.1341384649276733 | avg loss: 1.0891610607504845\n",
      "batch loss: 1.124240517616272 | avg loss: 1.0897007447022657\n",
      "batch loss: 1.0887423753738403 | avg loss: 1.0896862239548655\n",
      "batch loss: 1.0455682277679443 | avg loss: 1.0890277463998368\n",
      "batch loss: 1.0628986358642578 | avg loss: 1.0886434947743135\n",
      "batch loss: 1.0591059923171997 | avg loss: 1.0882154150285583\n",
      "batch loss: 1.069747805595398 | avg loss: 1.087951592036656\n",
      "batch loss: 1.0998256206512451 | avg loss: 1.088118831876298\n",
      "batch loss: 1.1056334972381592 | avg loss: 1.088362091117435\n",
      "batch loss: 1.0539085865020752 | avg loss: 1.0878901253007862\n",
      "batch loss: 1.1140767335891724 | avg loss: 1.0882439983857644\n",
      "batch loss: 1.1463907957077026 | avg loss: 1.0890192890167236\n",
      "batch loss: 1.022021770477295 | avg loss: 1.0881377427201522\n",
      "batch loss: 1.116581678390503 | avg loss: 1.0885071444821048\n",
      "batch loss: 1.125081181526184 | avg loss: 1.0889760423929264\n",
      "batch loss: 1.1502436399459839 | avg loss: 1.0897515816024588\n",
      "batch loss: 1.1192846298217773 | avg loss: 1.0901207447052002\n",
      "batch loss: 1.0993868112564087 | avg loss: 1.0902351405885484\n",
      "batch loss: 1.0801945924758911 | avg loss: 1.0901126948798574\n",
      "batch loss: 1.0714547634124756 | avg loss: 1.0898879005248288\n",
      "batch loss: 1.022244930267334 | avg loss: 1.0890826270693825\n",
      "batch loss: 1.0831577777862549 | avg loss: 1.0890129229601693\n",
      "batch loss: 1.134138822555542 | avg loss: 1.089537642722906\n",
      "batch loss: 1.1146408319473267 | avg loss: 1.0898261851277844\n",
      "batch loss: 1.1578624248504639 | avg loss: 1.0905993242155423\n",
      "batch loss: 1.1098341941833496 | avg loss: 1.090815446350012\n",
      "batch loss: 1.1092854738235474 | avg loss: 1.0910206688774957\n",
      "batch loss: 1.1165324449539185 | avg loss: 1.0913010180651486\n",
      "batch loss: 1.1080540418624878 | avg loss: 1.0914831161499023\n",
      "batch loss: 1.1422276496887207 | avg loss: 1.0920287562954811\n",
      "batch loss: 1.0617774724960327 | avg loss: 1.0917069341274017\n",
      "batch loss: 1.0561070442199707 | avg loss: 1.0913321984441657\n",
      "batch loss: 1.1123712062835693 | avg loss: 1.0915513547758262\n",
      "batch loss: 1.1287469863891602 | avg loss: 1.0919348148955512\n",
      "batch loss: 1.0726350545883179 | avg loss: 1.0917378785658856\n",
      "batch loss: 1.0988883972167969 | avg loss: 1.0918101060270058\n",
      "batch loss: 1.0793964862823486 | avg loss: 1.0916859698295593\n",
      "batch loss: 1.0884215831756592 | avg loss: 1.0916536491696198\n",
      "batch loss: 1.1204205751419067 | avg loss: 1.0919356778556226\n",
      "batch loss: 1.0796411037445068 | avg loss: 1.0918163130584273\n",
      "batch loss: 1.0651626586914062 | avg loss: 1.0915600279202828\n",
      "batch loss: 1.0718945264816284 | avg loss: 1.0913727374303908\n",
      "batch loss: 1.0671697854995728 | avg loss: 1.0911444076951944\n",
      "batch loss: 1.1289151906967163 | avg loss: 1.0914974056671713\n",
      "batch loss: 1.0573867559432983 | avg loss: 1.091181566317876\n",
      "batch loss: 1.1327033042907715 | avg loss: 1.0915624996937743\n",
      "batch loss: 1.0732054710388184 | avg loss: 1.091395617615093\n",
      "batch loss: 1.0702035427093506 | avg loss: 1.0912046980213475\n",
      "batch loss: 1.0286030769348145 | avg loss: 1.090645754975932\n",
      "batch loss: 1.1292853355407715 | avg loss: 1.0909876981667712\n",
      "batch loss: 1.056189775466919 | avg loss: 1.0906824532308077\n",
      "batch loss: 1.1429284811019897 | avg loss: 1.091136766516644\n",
      "batch loss: 1.1429920196533203 | avg loss: 1.0915837945609257\n",
      "batch loss: 1.078757882118225 | avg loss: 1.0914741713776548\n",
      "batch loss: 1.1135011911392212 | avg loss: 1.091660841036651\n",
      "batch loss: 1.06093430519104 | avg loss: 1.0914026348530745\n",
      "batch loss: 1.1502209901809692 | avg loss: 1.0918927878141402\n",
      "batch loss: 1.0959604978561401 | avg loss: 1.091926405252504\n",
      "batch loss: 1.0959386825561523 | avg loss: 1.0919592927713864\n",
      "batch loss: 1.0874152183532715 | avg loss: 1.09192234907693\n",
      "batch loss: 1.096472144126892 | avg loss: 1.0919590409724944\n",
      "batch loss: 1.061049461364746 | avg loss: 1.0917117643356322\n",
      "batch loss: 1.0965354442596436 | avg loss: 1.0917500475096324\n",
      "batch loss: 1.085587501525879 | avg loss: 1.0917015235255083\n",
      "batch loss: 1.0599497556686401 | avg loss: 1.0914534628391266\n",
      "batch loss: 1.0394012928009033 | avg loss: 1.0910499576450319\n",
      "batch loss: 1.1074715852737427 | avg loss: 1.0911762778575604\n",
      "batch loss: 1.0970975160598755 | avg loss: 1.0912214781491811\n",
      "batch loss: 1.0754657983779907 | avg loss: 1.0911021169387933\n",
      "batch loss: 1.13428795337677 | avg loss: 1.091426822475921\n",
      "batch loss: 1.0603251457214355 | avg loss: 1.091194720410589\n",
      "batch loss: 1.0828830003738403 | avg loss: 1.0911331521140204\n",
      "batch loss: 1.11226224899292 | avg loss: 1.091288513120483\n",
      "batch loss: 1.1164178848266602 | avg loss: 1.091471939191331\n",
      "batch loss: 1.123710036277771 | avg loss: 1.091705548590508\n",
      "batch loss: 1.0974187850952148 | avg loss: 1.0917466510114053\n",
      "batch loss: 1.0789027214050293 | avg loss: 1.091654908657074\n",
      "batch loss: 1.093213677406311 | avg loss: 1.0916659637545862\n",
      "batch loss: 1.0608861446380615 | avg loss: 1.0914492044650332\n",
      "batch loss: 1.0608502626419067 | avg loss: 1.0912352258508855\n",
      "batch loss: 1.0605645179748535 | avg loss: 1.0910222348239687\n",
      "batch loss: 1.074364423751831 | avg loss: 1.0909073533682987\n",
      "batch loss: 1.1429728269577026 | avg loss: 1.091263966201103\n",
      "batch loss: 1.0857205390930176 | avg loss: 1.0912262558126125\n",
      "batch loss: 1.0978128910064697 | avg loss: 1.091270760104463\n",
      "batch loss: 1.1243470907211304 | avg loss: 1.091492748900548\n",
      "batch loss: 1.1365456581115723 | avg loss: 1.0917931016286213\n",
      "batch loss: 1.0787594318389893 | avg loss: 1.0917067859346503\n",
      "batch loss: 1.10447359085083 | avg loss: 1.0917907780722569\n",
      "batch loss: 1.0603845119476318 | avg loss: 1.091585508359024\n",
      "batch loss: 1.0475316047668457 | avg loss: 1.091299444049984\n",
      "batch loss: 1.0911756753921509 | avg loss: 1.091298645542514\n",
      "batch loss: 1.1339324712753296 | avg loss: 1.0915719392972114\n",
      "batch loss: 1.0926998853683472 | avg loss: 1.0915791236670913\n",
      "batch loss: 1.0928733348846436 | avg loss: 1.091587314877329\n",
      "batch loss: 1.0812735557556152 | avg loss: 1.0915224484677584\n",
      "batch loss: 1.081019639968872 | avg loss: 1.0914568059146403\n",
      "batch loss: 1.1350151300430298 | avg loss: 1.0917273545117112\n",
      "batch loss: 1.1041653156280518 | avg loss: 1.0918041320494662\n",
      "batch loss: 1.097702980041504 | avg loss: 1.0918403213009513\n",
      "batch loss: 1.127892017364502 | avg loss: 1.092060148715973\n",
      "batch loss: 1.085278868675232 | avg loss: 1.0920190500490594\n",
      "batch loss: 1.1324917078018188 | avg loss: 1.092262861240341\n",
      "batch loss: 1.073943018913269 | avg loss: 1.0921531615856879\n",
      "batch loss: 1.0953960418701172 | avg loss: 1.0921724644445239\n",
      "batch loss: 1.0575517416000366 | avg loss: 1.0919676080963316\n",
      "batch loss: 1.0452063083648682 | avg loss: 1.0916925416273229\n",
      "batch loss: 1.1033891439437866 | avg loss: 1.0917609428104602\n",
      "batch loss: 1.0496944189071655 | avg loss: 1.091516369997069\n",
      "batch loss: 1.0623477697372437 | avg loss: 1.0913477653712895\n",
      "batch loss: 1.141778826713562 | avg loss: 1.0916375990571647\n",
      "batch loss: 1.0816991329193115 | avg loss: 1.0915808078220912\n",
      "batch loss: 1.1525719165802002 | avg loss: 1.0919273482127623\n",
      "batch loss: 1.0649486780166626 | avg loss: 1.0917749263472476\n",
      "batch loss: 1.067955493927002 | avg loss: 1.0916411093111789\n",
      "batch loss: 1.0335383415222168 | avg loss: 1.0913165128430842\n",
      "batch loss: 1.1132261753082275 | avg loss: 1.0914382331901127\n",
      "batch loss: 1.1102267503738403 | avg loss: 1.0915420371524536\n",
      "batch loss: 1.0989747047424316 | avg loss: 1.0915828759853656\n",
      "batch loss: 1.0917378664016724 | avg loss: 1.0915837229275314\n",
      "batch loss: 1.138636827468872 | avg loss: 1.0918394463217778\n",
      "batch loss: 1.1526830196380615 | avg loss: 1.0921683305018657\n",
      "batch loss: 1.084663987159729 | avg loss: 1.0921279845699188\n",
      "batch loss: 1.0779683589935303 | avg loss: 1.0920522646470503\n",
      "batch loss: 1.105746865272522 | avg loss: 1.0921251082673986\n",
      "batch loss: 1.0641604661941528 | avg loss: 1.091977147198228\n",
      "batch loss: 1.07364821434021 | avg loss: 1.0918806791305542\n",
      "batch loss: 1.0585252046585083 | avg loss: 1.091706043138554\n",
      "batch loss: 1.1291571855545044 | avg loss: 1.0919011011719704\n",
      "batch loss: 1.1289541721343994 | avg loss: 1.09209308599561\n",
      "batch loss: 1.0852590799331665 | avg loss: 1.0920578591602366\n",
      "batch loss: 1.095513939857483 | avg loss: 1.0920755826509916\n",
      "batch loss: 1.087208867073059 | avg loss: 1.0920507524694716\n",
      "batch loss: 1.0980420112609863 | avg loss: 1.0920811649506468\n",
      "batch loss: 1.1261807680130005 | avg loss: 1.0922533851681333\n",
      "batch loss: 1.0713247060775757 | avg loss: 1.0921482159264724\n",
      "batch loss: 1.103811502456665 | avg loss: 1.0922065323591232\n",
      "batch loss: 1.1139761209487915 | avg loss: 1.092314838770017\n",
      "batch loss: 1.096656322479248 | avg loss: 1.0923363312636272\n",
      "batch loss: 1.096534013748169 | avg loss: 1.0923570095024673\n",
      "batch loss: 1.0888367891311646 | avg loss: 1.092339753520255\n",
      "batch loss: 1.086722731590271 | avg loss: 1.0923123534132795\n",
      "batch loss: 1.120362639427185 | avg loss: 1.0924485198502403\n",
      "batch loss: 1.0643806457519531 | avg loss: 1.0923129262555624\n",
      "batch loss: 1.0800721645355225 | avg loss: 1.0922540764396007\n",
      "batch loss: 1.1039005517959595 | avg loss: 1.0923098012020713\n",
      "batch loss: 1.095302700996399 | avg loss: 1.0923240531058538\n",
      "batch loss: 1.0642400979995728 | avg loss: 1.092190953792554\n",
      "batch loss: 1.1204272508621216 | avg loss: 1.0923241438730709\n",
      "batch loss: 1.0552136898040771 | avg loss: 1.0921499163891788\n",
      "batch loss: 1.0389176607131958 | avg loss: 1.0919011675308798\n",
      "batch loss: 1.060455083847046 | avg loss: 1.0917549066765364\n",
      "batch loss: 1.0828897953033447 | avg loss: 1.091713864494253\n",
      "batch loss: 1.0789719820022583 | avg loss: 1.0916551461417554\n",
      "batch loss: 1.0337212085723877 | avg loss: 1.0913893941345565\n",
      "batch loss: 1.0958737134933472 | avg loss: 1.0914098704786606\n",
      "batch loss: 1.1304141283035278 | avg loss: 1.0915871625596827\n",
      "batch loss: 1.1694425344467163 | avg loss: 1.0919394493103027\n",
      "batch loss: 1.034072995185852 | avg loss: 1.0916787896070395\n",
      "batch loss: 1.0467890501022339 | avg loss: 1.0914774903267488\n",
      "batch loss: 1.1412687301635742 | avg loss: 1.0916997726474489\n",
      "batch loss: 1.0893958806991577 | avg loss: 1.0916895331276788\n",
      "batch loss: 1.046275019645691 | avg loss: 1.0914885839529798\n",
      "batch loss: 1.077705979347229 | avg loss: 1.0914278676331306\n",
      "batch loss: 1.103145718574524 | avg loss: 1.0914792617162068\n",
      "batch loss: 1.1498247385025024 | avg loss: 1.0917340454576316\n",
      "batch loss: 1.0975733995437622 | avg loss: 1.0917594339536585\n",
      "batch loss: 1.1086242198944092 | avg loss: 1.0918324416850036\n",
      "batch loss: 1.0883407592773438 | avg loss: 1.0918173913297982\n",
      "batch loss: 1.1602565050125122 | avg loss: 1.0921111214314407\n",
      "batch loss: 1.1434223651885986 | avg loss: 1.0923304002509158\n",
      "batch loss: 1.100633144378662 | avg loss: 1.0923657310769912\n",
      "batch loss: 1.1055504083633423 | avg loss: 1.0924215983536283\n",
      "batch loss: 1.1195436716079712 | avg loss: 1.0925360374812838\n",
      "batch loss: 1.0719568729400635 | avg loss: 1.0924495704033796\n",
      "batch loss: 1.0783860683441162 | avg loss: 1.0923907272985292\n",
      "batch loss: 1.1257338523864746 | avg loss: 1.0925296569863956\n",
      "batch loss: 1.087982416152954 | avg loss: 1.0925107887671697\n",
      "batch loss: 1.0791773796081543 | avg loss: 1.0924556920351076\n",
      "batch loss: 1.0999929904937744 | avg loss: 1.092486709724238\n",
      "batch loss: 1.0951322317123413 | avg loss: 1.0924975520274678\n",
      "batch loss: 1.1101218461990356 | avg loss: 1.0925694879220456\n",
      "batch loss: 1.1057653427124023 | avg loss: 1.0926231296081854\n",
      "batch loss: 1.1357860565185547 | avg loss: 1.0927978783001302\n",
      "batch loss: 1.1070151329040527 | avg loss: 1.0928552059396621\n",
      "batch loss: 1.0957043170928955 | avg loss: 1.0928666481531288\n",
      "batch loss: 1.0808950662612915 | avg loss: 1.0928187618255616\n",
      "batch loss: 1.0941035747528076 | avg loss: 1.0928238806021642\n",
      "batch loss: 1.057862401008606 | avg loss: 1.092685144572031\n",
      "batch loss: 1.1203815937042236 | avg loss: 1.0927946167029883\n",
      "batch loss: 1.0814582109451294 | avg loss: 1.0927499851842566\n",
      "batch loss: 1.09635329246521 | avg loss: 1.0927641158010446\n",
      "batch loss: 1.0816428661346436 | avg loss: 1.0927206734195352\n",
      "batch loss: 1.0714492797851562 | avg loss: 1.092637905350919\n",
      "batch loss: 1.0823158025741577 | avg loss: 1.0925978972006214\n",
      "batch loss: 1.034162998199463 | avg loss: 1.0923722798299604\n",
      "batch loss: 1.104587197303772 | avg loss: 1.0924192602817828\n",
      "batch loss: 1.12177574634552 | avg loss: 1.0925317372398815\n",
      "batch loss: 1.1192357540130615 | avg loss: 1.0926336609680234\n",
      "batch loss: 1.068595290184021 | avg loss: 1.0925422603186545\n",
      "batch loss: 1.0551177263259888 | avg loss: 1.0924005007201976\n",
      "batch loss: 1.0935553312301636 | avg loss: 1.0924048585711785\n",
      "batch loss: 1.0810627937316895 | avg loss: 1.0923622192296767\n",
      "batch loss: 1.0843889713287354 | avg loss: 1.0923323568779877\n",
      "batch loss: 1.0786175727844238 | avg loss: 1.0922811823104746\n",
      "batch loss: 1.095030426979065 | avg loss: 1.092291402550878\n",
      "batch loss: 1.0950381755828857 | avg loss: 1.09230157578433\n",
      "batch loss: 1.072216510772705 | avg loss: 1.0922274611532907\n",
      "batch loss: 1.0979692935943604 | avg loss: 1.092248570831383\n",
      "batch loss: 1.0762912034988403 | avg loss: 1.092190118936392\n",
      "batch loss: 1.1147913932800293 | avg loss: 1.092272605339106\n",
      "batch loss: 1.0987259149551392 | avg loss: 1.0922960719195278\n",
      "batch loss: 1.0719366073608398 | avg loss: 1.0922223057435907\n",
      "batch loss: 1.1347200870513916 | avg loss: 1.0923757273367596\n",
      "batch loss: 1.1421473026275635 | avg loss: 1.0925547617802518\n",
      "batch loss: 1.0865375995635986 | avg loss: 1.0925331948905863\n",
      "batch loss: 1.1248596906661987 | avg loss: 1.0926486466612135\n",
      "batch loss: 1.0993727445602417 | avg loss: 1.0926725758352314\n",
      "batch loss: 1.1116092205047607 | avg loss: 1.0927397270574637\n",
      "batch loss: 1.0953052043914795 | avg loss: 1.0927487923483967\n",
      "batch loss: 1.043121576309204 | avg loss: 1.092574048629949\n",
      "batch loss: 1.0916513204574585 | avg loss: 1.0925708109872383\n",
      "batch loss: 1.0865237712860107 | avg loss: 1.0925496674917794\n",
      "batch loss: 1.1105469465255737 | avg loss: 1.0926123757810957\n",
      "batch loss: 1.043494462966919 | avg loss: 1.0924418274727132\n",
      "batch loss: 1.1044062376022339 | avg loss: 1.092483226815722\n",
      "batch loss: 1.0704885721206665 | avg loss: 1.0924073831788426\n",
      "batch loss: 1.1349272727966309 | avg loss: 1.0925534996380102\n",
      "batch loss: 1.0925493240356445 | avg loss: 1.0925534853380021\n",
      "batch loss: 1.0726763010025024 | avg loss: 1.092485645118427\n",
      "batch loss: 1.1223007440567017 | avg loss: 1.0925870570195775\n",
      "batch loss: 1.1035135984420776 | avg loss: 1.0926240961430436\n",
      "batch loss: 1.080588936805725 | avg loss: 1.0925834368209582\n",
      "batch loss: 1.094874382019043 | avg loss: 1.0925911504411536\n",
      "batch loss: 1.0400584936141968 | avg loss: 1.0924148663578417\n",
      "batch loss: 1.1089885234832764 | avg loss: 1.0924702966492312\n",
      "batch loss: 1.1099656820297241 | avg loss: 1.0925286146004995\n",
      "batch loss: 1.0542408227920532 | avg loss: 1.0924014126343584\n",
      "batch loss: 1.0817019939422607 | avg loss: 1.092365984095643\n",
      "batch loss: 1.1353669166564941 | avg loss: 1.0925079013648207\n",
      "batch loss: 1.1085487604141235 | avg loss: 1.0925606673485355\n",
      "batch loss: 1.0948768854141235 | avg loss: 1.0925682615061276\n",
      "batch loss: 1.1350395679473877 | avg loss: 1.0927070566252166\n",
      "batch loss: 1.0945359468460083 | avg loss: 1.0927130139223526\n",
      "batch loss: 1.0951265096664429 | avg loss: 1.0927208499474959\n",
      "batch loss: 1.0567995309829712 | avg loss: 1.0926045997243097\n",
      "batch loss: 1.1210553646087646 | avg loss: 1.0926963763852273\n",
      "batch loss: 1.1073882579803467 | avg loss: 1.0927436171620606\n",
      "batch loss: 1.0941827297210693 | avg loss: 1.0927482297023137\n",
      "batch loss: 1.0954337120056152 | avg loss: 1.0927568095179792\n",
      "batch loss: 1.0707556009292603 | avg loss: 1.092686741974703\n",
      "batch loss: 1.0586566925048828 | avg loss: 1.0925787100716242\n",
      "batch loss: 1.0947442054748535 | avg loss: 1.0925855629051788\n",
      "batch loss: 1.1205600500106812 | avg loss: 1.0926738104985716\n",
      "batch loss: 1.107236623764038 | avg loss: 1.0927196055088404\n",
      "batch loss: 1.0943918228149414 | avg loss: 1.092724847569361\n",
      "batch loss: 1.1058064699172974 | avg loss: 1.0927657276391982\n",
      "batch loss: 1.0429683923721313 | avg loss: 1.0926105957536312\n",
      "batch loss: 1.0739802122116089 | avg loss: 1.0925527374196498\n",
      "batch loss: 1.1302289962768555 | avg loss: 1.0926693821839135\n",
      "batch loss: 1.1222882270812988 | avg loss: 1.0927607983718683\n",
      "batch loss: 1.0445066690444946 | avg loss: 1.092612324127784\n",
      "batch loss: 1.1087487936019897 | avg loss: 1.0926618225004043\n",
      "batch loss: 1.1061110496520996 | avg loss: 1.092702951635425\n",
      "batch loss: 1.0893824100494385 | avg loss: 1.0926928280330286\n",
      "batch loss: 1.055925726890564 | avg loss: 1.0925810739262125\n",
      "batch loss: 1.1123477220535278 | avg loss: 1.0926409728599318\n",
      "batch loss: 1.1202114820480347 | avg loss: 1.0927242674496238\n",
      "batch loss: 1.095082402229309 | avg loss: 1.0927313702652253\n",
      "batch loss: 1.084385633468628 | avg loss: 1.092706307992563\n",
      "batch loss: 1.0879411697387695 | avg loss: 1.0926920411115635\n",
      "batch loss: 1.094985008239746 | avg loss: 1.092698885789558\n",
      "batch loss: 1.063827395439148 | avg loss: 1.0926129587349438\n",
      "batch loss: 1.1195646524429321 | avg loss: 1.092692934087193\n",
      "batch loss: 1.0313832759857178 | avg loss: 1.0925115445661826\n",
      "batch loss: 1.009947657585144 | avg loss: 1.092267993277153\n",
      "batch loss: 1.1308649778366089 | avg loss: 1.092381513819975\n",
      "batch loss: 1.0951447486877441 | avg loss: 1.092389617148033\n",
      "batch loss: 1.0803539752960205 | avg loss: 1.0923544252127932\n",
      "batch loss: 1.0760602951049805 | avg loss: 1.0923069204602922\n",
      "batch loss: 1.0542305707931519 | avg loss: 1.0921962333973063\n",
      "batch loss: 1.0839945077896118 | avg loss: 1.092172460279603\n",
      "batch loss: 1.1107572317123413 | avg loss: 1.0922261734918364\n",
      "batch loss: 1.074707269668579 | avg loss: 1.0921756867373025\n",
      "batch loss: 1.106567144393921 | avg loss: 1.0922170415006835\n",
      "batch loss: 1.090318202972412 | avg loss: 1.0922116007026081\n",
      "batch loss: 1.098106861114502 | avg loss: 1.092228444303785\n",
      "batch loss: 1.04855477809906 | avg loss: 1.0921040179043413\n",
      "batch loss: 1.0431606769561768 | avg loss: 1.0919649743221023\n",
      "batch loss: 1.0370692014694214 | avg loss: 1.0918094622177037\n",
      "batch loss: 1.1329808235168457 | avg loss: 1.0919257654982097\n",
      "batch loss: 1.0733340978622437 | avg loss: 1.0918733946034607\n",
      "batch loss: 1.0997049808502197 | avg loss: 1.0918953934412323\n",
      "batch loss: 1.0236761569976807 | avg loss: 1.091704303143071\n",
      "batch loss: 1.0549204349517822 | avg loss: 1.0916015549079\n",
      "batch loss: 1.0942103862762451 | avg loss: 1.0916088218476447\n",
      "batch loss: 1.052903413772583 | avg loss: 1.091501306825214\n",
      "batch loss: 1.0473194122314453 | avg loss: 1.0913789193055636\n",
      "batch loss: 1.1115899085998535 | avg loss: 1.0914347507677025\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 0:27:26\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.39\n",
      "  Validation took: 0:01:45\n",
      "\n",
      "======= Epoch 3 / 5 =======\n",
      "batch loss: 1.1069384813308716 | avg loss: 1.1069384813308716\n",
      "batch loss: 1.0885589122772217 | avg loss: 1.0977486968040466\n",
      "batch loss: 1.0813932418823242 | avg loss: 1.092296878496806\n",
      "batch loss: 1.0580329895019531 | avg loss: 1.0837309062480927\n",
      "batch loss: 1.0353736877441406 | avg loss: 1.0740594625473023\n",
      "batch loss: 1.0635583400726318 | avg loss: 1.0723092754681904\n",
      "batch loss: 1.0525944232940674 | avg loss: 1.0694928680147444\n",
      "batch loss: 1.1071487665176392 | avg loss: 1.0741998553276062\n",
      "batch loss: 1.1064552068710327 | avg loss: 1.0777837832768757\n",
      "batch loss: 1.0807193517684937 | avg loss: 1.0780773401260375\n",
      "batch loss: 1.0767532587051392 | avg loss: 1.0779569690877742\n",
      "batch loss: 1.1115938425064087 | avg loss: 1.0807600418726604\n",
      "batch loss: 1.0203970670700073 | avg loss: 1.0761167361186101\n",
      "batch loss: 1.0894432067871094 | avg loss: 1.0770686268806458\n",
      "batch loss: 1.1614631414413452 | avg loss: 1.0826949278513591\n",
      "batch loss: 1.1192001104354858 | avg loss: 1.084976501762867\n",
      "batch loss: 1.0830342769622803 | avg loss: 1.0848622532451855\n",
      "batch loss: 1.1058616638183594 | avg loss: 1.0860288871659174\n",
      "batch loss: 1.0983835458755493 | avg loss: 1.086679132361161\n",
      "batch loss: 1.0347660779953003 | avg loss: 1.084083479642868\n",
      "batch loss: 1.1371381282806396 | avg loss: 1.086609891482762\n",
      "batch loss: 1.137129306793213 | avg loss: 1.0889062285423279\n",
      "batch loss: 1.1120591163635254 | avg loss: 1.0899128758389016\n",
      "batch loss: 1.1118580102920532 | avg loss: 1.0908272564411163\n",
      "batch loss: 1.0911346673965454 | avg loss: 1.0908395528793335\n",
      "batch loss: 1.0418797731399536 | avg loss: 1.0889564844278188\n",
      "batch loss: 1.0781664848327637 | avg loss: 1.0885568548131872\n",
      "batch loss: 1.0625807046890259 | avg loss: 1.0876291351658958\n",
      "batch loss: 1.1234033107757568 | avg loss: 1.0888627274283047\n",
      "batch loss: 1.076220154762268 | avg loss: 1.0884413083394369\n",
      "batch loss: 1.0212846994400024 | avg loss: 1.0862749661168745\n",
      "batch loss: 1.0611281394958496 | avg loss: 1.0854891277849674\n",
      "batch loss: 1.128772258758545 | avg loss: 1.0868007378144697\n",
      "batch loss: 1.1358362436294556 | avg loss: 1.088242958573734\n",
      "batch loss: 1.0617462396621704 | avg loss: 1.087485909461975\n",
      "batch loss: 1.021794319152832 | avg loss: 1.085661143064499\n",
      "batch loss: 1.09744393825531 | avg loss: 1.0859795969885748\n",
      "batch loss: 1.0729942321777344 | avg loss: 1.0856378768619739\n",
      "batch loss: 1.0485590696334839 | avg loss: 1.0846871382150896\n",
      "batch loss: 1.1373521089553833 | avg loss: 1.0860037624835968\n",
      "batch loss: 1.0597916841506958 | avg loss: 1.0853644434998675\n",
      "batch loss: 1.1049853563308716 | avg loss: 1.085831608091082\n",
      "batch loss: 1.0349234342575073 | avg loss: 1.0846476970716965\n",
      "batch loss: 1.1213170289993286 | avg loss: 1.0854810909791426\n",
      "batch loss: 1.0726913213729858 | avg loss: 1.0851968738767837\n",
      "batch loss: 1.1008275747299194 | avg loss: 1.085536671721417\n",
      "batch loss: 1.0200544595718384 | avg loss: 1.084143433165043\n",
      "batch loss: 1.08858060836792 | avg loss: 1.0842358743151028\n",
      "batch loss: 1.078927993774414 | avg loss: 1.0841275502224357\n",
      "batch loss: 1.0833951234817505 | avg loss: 1.084112901687622\n",
      "batch loss: 1.0608718395233154 | avg loss: 1.0836571945863611\n",
      "batch loss: 1.1421316862106323 | avg loss: 1.084781704040674\n",
      "batch loss: 1.079195261001587 | avg loss: 1.084676299455031\n",
      "batch loss: 1.0591272115707397 | avg loss: 1.0842031681979145\n",
      "batch loss: 1.132065773010254 | avg loss: 1.0850733973763205\n",
      "batch loss: 1.0087885856628418 | avg loss: 1.0837111685957228\n",
      "batch loss: 1.011443018913269 | avg loss: 1.082443306320592\n",
      "batch loss: 1.0534064769744873 | avg loss: 1.081942671331866\n",
      "batch loss: 1.046636939048767 | avg loss: 1.0813442690897797\n",
      "batch loss: 1.093637466430664 | avg loss: 1.0815491557121277\n",
      "batch loss: 1.0931026935577393 | avg loss: 1.081738557971892\n",
      "batch loss: 1.0458133220672607 | avg loss: 1.0811591186831075\n",
      "batch loss: 1.031247854232788 | avg loss: 1.0803668763902452\n",
      "batch loss: 1.1159487962722778 | avg loss: 1.080922843888402\n",
      "batch loss: 1.1080360412597656 | avg loss: 1.0813399700018076\n",
      "batch loss: 1.0951299667358398 | avg loss: 1.0815489093462627\n",
      "batch loss: 1.168169617652893 | avg loss: 1.082841755738899\n",
      "batch loss: 1.1178693771362305 | avg loss: 1.0833568678182715\n",
      "batch loss: 1.0795340538024902 | avg loss: 1.0833014647165935\n",
      "batch loss: 1.1032894849777222 | avg loss: 1.083587007863181\n",
      "batch loss: 1.0604764223098755 | avg loss: 1.0832615066582048\n",
      "batch loss: 1.0640308856964111 | avg loss: 1.0829944147004023\n",
      "batch loss: 0.9979962110519409 | avg loss: 1.0818300557463136\n",
      "batch loss: 1.0615077018737793 | avg loss: 1.0815554293426308\n",
      "batch loss: 1.0757898092269897 | avg loss: 1.0814785544077556\n",
      "batch loss: 0.9956527352333069 | avg loss: 1.0803492673133548\n",
      "batch loss: 1.0808582305908203 | avg loss: 1.0803558772260493\n",
      "batch loss: 1.034036636352539 | avg loss: 1.079762040804594\n",
      "batch loss: 1.143704891204834 | avg loss: 1.0805714439742173\n",
      "batch loss: 1.1324321031570435 | avg loss: 1.0812197022140027\n",
      "batch loss: 1.0430514812469482 | avg loss: 1.080748489609471\n",
      "batch loss: 1.0827064514160156 | avg loss: 1.0807723671924778\n",
      "batch loss: 1.0296999216079712 | avg loss: 1.0801570365227848\n",
      "batch loss: 1.0600920915603638 | avg loss: 1.0799181681303751\n",
      "batch loss: 1.057087779045105 | avg loss: 1.0796495753176072\n",
      "batch loss: 1.015100359916687 | avg loss: 1.0788990030455035\n",
      "batch loss: 1.0788166522979736 | avg loss: 1.078898056485187\n",
      "batch loss: 1.076709508895874 | avg loss: 1.0788731866262176\n",
      "batch loss: 0.988207221031189 | avg loss: 1.0778544679116666\n",
      "batch loss: 1.0252065658569336 | avg loss: 1.0772694912221696\n",
      "batch loss: 0.9831136465072632 | avg loss: 1.0762348116099179\n",
      "batch loss: 1.1072604656219482 | avg loss: 1.0765720469796138\n",
      "batch loss: 1.1414450407028198 | avg loss: 1.0772696060519065\n",
      "batch loss: 1.0738847255706787 | avg loss: 1.0772335966850848\n",
      "batch loss: 1.0603314638137817 | avg loss: 1.077055679496966\n",
      "batch loss: 1.1352616548538208 | avg loss: 1.0776619917402666\n",
      "batch loss: 1.150273323059082 | avg loss: 1.0784105621662337\n",
      "batch loss: 1.0491846799850464 | avg loss: 1.0781123388786704\n",
      "batch loss: 1.0387873649597168 | avg loss: 1.0777151169198933\n",
      "batch loss: 1.1155089139938354 | avg loss: 1.0780930548906327\n",
      "batch loss: 1.0513683557510376 | avg loss: 1.0778284539090526\n",
      "batch loss: 1.147705078125 | avg loss: 1.078513518852346\n",
      "batch loss: 1.0632702112197876 | avg loss: 1.07836552557436\n",
      "batch loss: 1.0880167484283447 | avg loss: 1.0784583257941098\n",
      "batch loss: 1.0715810060501099 | avg loss: 1.0783928275108337\n",
      "batch loss: 0.9607563018798828 | avg loss: 1.0772830489671454\n",
      "batch loss: 1.1416233777999878 | avg loss: 1.0778843604515647\n",
      "batch loss: 1.0692756175994873 | avg loss: 1.077804649869601\n",
      "batch loss: 1.2006946802139282 | avg loss: 1.0789320813406498\n",
      "batch loss: 1.1439772844314575 | avg loss: 1.079523401368748\n",
      "batch loss: 0.9213610887527466 | avg loss: 1.0780985156695049\n",
      "batch loss: 0.9820232391357422 | avg loss: 1.0772407007004534\n",
      "batch loss: 1.1945536136627197 | avg loss: 1.0782788680718007\n",
      "batch loss: 1.087432622909546 | avg loss: 1.0783591641668688\n",
      "batch loss: 1.2149766683578491 | avg loss: 1.0795471424641816\n",
      "batch loss: 1.2121073007583618 | avg loss: 1.0806899024494763\n",
      "batch loss: 1.1078591346740723 | avg loss: 1.0809221181095157\n",
      "batch loss: 1.1169638633728027 | avg loss: 1.081227556628696\n",
      "batch loss: 1.0505441427230835 | avg loss: 1.0809697128143632\n",
      "batch loss: 1.1842689514160156 | avg loss: 1.0818305398027102\n",
      "batch loss: 1.089178204536438 | avg loss: 1.0818912643046419\n",
      "batch loss: 1.0890997648239136 | avg loss: 1.0819503503744718\n",
      "batch loss: 1.084219217300415 | avg loss: 1.0819687964470406\n",
      "batch loss: 1.0986483097076416 | avg loss: 1.0821033086507552\n",
      "batch loss: 1.056304693222046 | avg loss: 1.0818969197273254\n",
      "batch loss: 1.0979939699172974 | avg loss: 1.0820246740939126\n",
      "batch loss: 1.0769293308258057 | avg loss: 1.081984553280778\n",
      "batch loss: 1.0574824810028076 | avg loss: 1.0817931308411062\n",
      "batch loss: 1.0569716691970825 | avg loss: 1.0816007164097572\n",
      "batch loss: 1.1141260862350464 | avg loss: 1.0818509115622594\n",
      "batch loss: 1.0994068384170532 | avg loss: 1.0819849262710746\n",
      "batch loss: 1.0688424110412598 | avg loss: 1.0818853617617579\n",
      "batch loss: 1.1406006813049316 | avg loss: 1.0823268303297515\n",
      "batch loss: 1.0593349933624268 | avg loss: 1.0821552494568611\n",
      "batch loss: 1.090151071548462 | avg loss: 1.0822144777686507\n",
      "batch loss: 1.111504077911377 | avg loss: 1.0824298424755825\n",
      "batch loss: 1.1217440366744995 | avg loss: 1.0827168073967426\n",
      "batch loss: 1.1264805793762207 | avg loss: 1.0830339361792025\n",
      "batch loss: 1.0986287593841553 | avg loss: 1.0831461291519\n",
      "batch loss: 1.0800477266311646 | avg loss: 1.0831239977053233\n",
      "batch loss: 1.0907756090164185 | avg loss: 1.0831782644522105\n",
      "batch loss: 1.0654969215393066 | avg loss: 1.083053747952824\n",
      "batch loss: 1.0654945373535156 | avg loss: 1.0829309562703113\n",
      "batch loss: 1.063949704170227 | avg loss: 1.0827991420196161\n",
      "batch loss: 1.0734091997146606 | avg loss: 1.0827343837968235\n",
      "batch loss: 1.1454322338104248 | avg loss: 1.0831638211256838\n",
      "batch loss: 1.0848942995071411 | avg loss: 1.0831755930874623\n",
      "batch loss: 1.0991750955581665 | avg loss: 1.083283697833886\n",
      "batch loss: 1.1287753582000732 | avg loss: 1.083589010990706\n",
      "batch loss: 1.1429537534713745 | avg loss: 1.0839847759405772\n",
      "batch loss: 1.078294277191162 | avg loss: 1.0839470905183957\n",
      "batch loss: 1.1046340465545654 | avg loss: 1.0840831889133704\n",
      "batch loss: 1.0605915784835815 | avg loss: 1.083929648975921\n",
      "batch loss: 1.0499963760375977 | avg loss: 1.0837093030477498\n",
      "batch loss: 1.0924814939498901 | avg loss: 1.0837658978277638\n",
      "batch loss: 1.1371883153915405 | avg loss: 1.0841083492224033\n",
      "batch loss: 1.0928515195846558 | avg loss: 1.0841640382056024\n",
      "batch loss: 1.093445897102356 | avg loss: 1.0842227841479868\n",
      "batch loss: 1.0819830894470215 | avg loss: 1.084208698017792\n",
      "batch loss: 1.0812087059020996 | avg loss: 1.0841899480670691\n",
      "batch loss: 1.1357016563415527 | avg loss: 1.0845098965656683\n",
      "batch loss: 1.103606104850769 | avg loss: 1.0846277743945887\n",
      "batch loss: 1.0978699922561646 | avg loss: 1.0847090149949665\n",
      "batch loss: 1.1265305280685425 | avg loss: 1.0849640242210248\n",
      "batch loss: 1.0806912183761597 | avg loss: 1.0849381284280257\n",
      "batch loss: 1.1250678300857544 | avg loss: 1.0851798736187348\n",
      "batch loss: 1.0763189792633057 | avg loss: 1.0851268143710975\n",
      "batch loss: 1.0876226425170898 | avg loss: 1.0851416704910142\n",
      "batch loss: 1.0674448013305664 | avg loss: 1.0850369552888814\n",
      "batch loss: 1.0557832717895508 | avg loss: 1.0848648747977088\n",
      "batch loss: 1.1187443733215332 | avg loss: 1.0850630005200703\n",
      "batch loss: 1.0714833736419678 | avg loss: 1.0849840492010117\n",
      "batch loss: 1.083466649055481 | avg loss: 1.0849752781019046\n",
      "batch loss: 1.1427932977676392 | avg loss: 1.0853075655712479\n",
      "batch loss: 1.0612115859985352 | avg loss: 1.0851698742594038\n",
      "batch loss: 1.1369099617004395 | avg loss: 1.085463852028955\n",
      "batch loss: 1.0758699178695679 | avg loss: 1.0854096490111054\n",
      "batch loss: 1.0718635320663452 | avg loss: 1.085333547230517\n",
      "batch loss: 1.0421547889709473 | avg loss: 1.085092325117335\n",
      "batch loss: 1.1203640699386597 | avg loss: 1.0852882792552312\n",
      "batch loss: 1.122725248336792 | avg loss: 1.0854951133385549\n",
      "batch loss: 1.1020338535308838 | avg loss: 1.0855859855374137\n",
      "batch loss: 1.0829359292984009 | avg loss: 1.0855715043557799\n",
      "batch loss: 1.138875961303711 | avg loss: 1.0858612024913663\n",
      "batch loss: 1.1562012434005737 | avg loss: 1.0862414189287135\n",
      "batch loss: 1.0837527513504028 | avg loss: 1.0862280389954966\n",
      "batch loss: 1.0839438438415527 | avg loss: 1.0862158240481494\n",
      "batch loss: 1.1158533096313477 | avg loss: 1.08637347024806\n",
      "batch loss: 1.0671747922897339 | avg loss: 1.0862718899414021\n",
      "batch loss: 1.0790777206420898 | avg loss: 1.0862340258924585\n",
      "batch loss: 1.0556472539901733 | avg loss: 1.0860738857254308\n",
      "batch loss: 1.1316502094268799 | avg loss: 1.0863112624113758\n",
      "batch loss: 1.1313341856002808 | avg loss: 1.086544541806137\n",
      "batch loss: 1.0850554704666138 | avg loss: 1.0865368661806756\n",
      "batch loss: 1.092345118522644 | avg loss: 1.0865666520901216\n",
      "batch loss: 1.0837626457214355 | avg loss: 1.0865523459351794\n",
      "batch loss: 1.0917190313339233 | avg loss: 1.0865785727646144\n",
      "batch loss: 1.1249148845672607 | avg loss: 1.0867721905009915\n",
      "batch loss: 1.0825401544570923 | avg loss: 1.086750923988208\n",
      "batch loss: 1.1072224378585815 | avg loss: 1.08685328155756\n",
      "batch loss: 1.1135151386260986 | avg loss: 1.0869859276126272\n",
      "batch loss: 1.0953521728515625 | avg loss: 1.0870273446682657\n",
      "batch loss: 1.0952520370483398 | avg loss: 1.0870678603942758\n",
      "batch loss: 1.0830907821655273 | avg loss: 1.0870483649127625\n",
      "batch loss: 1.102591633796692 | avg loss: 1.0871241857365863\n",
      "batch loss: 1.1212629079818726 | avg loss: 1.0872899076892335\n",
      "batch loss: 1.0576651096343994 | avg loss: 1.087146792722785\n",
      "batch loss: 1.082014799118042 | avg loss: 1.0871221196766083\n",
      "batch loss: 1.0952808856964111 | avg loss: 1.0871611568345978\n",
      "batch loss: 1.0823489427566528 | avg loss: 1.0871382415294648\n",
      "batch loss: 1.070180892944336 | avg loss: 1.0870578749484925\n",
      "batch loss: 1.1212215423583984 | avg loss: 1.0872190243230675\n",
      "batch loss: 1.055863380432129 | avg loss: 1.0870718147273355\n",
      "batch loss: 1.0418801307678223 | avg loss: 1.0868606386340667\n",
      "batch loss: 1.0504978895187378 | avg loss: 1.086691509568414\n",
      "batch loss: 1.0981651544570923 | avg loss: 1.0867446282947506\n",
      "batch loss: 1.0792423486709595 | avg loss: 1.0867100555775902\n",
      "batch loss: 1.0239007472991943 | avg loss: 1.08642193948457\n",
      "batch loss: 1.0968704223632812 | avg loss: 1.0864696494520527\n",
      "batch loss: 1.1591041088104248 | avg loss: 1.0867998060854998\n",
      "batch loss: 1.1881992816925049 | avg loss: 1.0872586272420925\n",
      "batch loss: 1.0273357629776 | avg loss: 1.0869887044300903\n",
      "batch loss: 1.029561996459961 | avg loss: 1.0867311855602693\n",
      "batch loss: 1.1528935432434082 | avg loss: 1.0870265532284975\n",
      "batch loss: 1.0960761308670044 | avg loss: 1.0870667735735575\n",
      "batch loss: 1.011086344718933 | avg loss: 1.0867305769857052\n",
      "batch loss: 1.0594650506973267 | avg loss: 1.0866104645350956\n",
      "batch loss: 1.1053844690322876 | avg loss: 1.0866928066600834\n",
      "batch loss: 1.1807609796524048 | avg loss: 1.0871035847080848\n",
      "batch loss: 1.1181700229644775 | avg loss: 1.0872386561787646\n",
      "batch loss: 1.1067403554916382 | avg loss: 1.087323079119513\n",
      "batch loss: 1.0793918371200562 | avg loss: 1.0872888927315842\n",
      "batch loss: 1.175889253616333 | avg loss: 1.0876691517911756\n",
      "batch loss: 1.1680976152420044 | avg loss: 1.088012863173444\n",
      "batch loss: 1.0972435474395752 | avg loss: 1.0880521426809595\n",
      "batch loss: 1.1207587718963623 | avg loss: 1.0881907300928892\n",
      "batch loss: 1.1399983167648315 | avg loss: 1.088409327505007\n",
      "batch loss: 1.0540052652359009 | avg loss: 1.0882647726215233\n",
      "batch loss: 1.0808258056640625 | avg loss: 1.0882336472367642\n",
      "batch loss: 1.1276999711990356 | avg loss: 1.0883980902532737\n",
      "batch loss: 1.0691936016082764 | avg loss: 1.0883184035783982\n",
      "batch loss: 1.0840221643447876 | avg loss: 1.0883006505237138\n",
      "batch loss: 1.0840808153152466 | avg loss: 1.0882832849467243\n",
      "batch loss: 1.0956283807754517 | avg loss: 1.0883133877984812\n",
      "batch loss: 1.1041297912597656 | avg loss: 1.088377944547303\n",
      "batch loss: 1.1261523962020874 | avg loss: 1.0885314992288264\n",
      "batch loss: 1.1312100887298584 | avg loss: 1.0887042870405714\n",
      "batch loss: 1.1180089712142944 | avg loss: 1.0888224510896591\n",
      "batch loss: 1.0877951383590698 | avg loss: 1.0888183253357209\n",
      "batch loss: 1.1005409955978394 | avg loss: 1.0888652160167693\n",
      "batch loss: 1.1041598320007324 | avg loss: 1.088926150741805\n",
      "batch loss: 1.0715000629425049 | avg loss: 1.0888569995997444\n",
      "batch loss: 1.1035808324813843 | avg loss: 1.088915196567656\n",
      "batch loss: 1.099242091178894 | avg loss: 1.0889558536330546\n",
      "batch loss: 1.083085060119629 | avg loss: 1.088932830913394\n",
      "batch loss: 1.0977809429168701 | avg loss: 1.0889673938509077\n",
      "batch loss: 1.0773075819015503 | avg loss: 1.0889220249328169\n",
      "batch loss: 1.0905418395996094 | avg loss: 1.0889283032842385\n",
      "batch loss: 1.053963303565979 | avg loss: 1.0887933032853263\n",
      "batch loss: 1.11774480342865 | avg loss: 1.0889046552089545\n",
      "batch loss: 1.1062449216842651 | avg loss: 1.088971093011542\n",
      "batch loss: 1.1172066926956177 | avg loss: 1.0890788624759848\n",
      "batch loss: 1.0775718688964844 | avg loss: 1.0890351096486863\n",
      "batch loss: 1.0669760704040527 | avg loss: 1.0889515526818507\n",
      "batch loss: 1.0953506231307983 | avg loss: 1.088975700117507\n",
      "batch loss: 1.0819611549377441 | avg loss: 1.0889493296469064\n",
      "batch loss: 1.0826250314712524 | avg loss: 1.0889256431368852\n",
      "batch loss: 1.079220175743103 | avg loss: 1.0888894287063116\n",
      "batch loss: 1.0938529968261719 | avg loss: 1.0889078806324075\n",
      "batch loss: 1.0937405824661255 | avg loss: 1.088925779528088\n",
      "batch loss: 1.0658478736877441 | avg loss: 1.0888406212039539\n",
      "batch loss: 1.099513053894043 | avg loss: 1.088879858088844\n",
      "batch loss: 1.0693224668502808 | avg loss: 1.0888082192930983\n",
      "batch loss: 1.1238436698913574 | avg loss: 1.0889360859011212\n",
      "batch loss: 1.1043025255203247 | avg loss: 1.0889919638633727\n",
      "batch loss: 1.060001015663147 | avg loss: 1.0888869241959807\n",
      "batch loss: 1.1443051099777222 | avg loss: 1.0890869898486224\n",
      "batch loss: 1.160970687866211 | avg loss: 1.089345564301923\n",
      "batch loss: 1.0890864133834839 | avg loss: 1.0893446354455845\n",
      "batch loss: 1.1341160535812378 | avg loss: 1.0895045333674975\n",
      "batch loss: 1.105887770652771 | avg loss: 1.0895628367030323\n",
      "batch loss: 1.114375114440918 | avg loss: 1.0896508235035214\n",
      "batch loss: 1.0959140062332153 | avg loss: 1.0896729548912587\n",
      "batch loss: 1.04067063331604 | avg loss: 1.0895004115054305\n",
      "batch loss: 1.0884807109832764 | avg loss: 1.0894968336088615\n",
      "batch loss: 1.0864379405975342 | avg loss: 1.089486138178752\n",
      "batch loss: 1.1083141565322876 | avg loss: 1.089551741030158\n",
      "batch loss: 1.0452244281768799 | avg loss: 1.0893978267494175\n",
      "batch loss: 1.1019529104232788 | avg loss: 1.089441269945521\n",
      "batch loss: 1.0679442882537842 | avg loss: 1.0893671424224458\n",
      "batch loss: 1.1230926513671875 | avg loss: 1.0894830376421873\n",
      "batch loss: 1.0922141075134277 | avg loss: 1.0894923906211984\n",
      "batch loss: 1.0675952434539795 | avg loss: 1.0894176563305253\n",
      "batch loss: 1.116756558418274 | avg loss: 1.0895106457934087\n",
      "batch loss: 1.091302752494812 | avg loss: 1.0895167207313796\n",
      "batch loss: 1.0837244987487793 | avg loss: 1.0894971524138708\n",
      "batch loss: 1.0841522216796875 | avg loss: 1.0894791560140924\n",
      "batch loss: 1.0577911138534546 | avg loss: 1.089372820302144\n",
      "batch loss: 1.0919016599655151 | avg loss: 1.089381277959881\n",
      "batch loss: 1.0943255424499512 | avg loss: 1.0893977588415147\n",
      "batch loss: 1.0582304000854492 | avg loss: 1.0892942127991356\n",
      "batch loss: 1.0479774475097656 | avg loss: 1.089157402318045\n",
      "batch loss: 1.1306697130203247 | avg loss: 1.089294406643795\n",
      "batch loss: 1.075227975845337 | avg loss: 1.0892481354898529\n",
      "batch loss: 1.0760071277618408 | avg loss: 1.089204722349761\n",
      "batch loss: 1.1194069385528564 | avg loss: 1.0893034224027123\n",
      "batch loss: 1.0001068115234375 | avg loss: 1.089012879696265\n",
      "batch loss: 1.0079345703125 | avg loss: 1.088749638432032\n",
      "batch loss: 1.116400957107544 | avg loss: 1.0888391249002376\n",
      "batch loss: 1.0769059658050537 | avg loss: 1.0888006308386402\n",
      "batch loss: 1.0848939418792725 | avg loss: 1.0887880691378062\n",
      "batch loss: 1.0723726749420166 | avg loss: 1.0887354556948712\n",
      "batch loss: 1.0996947288513184 | avg loss: 1.088770469347128\n",
      "batch loss: 1.1174126863479614 | avg loss: 1.088861686598723\n",
      "batch loss: 1.1349364519119263 | avg loss: 1.0890079556949555\n",
      "batch loss: 1.0697988271713257 | avg loss: 1.0889471673135516\n",
      "batch loss: 1.143378496170044 | avg loss: 1.0891188749755594\n",
      "batch loss: 1.1077607870101929 | avg loss: 1.0891774973404482\n",
      "batch loss: 1.1267352104187012 | avg loss: 1.089295233118123\n",
      "batch loss: 1.1270900964736938 | avg loss: 1.0894133420661092\n",
      "batch loss: 1.1097955703735352 | avg loss: 1.0894768381044502\n",
      "batch loss: 1.0704561471939087 | avg loss: 1.0894177676357837\n",
      "batch loss: 1.1193040609359741 | avg loss: 1.0895102948596234\n",
      "batch loss: 1.1035184860229492 | avg loss: 1.089553530017535\n",
      "batch loss: 1.0711027383804321 | avg loss: 1.0894967583509592\n",
      "batch loss: 1.101102352142334 | avg loss: 1.0895323583319143\n",
      "batch loss: 1.1037029027938843 | avg loss: 1.089575693330269\n",
      "batch loss: 1.0983037948608398 | avg loss: 1.089602303395911\n",
      "batch loss: 1.0718450546264648 | avg loss: 1.0895483299953959\n",
      "batch loss: 1.103798747062683 | avg loss: 1.089591513077418\n",
      "batch loss: 1.1129095554351807 | avg loss: 1.0896619603352964\n",
      "batch loss: 1.0949883460998535 | avg loss: 1.0896780036659126\n",
      "batch loss: 1.0867011547088623 | avg loss: 1.089669064179555\n",
      "batch loss: 1.0923025608062744 | avg loss: 1.0896769488999944\n",
      "batch loss: 1.0949753522872925 | avg loss: 1.0896927650295087\n",
      "batch loss: 1.0696429014205933 | avg loss: 1.0896330928163869\n",
      "batch loss: 1.1150981187820435 | avg loss: 1.0897086566916558\n",
      "batch loss: 1.0428190231323242 | avg loss: 1.0895699299651491\n",
      "batch loss: 1.0221967697143555 | avg loss: 1.0893711890794535\n",
      "batch loss: 1.12748122215271 | avg loss: 1.0894832774120218\n",
      "batch loss: 1.095038652420044 | avg loss: 1.0894995688343327\n",
      "batch loss: 1.0811692476272583 | avg loss: 1.0894752111699846\n",
      "batch loss: 1.076529860496521 | avg loss: 1.0894374696228317\n",
      "batch loss: 1.0535411834716797 | avg loss: 1.0893331199537877\n",
      "batch loss: 1.0840139389038086 | avg loss: 1.0893177020377007\n",
      "batch loss: 1.113853096961975 | avg loss: 1.0893886135837245\n",
      "batch loss: 1.0712758302688599 | avg loss: 1.0893364153609153\n",
      "batch loss: 1.1096774339675903 | avg loss: 1.089394866563808\n",
      "batch loss: 1.0897337198257446 | avg loss: 1.0893958374900599\n",
      "batch loss: 1.1052662134170532 | avg loss: 1.0894411814212799\n",
      "batch loss: 1.0412523746490479 | avg loss: 1.0893038913734958\n",
      "batch loss: 1.0319160223007202 | avg loss: 1.089140857654539\n",
      "batch loss: 1.0215587615966797 | avg loss: 1.0889494069574912\n",
      "batch loss: 1.1469584703445435 | avg loss: 1.089113274368189\n",
      "batch loss: 1.0661625862121582 | avg loss: 1.0890486245423976\n",
      "batch loss: 1.1071956157684326 | avg loss: 1.0890995992368526\n",
      "batch loss: 0.995290994644165 | avg loss: 1.088836830036313\n",
      "batch loss: 1.0443270206451416 | avg loss: 1.0887125009598013\n",
      "batch loss: 1.0945839881896973 | avg loss: 1.0887288560774333\n",
      "batch loss: 1.0346506834030151 | avg loss: 1.0885786389311154\n",
      "batch loss: 1.0307048559188843 | avg loss: 1.0884183237427159\n",
      "batch loss: 1.1308616399765015 | avg loss: 1.0885355704726436\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 0:29:36\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.39\n",
      "  Validation took: 0:01:33\n",
      "\n",
      "======= Epoch 4 / 5 =======\n",
      "batch loss: 1.1253323554992676 | avg loss: 1.1253323554992676\n",
      "batch loss: 1.0951277017593384 | avg loss: 1.110230028629303\n",
      "batch loss: 1.0756784677505493 | avg loss: 1.0987128416697185\n",
      "batch loss: 1.0483169555664062 | avg loss: 1.0861138701438904\n",
      "batch loss: 1.023998737335205 | avg loss: 1.0736908435821533\n",
      "batch loss: 1.057733416557312 | avg loss: 1.0710312724113464\n",
      "batch loss: 1.0366027355194092 | avg loss: 1.0661129099982125\n",
      "batch loss: 1.1250184774398804 | avg loss: 1.073476105928421\n",
      "batch loss: 1.1362735033035278 | avg loss: 1.080453594525655\n",
      "batch loss: 1.0827741622924805 | avg loss: 1.0806856513023377\n",
      "batch loss: 1.0623842477798462 | avg loss: 1.0790218873457476\n",
      "batch loss: 1.135727882385254 | avg loss: 1.083747386932373\n",
      "batch loss: 0.9839977025985718 | avg loss: 1.0760743342913115\n",
      "batch loss: 1.0983567237854004 | avg loss: 1.0776659335408891\n",
      "batch loss: 1.1899443864822388 | avg loss: 1.085151163736979\n",
      "batch loss: 1.1269783973693848 | avg loss: 1.0877653658390045\n",
      "batch loss: 1.081472635269165 | avg loss: 1.0873952052172493\n",
      "batch loss: 1.120739459991455 | avg loss: 1.0892476638158162\n",
      "batch loss: 1.1039658784866333 | avg loss: 1.0900223066932277\n",
      "batch loss: 1.0194441080093384 | avg loss: 1.0864933967590331\n",
      "batch loss: 1.1619837284088135 | avg loss: 1.0900881744566417\n",
      "batch loss: 1.1518363952636719 | avg loss: 1.0928949117660522\n",
      "batch loss: 1.128866195678711 | avg loss: 1.09445888063182\n",
      "batch loss: 1.1195225715637207 | avg loss: 1.0955032010873158\n",
      "batch loss: 1.0866167545318604 | avg loss: 1.0951477432250976\n",
      "batch loss: 1.0328258275985718 | avg loss: 1.0927507464702313\n",
      "batch loss: 1.0691403150558472 | avg loss: 1.0918762860474762\n",
      "batch loss: 1.0581411123275757 | avg loss: 1.0906714584146227\n",
      "batch loss: 1.1309205293655396 | avg loss: 1.0920593574129303\n",
      "batch loss: 1.0686918497085571 | avg loss: 1.0912804404894512\n",
      "batch loss: 1.017620325088501 | avg loss: 1.0889043077345817\n",
      "batch loss: 1.0592988729476929 | avg loss: 1.0879791378974915\n",
      "batch loss: 1.1398087739944458 | avg loss: 1.0895497329307324\n",
      "batch loss: 1.1441339254379272 | avg loss: 1.0911551503574146\n",
      "batch loss: 1.0677788257598877 | avg loss: 1.090487255368914\n",
      "batch loss: 1.0210990905761719 | avg loss: 1.0885598063468933\n",
      "batch loss: 1.08964204788208 | avg loss: 1.0885890561181146\n",
      "batch loss: 1.0678257942199707 | avg loss: 1.088042654489216\n",
      "batch loss: 1.0449531078338623 | avg loss: 1.0869377943185659\n",
      "batch loss: 1.1290793418884277 | avg loss: 1.0879913330078126\n",
      "batch loss: 1.0669479370117188 | avg loss: 1.087478079446932\n",
      "batch loss: 1.101711392402649 | avg loss: 1.0878169678506397\n",
      "batch loss: 1.0448969602584839 | avg loss: 1.0868188281391942\n",
      "batch loss: 1.1212126016616821 | avg loss: 1.0876005048101598\n",
      "batch loss: 1.0749151706695557 | avg loss: 1.087318608495924\n",
      "batch loss: 1.1014143228530884 | avg loss: 1.087625037068906\n",
      "batch loss: 1.028963327407837 | avg loss: 1.0863769155867555\n",
      "batch loss: 1.0951523780822754 | avg loss: 1.086559737722079\n",
      "batch loss: 1.0920730829238892 | avg loss: 1.0866722549710954\n",
      "batch loss: 1.084879994392395 | avg loss: 1.0866364097595216\n",
      "batch loss: 1.0712788105010986 | avg loss: 1.0863352803622974\n",
      "batch loss: 1.1502156257629395 | avg loss: 1.0875637485430791\n",
      "batch loss: 1.0948834419250488 | avg loss: 1.0877018559653804\n",
      "batch loss: 1.1184850931167603 | avg loss: 1.0882719159126282\n",
      "batch loss: 1.1254030466079712 | avg loss: 1.0889470273798163\n",
      "batch loss: 1.0859092473983765 | avg loss: 1.088892781308719\n",
      "batch loss: 1.0688203573226929 | avg loss: 1.0885406335194905\n",
      "batch loss: 1.0474820137023926 | avg loss: 1.0878327262812648\n",
      "batch loss: 1.0789241790771484 | avg loss: 1.0876817339557712\n",
      "batch loss: 1.1584041118621826 | avg loss: 1.0888604402542115\n",
      "batch loss: 1.079209327697754 | avg loss: 1.0887022252942695\n",
      "batch loss: 1.1044968366622925 | avg loss: 1.0889569770905279\n",
      "batch loss: 1.1138811111450195 | avg loss: 1.089352598265996\n",
      "batch loss: 1.1272090673446655 | avg loss: 1.0899441055953503\n",
      "batch loss: 1.1191977262496948 | avg loss: 1.0903941612977248\n",
      "batch loss: 1.0899462699890137 | avg loss: 1.0903873750657747\n",
      "batch loss: 1.050304651260376 | avg loss: 1.089789125456739\n",
      "batch loss: 1.0658520460128784 | avg loss: 1.0894371095825643\n",
      "batch loss: 1.0633163452148438 | avg loss: 1.0890585477801338\n",
      "batch loss: 1.070121169090271 | avg loss: 1.08878801379885\n",
      "batch loss: 1.0975855588912964 | avg loss: 1.088911922884659\n",
      "batch loss: 1.1066300868988037 | avg loss: 1.0891580084959667\n",
      "batch loss: 1.0599684715270996 | avg loss: 1.0887581518251601\n",
      "batch loss: 1.11164391040802 | avg loss: 1.0890674188330367\n",
      "batch loss: 1.1423803567886353 | avg loss: 1.089778258005778\n",
      "batch loss: 1.0292589664459229 | avg loss: 1.0889819515378851\n",
      "batch loss: 1.1132464408874512 | avg loss: 1.0892970747761912\n",
      "batch loss: 1.123718023300171 | avg loss: 1.089738368988037\n",
      "batch loss: 1.1441071033477783 | avg loss: 1.0904265808153757\n",
      "batch loss: 1.1147369146347046 | avg loss: 1.0907304599881171\n",
      "batch loss: 1.0973773002624512 | avg loss: 1.0908125197445904\n",
      "batch loss: 1.081103801727295 | avg loss: 1.0906941207443797\n",
      "batch loss: 1.076580286026001 | avg loss: 1.0905240745429534\n",
      "batch loss: 1.034403920173645 | avg loss: 1.0898559774671281\n",
      "batch loss: 1.088162899017334 | avg loss: 1.0898360588971305\n",
      "batch loss: 1.1311376094818115 | avg loss: 1.0903163094853245\n",
      "batch loss: 1.1107906103134155 | avg loss: 1.0905516462764522\n",
      "batch loss: 1.1488443613052368 | avg loss: 1.0912140634926883\n",
      "batch loss: 1.10726797580719 | avg loss: 1.09139444452993\n",
      "batch loss: 1.1066069602966309 | avg loss: 1.091563472482893\n",
      "batch loss: 1.1130884885787964 | avg loss: 1.0918000111213098\n",
      "batch loss: 1.1050832271575928 | avg loss: 1.0919443939043127\n",
      "batch loss: 1.1310746669769287 | avg loss: 1.0923651495287496\n",
      "batch loss: 1.0699362754821777 | avg loss: 1.092126544485701\n",
      "batch loss: 1.0661154985427856 | avg loss: 1.0918527440020913\n",
      "batch loss: 1.107548713684082 | avg loss: 1.0920162436862786\n",
      "batch loss: 1.1193495988845825 | avg loss: 1.0922980308532715\n",
      "batch loss: 1.0786566734313965 | avg loss: 1.0921588333285586\n",
      "batch loss: 1.0950602293014526 | avg loss: 1.0921881403585878\n",
      "batch loss: 1.086118221282959 | avg loss: 1.0921274411678314\n",
      "batch loss: 1.0876840353012085 | avg loss: 1.09208344705034\n",
      "batch loss: 1.1167858839035034 | avg loss: 1.0923256278038025\n",
      "batch loss: 1.0821069478988647 | avg loss: 1.0922264173192886\n",
      "batch loss: 1.0741937160491943 | avg loss: 1.0920530259609222\n",
      "batch loss: 1.0771431922912598 | avg loss: 1.0919110275450208\n",
      "batch loss: 1.0774753093719482 | avg loss: 1.09177484152452\n",
      "batch loss: 1.1221635341644287 | avg loss: 1.0920588479977902\n",
      "batch loss: 1.066778302192688 | avg loss: 1.0918247688699652\n",
      "batch loss: 1.1296690702438354 | avg loss: 1.0921719642954135\n",
      "batch loss: 1.0785518884658813 | avg loss: 1.092048145424236\n",
      "batch loss: 1.0725680589675903 | avg loss: 1.0918726491498518\n",
      "batch loss: 1.0363341569900513 | avg loss: 1.0913767697555679\n",
      "batch loss: 1.124245047569275 | avg loss: 1.0916676394707334\n",
      "batch loss: 1.062559962272644 | avg loss: 1.0914123089689958\n",
      "batch loss: 1.1423956155776978 | avg loss: 1.091855642069941\n",
      "batch loss: 1.143050193786621 | avg loss: 1.092296974412326\n",
      "batch loss: 1.0794063806533813 | avg loss: 1.0921867983972924\n",
      "batch loss: 1.1116482019424438 | avg loss: 1.0923517255459803\n",
      "batch loss: 1.061441421508789 | avg loss: 1.092091975091886\n",
      "batch loss: 1.1508855819702148 | avg loss: 1.092581921815872\n",
      "batch loss: 1.0944445133209229 | avg loss: 1.0925973151340957\n",
      "batch loss: 1.09438157081604 | avg loss: 1.092611940180669\n",
      "batch loss: 1.0866726636886597 | avg loss: 1.092563653379921\n",
      "batch loss: 1.0969229936599731 | avg loss: 1.0925988093499215\n",
      "batch loss: 1.0586062669754028 | avg loss: 1.0923268690109253\n",
      "batch loss: 1.0970556735992432 | avg loss: 1.0923643992060708\n",
      "batch loss: 1.0822886228561401 | avg loss: 1.0922850623844176\n",
      "batch loss: 1.0570731163024902 | avg loss: 1.0920099690556526\n",
      "batch loss: 1.041819453239441 | avg loss: 1.0916208952896356\n",
      "batch loss: 1.1098966598510742 | avg loss: 1.0917614780939542\n",
      "batch loss: 1.0978717803955078 | avg loss: 1.0918081216229738\n",
      "batch loss: 1.072854995727539 | avg loss: 1.091664537335887\n",
      "batch loss: 1.1371376514434814 | avg loss: 1.092006440449478\n",
      "batch loss: 1.0591336488723755 | avg loss: 1.0917611211093503\n",
      "batch loss: 1.0850751399993896 | avg loss: 1.0917115953233507\n",
      "batch loss: 1.1120885610580444 | avg loss: 1.091861425953753\n",
      "batch loss: 1.1186655759811401 | avg loss: 1.0920570766838797\n",
      "batch loss: 1.1253068447113037 | avg loss: 1.0922980170319045\n",
      "batch loss: 1.0982322692871094 | avg loss: 1.0923407094941722\n",
      "batch loss: 1.07877779006958 | avg loss: 1.0922438314982823\n",
      "batch loss: 1.0916831493377686 | avg loss: 1.0922398550290588\n",
      "batch loss: 1.0603482723236084 | avg loss: 1.092015266418457\n",
      "batch loss: 1.0603212118148804 | avg loss: 1.0917936296729776\n",
      "batch loss: 1.0598390102386475 | avg loss: 1.0915717225935724\n",
      "batch loss: 1.072107195854187 | avg loss: 1.0914374844781283\n",
      "batch loss: 1.1456719636917114 | avg loss: 1.0918089535138378\n",
      "batch loss: 1.0849339962005615 | avg loss: 1.0917621850967407\n",
      "batch loss: 1.0987540483474731 | avg loss: 1.0918094274160024\n",
      "batch loss: 1.1265524625778198 | avg loss: 1.092042602148632\n",
      "batch loss: 1.1404414176940918 | avg loss: 1.0923652609189352\n",
      "batch loss: 1.0783188343048096 | avg loss: 1.0922722382261263\n",
      "batch loss: 1.1049644947052002 | avg loss: 1.0923557399134887\n",
      "batch loss: 1.0588880777359009 | avg loss: 1.0921369970234391\n",
      "batch loss: 1.0468246936798096 | avg loss: 1.0918427612874415\n",
      "batch loss: 1.092367172241211 | avg loss: 1.0918461445839174\n",
      "batch loss: 1.1394788026809692 | avg loss: 1.0921514821358216\n",
      "batch loss: 1.0918081998825073 | avg loss: 1.092149295624654\n",
      "batch loss: 1.0919950008392334 | avg loss: 1.0921483190753791\n",
      "batch loss: 1.0788630247116089 | avg loss: 1.0920647637649152\n",
      "batch loss: 1.078559160232544 | avg loss: 1.0919803537428379\n",
      "batch loss: 1.1382662057876587 | avg loss: 1.0922678435070914\n",
      "batch loss: 1.1045697927474976 | avg loss: 1.0923437814653656\n",
      "batch loss: 1.0984439849853516 | avg loss: 1.0923812060268379\n",
      "batch loss: 1.1307249069213867 | avg loss: 1.092615009081073\n",
      "batch loss: 1.0819147825241089 | avg loss: 1.092550159223152\n",
      "batch loss: 1.1338521242141724 | avg loss: 1.0927989662411701\n",
      "batch loss: 1.0722839832305908 | avg loss: 1.092676122031526\n",
      "batch loss: 1.0926762819290161 | avg loss: 1.0926761229832966\n",
      "batch loss: 1.0563091039657593 | avg loss: 1.0924609335216544\n",
      "batch loss: 1.0431541204452515 | avg loss: 1.0921708934447345\n",
      "batch loss: 1.107775092124939 | avg loss: 1.0922621460685953\n",
      "batch loss: 1.0516221523284912 | avg loss: 1.0920258670352225\n",
      "batch loss: 1.0651929378509521 | avg loss: 1.0918707633983193\n",
      "batch loss: 1.1442818641662598 | avg loss: 1.0921719766211235\n",
      "batch loss: 1.0762594938278198 | avg loss: 1.092081048148019\n",
      "batch loss: 1.1512387990951538 | avg loss: 1.0924171717329458\n",
      "batch loss: 1.0662195682525635 | avg loss: 1.0922691626737346\n",
      "batch loss: 1.0678290128707886 | avg loss: 1.0921318584613586\n",
      "batch loss: 1.0335111618041992 | avg loss: 1.0918043685358996\n",
      "batch loss: 1.1151776313781738 | avg loss: 1.0919342199961344\n",
      "batch loss: 1.1132068634033203 | avg loss: 1.0920517484127488\n",
      "batch loss: 1.0997366905212402 | avg loss: 1.0920939733693888\n",
      "batch loss: 1.0895518064498901 | avg loss: 1.0920800817468779\n",
      "batch loss: 1.139289379119873 | avg loss: 1.0923366540152093\n",
      "batch loss: 1.1544294357299805 | avg loss: 1.092672290673127\n",
      "batch loss: 1.0842101573944092 | avg loss: 1.092626795332919\n",
      "batch loss: 1.07920241355896 | avg loss: 1.0925550071950902\n",
      "batch loss: 1.1089260578155518 | avg loss: 1.092642087251582\n",
      "batch loss: 1.0642577409744263 | avg loss: 1.092491905525248\n",
      "batch loss: 1.0751659870147705 | avg loss: 1.092400716480456\n",
      "batch loss: 1.0560016632080078 | avg loss: 1.0922101455209143\n",
      "batch loss: 1.1314120292663574 | avg loss: 1.0924143219987552\n",
      "batch loss: 1.1312973499298096 | avg loss: 1.0926157884647192\n",
      "batch loss: 1.0847476720809937 | avg loss: 1.0925752311637722\n",
      "batch loss: 1.0978519916534424 | avg loss: 1.0926022914739755\n",
      "batch loss: 1.088145136833191 | avg loss: 1.0925795508890737\n",
      "batch loss: 1.0999009609222412 | avg loss: 1.0926167154070086\n",
      "batch loss: 1.1287535429000854 | avg loss: 1.0927992246367715\n",
      "batch loss: 1.0685445070266724 | avg loss: 1.0926773416337057\n",
      "batch loss: 1.1042805910110474 | avg loss: 1.0927353578805923\n",
      "batch loss: 1.1154298782348633 | avg loss: 1.0928482659420564\n",
      "batch loss: 1.0970346927642822 | avg loss: 1.092868990827315\n",
      "batch loss: 1.0968903303146362 | avg loss: 1.0928888003814397\n",
      "batch loss: 1.089277744293213 | avg loss: 1.0928710991261052\n",
      "batch loss: 1.0853484869003296 | avg loss: 1.0928344034567112\n",
      "batch loss: 1.1209535598754883 | avg loss: 1.0929709042160256\n",
      "batch loss: 1.0646320581436157 | avg loss: 1.0928340015779947\n",
      "batch loss: 1.079966425895691 | avg loss: 1.0927721382333682\n",
      "batch loss: 1.1044766902923584 | avg loss: 1.0928281408747988\n",
      "batch loss: 1.0959888696670532 | avg loss: 1.0928431919642858\n",
      "batch loss: 1.0641460418701172 | avg loss: 1.0927071865136024\n",
      "batch loss: 1.1204930543899536 | avg loss: 1.0928382519281135\n",
      "batch loss: 1.0552315711975098 | avg loss: 1.0926616947415848\n",
      "batch loss: 1.0388551950454712 | avg loss: 1.0924102625000143\n",
      "batch loss: 1.0603153705596924 | avg loss: 1.0922609839328499\n",
      "batch loss: 1.0827593803405762 | avg loss: 1.0922169950273302\n",
      "batch loss: 1.0787205696105957 | avg loss: 1.0921547995184973\n",
      "batch loss: 1.0321221351623535 | avg loss: 1.091879420324203\n",
      "batch loss: 1.0960192680358887 | avg loss: 1.0918983237384117\n",
      "batch loss: 1.1323350667953491 | avg loss: 1.0920821271159433\n",
      "batch loss: 1.1732165813446045 | avg loss: 1.0924492513432222\n",
      "batch loss: 1.0312907695770264 | avg loss: 1.0921737626866177\n",
      "batch loss: 1.0443626642227173 | avg loss: 1.0919593631419366\n",
      "batch loss: 1.1441391706466675 | avg loss: 1.092192308711154\n",
      "batch loss: 1.0894452333450317 | avg loss: 1.0921800994873048\n",
      "batch loss: 1.043168067932129 | avg loss: 1.0919632320910428\n",
      "batch loss: 1.0766695737838745 | avg loss: 1.0918958591469583\n",
      "batch loss: 1.104033350944519 | avg loss: 1.0919490937601055\n",
      "batch loss: 1.1541789770126343 | avg loss: 1.092220839975182\n",
      "batch loss: 1.0983166694641113 | avg loss: 1.0922473435816558\n",
      "batch loss: 1.1098217964172363 | avg loss: 1.0923234234640609\n",
      "batch loss: 1.088189721107483 | avg loss: 1.0923056057814895\n",
      "batch loss: 1.1645601987838745 | avg loss: 1.0926157113308559\n",
      "batch loss: 1.146629810333252 | avg loss: 1.0928465408137722\n",
      "batch loss: 1.1010717153549194 | avg loss: 1.0928815415565003\n",
      "batch loss: 1.1064702272415161 | avg loss: 1.0929391207331318\n",
      "batch loss: 1.120913028717041 | avg loss: 1.0930571541001526\n",
      "batch loss: 1.0710115432739258 | avg loss: 1.0929645254832356\n",
      "batch loss: 1.078187108039856 | avg loss: 1.0929026952847278\n",
      "batch loss: 1.1261931657791138 | avg loss: 1.0930414055784543\n",
      "batch loss: 1.0873361825942993 | avg loss: 1.0930177324540387\n",
      "batch loss: 1.0792757272720337 | avg loss: 1.0929609473086586\n",
      "batch loss: 1.0994306802749634 | avg loss: 1.0929875717241577\n",
      "batch loss: 1.0950597524642944 | avg loss: 1.0929960642681746\n",
      "batch loss: 1.1096150875091553 | avg loss: 1.093063897016097\n",
      "batch loss: 1.1061488389968872 | avg loss: 1.0931170878371572\n",
      "batch loss: 1.1344190835952759 | avg loss: 1.0932843023948824\n",
      "batch loss: 1.1069527864456177 | avg loss: 1.0933394172499258\n",
      "batch loss: 1.0951851606369019 | avg loss: 1.0933468298739697\n",
      "batch loss: 1.082425832748413 | avg loss: 1.0933031458854676\n",
      "batch loss: 1.0945473909378052 | avg loss: 1.0933081030370704\n",
      "batch loss: 1.0605660676956177 | avg loss: 1.093178174325398\n",
      "batch loss: 1.1178855895996094 | avg loss: 1.0932758320932803\n",
      "batch loss: 1.083089828491211 | avg loss: 1.0932357297168942\n",
      "batch loss: 1.0955792665481567 | avg loss: 1.093244920057409\n",
      "batch loss: 1.083192229270935 | avg loss: 1.0932056517340243\n",
      "batch loss: 1.0732122659683228 | avg loss: 1.0931278564586713\n",
      "batch loss: 1.083359718322754 | avg loss: 1.0930899954581446\n",
      "batch loss: 1.038387417793274 | avg loss: 1.0928787885945737\n",
      "batch loss: 1.1040133237838745 | avg loss: 1.0929216137299171\n",
      "batch loss: 1.1204566955566406 | avg loss: 1.0930271121277206\n",
      "batch loss: 1.1179916858673096 | avg loss: 1.0931223967603145\n",
      "batch loss: 1.0695381164550781 | avg loss: 1.0930327226907128\n",
      "batch loss: 1.0562305450439453 | avg loss: 1.092893320502657\n",
      "batch loss: 1.0933175086975098 | avg loss: 1.092894921212826\n",
      "batch loss: 1.0811764001846313 | avg loss: 1.0928508666224945\n",
      "batch loss: 1.0849924087524414 | avg loss: 1.092821434196015\n",
      "batch loss: 1.0781015157699585 | avg loss: 1.0927665091272611\n",
      "batch loss: 1.0950398445129395 | avg loss: 1.0927749601881744\n",
      "batch loss: 1.0951039791107178 | avg loss: 1.0927835861841837\n",
      "batch loss: 1.0701240301132202 | avg loss: 1.0926999715861359\n",
      "batch loss: 1.0986716747283936 | avg loss: 1.0927219263771002\n",
      "batch loss: 1.0750396251678467 | avg loss: 1.0926571560430003\n",
      "batch loss: 1.1165374517440796 | avg loss: 1.0927443104068728\n",
      "batch loss: 1.09958016872406 | avg loss: 1.0927691680734808\n",
      "batch loss: 1.0699079036712646 | avg loss: 1.0926863374053568\n",
      "batch loss: 1.1364469528198242 | avg loss: 1.0928443179664198\n",
      "batch loss: 1.1451306343078613 | avg loss: 1.0930323982410293\n",
      "batch loss: 1.0868099927902222 | avg loss: 1.093010095712532\n",
      "batch loss: 1.1266789436340332 | avg loss: 1.0931303415979658\n",
      "batch loss: 1.1000289916992188 | avg loss: 1.093154891954198\n",
      "batch loss: 1.1126339435577393 | avg loss: 1.0932239666052743\n",
      "batch loss: 1.095455527305603 | avg loss: 1.0932318519787738\n",
      "batch loss: 1.0415265560150146 | avg loss: 1.093049791077493\n",
      "batch loss: 1.0915684700012207 | avg loss: 1.0930445934596815\n",
      "batch loss: 1.0864108800888062 | avg loss: 1.0930213986576853\n",
      "batch loss: 1.1108887195587158 | avg loss: 1.0930836541312081\n",
      "batch loss: 1.042570948600769 | avg loss: 1.0929082627925608\n",
      "batch loss: 1.104707956314087 | avg loss: 1.0929490921819087\n",
      "batch loss: 1.0701748132705688 | avg loss: 1.0928705601856625\n",
      "batch loss: 1.135231852531433 | avg loss: 1.0930161316370226\n",
      "batch loss: 1.0926213264465332 | avg loss: 1.0930147795644525\n",
      "batch loss: 1.0723551511764526 | avg loss: 1.0929442688873603\n",
      "batch loss: 1.1222312450408936 | avg loss: 1.0930438844525083\n",
      "batch loss: 1.1029314994812012 | avg loss: 1.0930774017915887\n",
      "batch loss: 1.080767035484314 | avg loss: 1.0930358127162263\n",
      "batch loss: 1.0948132276535034 | avg loss: 1.0930417972783046\n",
      "batch loss: 1.041050910949707 | avg loss: 1.0928673312167994\n",
      "batch loss: 1.108616590499878 | avg loss: 1.0929200043247695\n",
      "batch loss: 1.109533667564392 | avg loss: 1.0929753832022349\n",
      "batch loss: 1.05530846118927 | avg loss: 1.0928502439264443\n",
      "batch loss: 1.0819673538208008 | avg loss: 1.092814207866492\n",
      "batch loss: 1.1341559886932373 | avg loss: 1.0929506493873722\n",
      "batch loss: 1.1080340147018433 | avg loss: 1.0930002657206435\n",
      "batch loss: 1.0948189496994019 | avg loss: 1.0930062286189346\n",
      "batch loss: 1.133636474609375 | avg loss: 1.0931390072006026\n",
      "batch loss: 1.0940927267074585 | avg loss: 1.0931421137788009\n",
      "batch loss: 1.0952776670455933 | avg loss: 1.0931490473933034\n",
      "batch loss: 1.05912184715271 | avg loss: 1.0930389270041754\n",
      "batch loss: 1.119807481765747 | avg loss: 1.0931252771808255\n",
      "batch loss: 1.1065080165863037 | avg loss: 1.093168308497242\n",
      "batch loss: 1.0938687324523926 | avg loss: 1.093170553445816\n",
      "batch loss: 1.095681071281433 | avg loss: 1.093178574269572\n",
      "batch loss: 1.0728942155838013 | avg loss: 1.093113974401146\n",
      "batch loss: 1.0618582963943481 | avg loss: 1.0930147500265213\n",
      "batch loss: 1.0947449207305908 | avg loss: 1.0930202252502683\n",
      "batch loss: 1.1198737621307373 | avg loss: 1.0931049367230774\n",
      "batch loss: 1.1065400838851929 | avg loss: 1.0931471856135242\n",
      "batch loss: 1.0952411890029907 | avg loss: 1.0931537498874724\n",
      "batch loss: 1.10615873336792 | avg loss: 1.0931943904608488\n",
      "batch loss: 1.0476813316345215 | avg loss: 1.0930526052308602\n",
      "batch loss: 1.073979139328003 | avg loss: 1.0929933708647024\n",
      "batch loss: 1.1284245252609253 | avg loss: 1.0931030648411613\n",
      "batch loss: 1.1197710037231445 | avg loss: 1.0931853732945007\n",
      "batch loss: 1.0480552911758423 | avg loss: 1.0930465115033663\n",
      "batch loss: 1.1074365377426147 | avg loss: 1.0930906526881492\n",
      "batch loss: 1.1055407524108887 | avg loss: 1.0931287263876073\n",
      "batch loss: 1.0907902717590332 | avg loss: 1.093121596952764\n",
      "batch loss: 1.0591983795166016 | avg loss: 1.0930184868693713\n",
      "batch loss: 1.110106348991394 | avg loss: 1.093070268269741\n",
      "batch loss: 1.1180169582366943 | avg loss: 1.093145635913146\n",
      "batch loss: 1.094930648803711 | avg loss: 1.093151012457997\n",
      "batch loss: 1.084986925125122 | avg loss: 1.0931264956792195\n",
      "batch loss: 1.0895518064498901 | avg loss: 1.0931157930168562\n",
      "batch loss: 1.0949066877365112 | avg loss: 1.0931211389712434\n",
      "batch loss: 1.0658512115478516 | avg loss: 1.0930399784729594\n",
      "batch loss: 1.1172912120819092 | avg loss: 1.0931119405904934\n",
      "batch loss: 1.0376914739608765 | avg loss: 1.092947974712891\n",
      "batch loss: 1.017882227897644 | avg loss: 1.09272654183143\n",
      "batch loss: 1.1275893449783325 | avg loss: 1.0928290794877444\n",
      "batch loss: 1.0949417352676392 | avg loss: 1.0928352749592398\n",
      "batch loss: 1.0819021463394165 | avg loss: 1.0928033067469012\n",
      "batch loss: 1.0784525871276855 | avg loss: 1.0927614679141922\n",
      "batch loss: 1.0579888820648193 | avg loss: 1.0926603848157928\n",
      "batch loss: 1.0846861600875854 | avg loss: 1.0926372711209282\n",
      "batch loss: 1.109226107597351 | avg loss: 1.092685215735022\n",
      "batch loss: 1.076806902885437 | avg loss: 1.0926394569083662\n",
      "batch loss: 1.105833888053894 | avg loss: 1.0926773719403935\n",
      "batch loss: 1.090968370437622 | avg loss: 1.0926724750879502\n",
      "batch loss: 1.0971038341522217 | avg loss: 1.092685136113848\n",
      "batch loss: 1.0509425401687622 | avg loss: 1.0925662113390757\n",
      "batch loss: 1.0457961559295654 | avg loss: 1.0924333418634804\n",
      "batch loss: 1.0401248931884766 | avg loss: 1.0922851592893303\n",
      "batch loss: 1.131226897239685 | avg loss: 1.0923951641987946\n",
      "batch loss: 1.0742701292037964 | avg loss: 1.092344107762189\n",
      "batch loss: 1.1000301837921143 | avg loss: 1.0923656978633967\n",
      "batch loss: 1.0262949466705322 | avg loss: 1.0921806257312037\n",
      "batch loss: 1.0568543672561646 | avg loss: 1.0920819490315528\n",
      "batch loss: 1.094436526298523 | avg loss: 1.0920885077426028\n",
      "batch loss: 1.0537865161895752 | avg loss: 1.0919821133216223\n",
      "batch loss: 1.049292802810669 | avg loss: 1.0918638603839188\n",
      "batch loss: 1.111177682876587 | avg loss: 1.0919172134847273\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 0:26:06\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.39\n",
      "  Validation took: 0:01:48\n",
      "\n",
      "======= Epoch 5 / 5 =======\n",
      "batch loss: 1.1073096990585327 | avg loss: 1.1073096990585327\n",
      "batch loss: 1.090926170349121 | avg loss: 1.099117934703827\n",
      "batch loss: 1.0801589488983154 | avg loss: 1.0927982727686565\n",
      "batch loss: 1.0604404211044312 | avg loss: 1.0847088098526\n",
      "batch loss: 1.04441499710083 | avg loss: 1.076650047302246\n",
      "batch loss: 1.0652700662612915 | avg loss: 1.0747533837954204\n",
      "batch loss: 1.052970051765442 | avg loss: 1.071641479219709\n",
      "batch loss: 1.1083070039749146 | avg loss: 1.0762246698141098\n",
      "batch loss: 1.1147531270980835 | avg loss: 1.080505609512329\n",
      "batch loss: 1.0806487798690796 | avg loss: 1.080519926548004\n",
      "batch loss: 1.0680527687072754 | avg loss: 1.0793865485624834\n",
      "batch loss: 1.1158779859542847 | avg loss: 1.0824275016784668\n",
      "batch loss: 1.0143989324569702 | avg loss: 1.0771945348152747\n",
      "batch loss: 1.0918090343475342 | avg loss: 1.0782384276390076\n",
      "batch loss: 1.155235767364502 | avg loss: 1.0833715836207072\n",
      "batch loss: 1.1123886108398438 | avg loss: 1.0851851478219032\n",
      "batch loss: 1.0806330442428589 | avg loss: 1.084917377023136\n",
      "batch loss: 1.1101486682891846 | avg loss: 1.0863191154268053\n",
      "batch loss: 1.0986227989196777 | avg loss: 1.086966677715904\n",
      "batch loss: 1.0311092138290405 | avg loss: 1.0841738045215608\n",
      "batch loss: 1.1462348699569702 | avg loss: 1.0871290933518183\n",
      "batch loss: 1.1412277221679688 | avg loss: 1.0895881219343706\n",
      "batch loss: 1.120959758758545 | avg loss: 1.0909521061441172\n",
      "batch loss: 1.1158859729766846 | avg loss: 1.091991017262141\n",
      "batch loss: 1.0874896049499512 | avg loss: 1.0918109607696533\n",
      "batch loss: 1.0334229469299316 | avg loss: 1.0895652679296641\n",
      "batch loss: 1.0706079006195068 | avg loss: 1.088863143214473\n",
      "batch loss: 1.0573543310165405 | avg loss: 1.0877378284931183\n",
      "batch loss: 1.131972312927246 | avg loss: 1.089263155542571\n",
      "batch loss: 1.0702158212661743 | avg loss: 1.0886282444000244\n",
      "batch loss: 1.010688304901123 | avg loss: 1.0861140528032858\n",
      "batch loss: 1.0572566986083984 | avg loss: 1.0852122604846954\n",
      "batch loss: 1.13790762424469 | avg loss: 1.0868090896895437\n",
      "batch loss: 1.144334316253662 | avg loss: 1.0885010081179\n",
      "batch loss: 1.0620229244232178 | avg loss: 1.0877444914409093\n",
      "batch loss: 1.0156503915786743 | avg loss: 1.0857418775558472\n",
      "batch loss: 1.0935510396957397 | avg loss: 1.0859529359920606\n",
      "batch loss: 1.0693703889846802 | avg loss: 1.0855165531760769\n",
      "batch loss: 1.0448837280273438 | avg loss: 1.0844746858645709\n",
      "batch loss: 1.135632872581482 | avg loss: 1.0857536405324937\n",
      "batch loss: 1.0616166591644287 | avg loss: 1.085164933669858\n",
      "batch loss: 1.1046618223190308 | avg loss: 1.085629145304362\n",
      "batch loss: 1.0372991561889648 | avg loss: 1.0845051920691202\n",
      "batch loss: 1.1232177019119263 | avg loss: 1.0853850218382748\n",
      "batch loss: 1.0739483833312988 | avg loss: 1.0851308743158976\n",
      "batch loss: 1.1043047904968262 | avg loss: 1.0855476985807004\n",
      "batch loss: 1.0246278047561646 | avg loss: 1.0842515306269869\n",
      "batch loss: 1.0929183959960938 | avg loss: 1.0844320903221767\n",
      "batch loss: 1.0872629880905151 | avg loss: 1.0844898637460203\n",
      "batch loss: 1.0912002325057983 | avg loss: 1.0846240711212158\n",
      "batch loss: 1.0680298805236816 | avg loss: 1.0842986948349898\n",
      "batch loss: 1.1544843912124634 | avg loss: 1.0856484197653258\n",
      "batch loss: 1.0929443836212158 | avg loss: 1.08578607946072\n",
      "batch loss: 1.1177791357040405 | avg loss: 1.0863785434652258\n",
      "batch loss: 1.1289901733398438 | avg loss: 1.087153300372037\n",
      "batch loss: 1.091202974319458 | avg loss: 1.087225615978241\n",
      "batch loss: 1.0629241466522217 | avg loss: 1.086799274411118\n",
      "batch loss: 1.0391886234283447 | avg loss: 1.0859784011183113\n",
      "batch loss: 1.0797568559646606 | avg loss: 1.0858729512004528\n",
      "batch loss: 1.1641875505447388 | avg loss: 1.0871781945228576\n",
      "batch loss: 1.07997465133667 | avg loss: 1.0870601036509528\n",
      "batch loss: 1.0996702909469604 | avg loss: 1.0872634937686305\n",
      "batch loss: 1.115693211555481 | avg loss: 1.0877147591303264\n",
      "batch loss: 1.1374963521957397 | avg loss: 1.0884925965219736\n",
      "batch loss: 1.1259236335754395 | avg loss: 1.0890684586304884\n",
      "batch loss: 1.0868134498596191 | avg loss: 1.08903429183093\n",
      "batch loss: 1.0461429357528687 | avg loss: 1.0883941223372275\n",
      "batch loss: 1.0631903409957886 | avg loss: 1.088023478493971\n",
      "batch loss: 1.0581673383712769 | avg loss: 1.0875907808110334\n",
      "batch loss: 1.072727918624878 | avg loss: 1.087378454208374\n",
      "batch loss: 1.101675033569336 | avg loss: 1.0875798144810636\n",
      "batch loss: 1.1006791591644287 | avg loss: 1.087761749823888\n",
      "batch loss: 1.0493298768997192 | avg loss: 1.0872352858112282\n",
      "batch loss: 1.114081859588623 | avg loss: 1.0875980773487606\n",
      "batch loss: 1.144362449645996 | avg loss: 1.0883549356460571\n",
      "batch loss: 1.0195916891098022 | avg loss: 1.0874501560863696\n",
      "batch loss: 1.1183091402053833 | avg loss: 1.0878509221138892\n",
      "batch loss: 1.12196683883667 | avg loss: 1.0882883056616173\n",
      "batch loss: 1.1518667936325073 | avg loss: 1.0890930966485906\n",
      "batch loss: 1.1222057342529297 | avg loss: 1.0895070046186448\n",
      "batch loss: 1.1008864641189575 | avg loss: 1.0896474917729695\n",
      "batch loss: 1.0802333354949951 | avg loss: 1.0895326849890918\n",
      "batch loss: 1.0678001642227173 | avg loss: 1.089270847389497\n",
      "batch loss: 1.0160061120986938 | avg loss: 1.0883986481598444\n",
      "batch loss: 1.0797793865203857 | avg loss: 1.0882972450817332\n",
      "batch loss: 1.1352362632751465 | avg loss: 1.088843047618866\n",
      "batch loss: 1.1163536310195923 | avg loss: 1.0891592612211731\n",
      "batch loss: 1.16264009475708 | avg loss: 1.089994270693172\n",
      "batch loss: 1.1113367080688477 | avg loss: 1.0902340733603146\n",
      "batch loss: 1.111092209815979 | avg loss: 1.090465830432044\n",
      "batch loss: 1.1204179525375366 | avg loss: 1.0907949746310055\n",
      "batch loss: 1.1101250648498535 | avg loss: 1.0910050843072974\n",
      "batch loss: 1.1471155881881714 | avg loss: 1.0916084230587046\n",
      "batch loss: 1.0590052604675293 | avg loss: 1.0912615809034794\n",
      "batch loss: 1.0544769763946533 | avg loss: 1.0908743745402285\n",
      "batch loss: 1.1135050058364868 | avg loss: 1.091110110282898\n",
      "batch loss: 1.1312235593795776 | avg loss: 1.0915236509952349\n",
      "batch loss: 1.0725641250610352 | avg loss: 1.0913301864448859\n",
      "batch loss: 1.1090188026428223 | avg loss: 1.091508859335774\n",
      "batch loss: 1.0719982385635376 | avg loss: 1.0913137531280517\n",
      "batch loss: 1.0960217714309692 | avg loss: 1.0913603671706549\n",
      "batch loss: 1.1123945713043213 | avg loss: 1.0915665848582399\n",
      "batch loss: 1.0839329957962036 | avg loss: 1.0914924723430746\n",
      "batch loss: 1.0616462230682373 | avg loss: 1.0912054891769702\n",
      "batch loss: 1.0724941492080688 | avg loss: 1.0910272859391712\n",
      "batch loss: 1.0566188097000122 | avg loss: 1.090702677672764\n",
      "batch loss: 1.130061149597168 | avg loss: 1.0910705138589734\n",
      "batch loss: 1.050184726715088 | avg loss: 1.0906919417557892\n",
      "batch loss: 1.1262856721878052 | avg loss: 1.0910184897414041\n",
      "batch loss: 1.067166805267334 | avg loss: 1.0908016562461853\n",
      "batch loss: 1.0710806846618652 | avg loss: 1.0906239898355157\n",
      "batch loss: 1.0241142511367798 | avg loss: 1.0900301528828484\n",
      "batch loss: 1.1363714933395386 | avg loss: 1.0904402532408723\n",
      "batch loss: 1.0433979034423828 | avg loss: 1.0900276010496575\n",
      "batch loss: 1.140586495399475 | avg loss: 1.090467243609221\n",
      "batch loss: 1.1405938863754272 | avg loss: 1.0908993698399643\n",
      "batch loss: 1.0772242546081543 | avg loss: 1.0907824885131967\n",
      "batch loss: 1.1194185018539429 | avg loss: 1.0910251665923556\n",
      "batch loss: 1.0584717988967896 | avg loss: 1.0907516088806282\n",
      "batch loss: 1.1522890329360962 | avg loss: 1.0912644207477569\n",
      "batch loss: 1.100732684135437 | avg loss: 1.0913426708583989\n",
      "batch loss: 1.1003491878509521 | avg loss: 1.091416494768174\n",
      "batch loss: 1.088907241821289 | avg loss: 1.0913960943377115\n",
      "batch loss: 1.0959073305130005 | avg loss: 1.091432475274609\n",
      "batch loss: 1.0627554655075073 | avg loss: 1.091203059196472\n",
      "batch loss: 1.0959832668304443 | avg loss: 1.0912409973522974\n",
      "batch loss: 1.092037558555603 | avg loss: 1.0912472694877564\n",
      "batch loss: 1.0623747110366821 | avg loss: 1.0910217026248574\n",
      "batch loss: 1.0265663862228394 | avg loss: 1.090522049009338\n",
      "batch loss: 1.1040575504302979 | avg loss: 1.0906261682510376\n",
      "batch loss: 1.0962461233139038 | avg loss: 1.090669068671365\n",
      "batch loss: 1.07990562915802 | avg loss: 1.0905875274629304\n",
      "batch loss: 1.1342650651931763 | avg loss: 1.0909159300022555\n",
      "batch loss: 1.058011531829834 | avg loss: 1.0906703747920137\n",
      "batch loss: 1.074931263923645 | avg loss: 1.0905537887855812\n",
      "batch loss: 1.117547869682312 | avg loss: 1.0907522746745277\n",
      "batch loss: 1.1154749393463135 | avg loss: 1.0909327320808913\n",
      "batch loss: 1.125986099243164 | avg loss: 1.0911867419878643\n",
      "batch loss: 1.0955891609191895 | avg loss: 1.0912184140665069\n",
      "batch loss: 1.0768558979034424 | avg loss: 1.091115824665342\n",
      "batch loss: 1.098000407218933 | avg loss: 1.0911646514919633\n",
      "batch loss: 1.058642029762268 | avg loss: 1.0909356189445711\n",
      "batch loss: 1.0587189197540283 | avg loss: 1.09071032734184\n",
      "batch loss: 1.0584808588027954 | avg loss: 1.0904865115880966\n",
      "batch loss: 1.0784176588058472 | avg loss: 1.0904032781206328\n",
      "batch loss: 1.1450856924057007 | avg loss: 1.0907778152047771\n",
      "batch loss: 1.0871392488479614 | avg loss: 1.09075306305269\n",
      "batch loss: 1.0967493057250977 | avg loss: 1.090793578205882\n",
      "batch loss: 1.1257127523422241 | avg loss: 1.0910279350792802\n",
      "batch loss: 1.1363186836242676 | avg loss: 1.0913298734029133\n",
      "batch loss: 1.0767128467559814 | avg loss: 1.0912330719019403\n",
      "batch loss: 1.105791449546814 | avg loss: 1.0913288507022356\n",
      "batch loss: 1.0577278137207031 | avg loss: 1.0911092360814412\n",
      "batch loss: 1.0382201671600342 | avg loss: 1.0907658005689647\n",
      "batch loss: 1.0870798826217651 | avg loss: 1.0907420204531761\n",
      "batch loss: 1.12471342086792 | avg loss: 1.0909597858404503\n",
      "batch loss: 1.0966531038284302 | avg loss: 1.0909960490123483\n",
      "batch loss: 1.096036434173584 | avg loss: 1.0910279501842548\n",
      "batch loss: 1.086531162261963 | avg loss: 1.090999668499209\n",
      "batch loss: 1.0858436822891235 | avg loss: 1.0909674435853958\n",
      "batch loss: 1.134856939315796 | avg loss: 1.0912400491489387\n",
      "batch loss: 1.105716586112976 | avg loss: 1.091329410488223\n",
      "batch loss: 1.0964477062225342 | avg loss: 1.09136081107555\n",
      "batch loss: 1.1236237287521362 | avg loss: 1.091557536183334\n",
      "batch loss: 1.0943572521209717 | avg loss: 1.0915745041587137\n",
      "batch loss: 1.1333738565444946 | avg loss: 1.0918263074863388\n",
      "batch loss: 1.0771818161010742 | avg loss: 1.0917386159211575\n",
      "batch loss: 1.1029205322265625 | avg loss: 1.091805174946785\n",
      "batch loss: 1.058035135269165 | avg loss: 1.091605352226799\n",
      "batch loss: 1.0488646030426025 | avg loss: 1.0913539360551272\n",
      "batch loss: 1.089744210243225 | avg loss: 1.091344522453888\n",
      "batch loss: 1.0430772304534912 | avg loss: 1.0910638986631882\n",
      "batch loss: 1.052013874053955 | avg loss: 1.0908381759775856\n",
      "batch loss: 1.1352370977401733 | avg loss: 1.091093342194612\n",
      "batch loss: 1.0978924036026 | avg loss: 1.0911321939740861\n",
      "batch loss: 1.1583034992218018 | avg loss: 1.0915138491175391\n",
      "batch loss: 1.0584192276000977 | avg loss: 1.0913268738547288\n",
      "batch loss: 1.06589674949646 | avg loss: 1.0911840079875474\n",
      "batch loss: 1.0288195610046387 | avg loss: 1.0908356032557993\n",
      "batch loss: 1.1061171293258667 | avg loss: 1.0909205006228553\n",
      "batch loss: 1.0959416627883911 | avg loss: 1.0909482418502892\n",
      "batch loss: 1.0969367027282715 | avg loss: 1.0909811454814868\n",
      "batch loss: 1.100195288658142 | avg loss: 1.0910314959906489\n",
      "batch loss: 1.1415412425994873 | avg loss: 1.0913060054830883\n",
      "batch loss: 1.1569043397903442 | avg loss: 1.0916605910739383\n",
      "batch loss: 1.082167387008667 | avg loss: 1.0916095523424045\n",
      "batch loss: 1.0652351379394531 | avg loss: 1.091468512693191\n",
      "batch loss: 1.0876414775848389 | avg loss: 1.0914481561234657\n",
      "batch loss: 1.054352879524231 | avg loss: 1.0912518848187078\n",
      "batch loss: 1.0523416996002197 | avg loss: 1.0910470943701895\n",
      "batch loss: 1.0543110370635986 | avg loss: 1.0908547589916209\n",
      "batch loss: 1.1235506534576416 | avg loss: 1.0910250501086314\n",
      "batch loss: 1.1215198040008545 | avg loss: 1.0911830540148089\n",
      "batch loss: 1.0735048055648804 | avg loss: 1.0910919290227987\n",
      "batch loss: 0.949974775314331 | avg loss: 1.0903682513114734\n",
      "batch loss: 0.9381181001663208 | avg loss: 1.089591464826039\n",
      "batch loss: 0.8543137907981873 | avg loss: 1.088397161912192\n",
      "batch loss: 0.9800939559936523 | avg loss: 1.0878501760237145\n",
      "batch loss: 1.0779545307159424 | avg loss: 1.0878004491628714\n",
      "batch loss: 1.10663902759552 | avg loss: 1.0878946420550346\n",
      "batch loss: 1.096777319908142 | avg loss: 1.0879388344821646\n",
      "batch loss: 1.088749647140503 | avg loss: 1.0879428484062157\n",
      "batch loss: 1.0922303199768066 | avg loss: 1.0879639689558245\n",
      "batch loss: 1.0863779783248901 | avg loss: 1.0879561944919474\n",
      "batch loss: 1.083927869796753 | avg loss: 1.0879365441275806\n",
      "batch loss: 1.1141119003295898 | avg loss: 1.0880636089635127\n",
      "batch loss: 1.0642788410186768 | avg loss: 1.0879487067029097\n",
      "batch loss: 1.078747034072876 | avg loss: 1.0879044678921883\n",
      "batch loss: 1.1017255783081055 | avg loss: 1.087970597607097\n",
      "batch loss: 1.0928874015808105 | avg loss: 1.0879940109593527\n",
      "batch loss: 1.0545971393585205 | avg loss: 1.0878357319470267\n",
      "batch loss: 1.1196255683898926 | avg loss: 1.0879856840057194\n",
      "batch loss: 1.0439292192459106 | avg loss: 1.0877788461429971\n",
      "batch loss: 1.0324419736862183 | avg loss: 1.087520262626844\n",
      "batch loss: 1.0484116077423096 | avg loss: 1.087338361906451\n",
      "batch loss: 1.0576879978179932 | avg loss: 1.0872010917023376\n",
      "batch loss: 1.0668394565582275 | avg loss: 1.0871072592823188\n",
      "batch loss: 1.0162806510925293 | avg loss: 1.0867823665842005\n",
      "batch loss: 1.0925822257995605 | avg loss: 1.086808849959613\n",
      "batch loss: 1.140208125114441 | avg loss: 1.0870515739375894\n",
      "batch loss: 1.2007336616516113 | avg loss: 1.0875659725245308\n",
      "batch loss: 0.969290018081665 | avg loss: 1.0870331979549683\n",
      "batch loss: 1.003836989402771 | avg loss: 1.0866601207865727\n",
      "batch loss: 1.1321470737457275 | avg loss: 1.0868631875408548\n",
      "batch loss: 1.027717113494873 | avg loss: 1.0866003161006503\n",
      "batch loss: 1.0231804847717285 | avg loss: 1.086319697377956\n",
      "batch loss: 0.9216694831848145 | avg loss: 1.0855943660378982\n",
      "batch loss: 1.1297332048416138 | avg loss: 1.08578795743616\n",
      "batch loss: 1.5261576175689697 | avg loss: 1.0877109690524605\n",
      "batch loss: 1.3577967882156372 | avg loss: 1.0888852552227353\n",
      "batch loss: 1.0738202333450317 | avg loss: 1.08882003867781\n",
      "batch loss: 1.075219988822937 | avg loss: 1.0887614177732632\n",
      "batch loss: 1.1953176259994507 | avg loss: 1.08921874055535\n",
      "batch loss: 1.1316050291061401 | avg loss: 1.0893998785406096\n",
      "batch loss: 1.0534294843673706 | avg loss: 1.0892468130334896\n",
      "batch loss: 1.0974019765853882 | avg loss: 1.0892813688112517\n",
      "batch loss: 1.0958555936813354 | avg loss: 1.0893091081566952\n",
      "batch loss: 1.014114499092102 | avg loss: 1.0889931644211297\n",
      "batch loss: 1.0440126657485962 | avg loss: 1.0888049614978137\n",
      "batch loss: 1.0780376195907593 | avg loss: 1.0887600975732008\n",
      "batch loss: 1.0350830554962158 | avg loss: 1.0885373712575288\n",
      "batch loss: 1.006912350654602 | avg loss: 1.0882000777839629\n",
      "batch loss: 1.0752639770507812 | avg loss: 1.0881468428015217\n",
      "batch loss: 1.083908200263977 | avg loss: 1.0881294713157121\n",
      "batch loss: 1.0566213130950928 | avg loss: 1.0880008665882812\n",
      "batch loss: 1.0441151857376099 | avg loss: 1.0878224695116523\n",
      "batch loss: 1.0495928525924683 | avg loss: 1.0876676937346517\n",
      "batch loss: 1.0598729848861694 | avg loss: 1.0875556182957464\n",
      "batch loss: 0.9524056315422058 | avg loss: 1.0870128472646077\n",
      "batch loss: 1.0108722448349 | avg loss: 1.086708284854889\n",
      "batch loss: 1.0569331645965576 | avg loss: 1.0865896588777642\n",
      "batch loss: 1.0687819719314575 | avg loss: 1.0865189934533739\n",
      "batch loss: 1.0400253534317017 | avg loss: 1.086335224125225\n",
      "batch loss: 0.9420233964920044 | avg loss: 1.0857670673235196\n",
      "batch loss: 0.9493946433067322 | avg loss: 1.0852322735038458\n",
      "batch loss: 0.9346526265144348 | avg loss: 1.0846440717577934\n",
      "batch loss: 0.823637843132019 | avg loss: 1.0836284833195609\n",
      "batch loss: 0.9791520833969116 | avg loss: 1.083223536033039\n",
      "batch loss: 0.7439006567001343 | avg loss: 1.0819134090858076\n",
      "batch loss: 1.1059644222259521 | avg loss: 1.0820059129825006\n",
      "batch loss: 1.0213596820831299 | avg loss: 1.081773551944572\n",
      "batch loss: 1.1693899631500244 | avg loss: 1.0821079657277988\n",
      "batch loss: 0.9814100861549377 | avg loss: 1.0817250840564192\n",
      "batch loss: 0.9975098967552185 | avg loss: 1.0814060871348237\n",
      "batch loss: 1.1925469636917114 | avg loss: 1.0818254866690007\n",
      "batch loss: 0.7923314571380615 | avg loss: 1.0807371632497114\n",
      "batch loss: 0.8740267753601074 | avg loss: 1.079962967040387\n",
      "batch loss: 1.0918793678283691 | avg loss: 1.0800074312224317\n",
      "batch loss: 0.9527992010116577 | avg loss: 1.079534538173321\n",
      "batch loss: 0.9662889838218689 | avg loss: 1.0791151101942416\n",
      "batch loss: 0.9190965294837952 | avg loss: 1.0785246357266753\n",
      "batch loss: 1.1617738008499146 | avg loss: 1.0788306988337462\n",
      "batch loss: 0.9935087561607361 | avg loss: 1.0785181642452002\n",
      "batch loss: 0.9717357754707336 | avg loss: 1.078128447497848\n",
      "batch loss: 1.2429486513137817 | avg loss: 1.0787277936935424\n",
      "batch loss: 1.084079384803772 | avg loss: 1.0787471835164055\n",
      "batch loss: 1.350460171699524 | avg loss: 1.0797280968311462\n",
      "batch loss: 1.359427571296692 | avg loss: 1.0807342100486481\n",
      "batch loss: 0.9261155128479004 | avg loss: 1.0801800211697923\n",
      "batch loss: 0.9947635531425476 | avg loss: 1.0798749623554094\n",
      "batch loss: 1.1062861680984497 | avg loss: 1.0799689524114344\n",
      "batch loss: 1.1152549982070923 | avg loss: 1.080094080233405\n",
      "batch loss: 1.2016795873641968 | avg loss: 1.0805237110006514\n",
      "batch loss: 0.8269398212432861 | avg loss: 1.079630809980379\n",
      "batch loss: 0.8635631799697876 | avg loss: 1.0788726779452542\n",
      "batch loss: 1.0846725702285767 | avg loss: 1.078892957288902\n",
      "batch loss: 1.2468146085739136 | avg loss: 1.0794780501505223\n",
      "batch loss: 0.9215897917747498 | avg loss: 1.078929827031162\n",
      "batch loss: 1.0565739870071411 | avg loss: 1.078852471183328\n",
      "batch loss: 0.8377104997634888 | avg loss: 1.0780209471439492\n",
      "batch loss: 0.9739407300949097 | avg loss: 1.0776632831678359\n",
      "batch loss: 0.9853095412254333 | avg loss: 1.0773470032296768\n",
      "batch loss: 1.0518646240234375 | avg loss: 1.077260032652181\n",
      "batch loss: 1.0940930843353271 | avg loss: 1.077317287930015\n",
      "batch loss: 0.9647641181945801 | avg loss: 1.0769357517614202\n",
      "batch loss: 0.8969565033912659 | avg loss: 1.0763277137601697\n",
      "batch loss: 0.9267648458480835 | avg loss: 1.075824135080331\n",
      "batch loss: 0.7110763192176819 | avg loss: 1.0746001491210604\n",
      "batch loss: 0.874950647354126 | avg loss: 1.0739324250348834\n",
      "batch loss: 0.9095113277435303 | avg loss: 1.073384354710579\n",
      "batch loss: 1.0214622020721436 | avg loss: 1.073211855864604\n",
      "batch loss: 0.8343409895896912 | avg loss: 1.0724208927312433\n",
      "batch loss: 1.1933950185775757 | avg loss: 1.0728201472719905\n",
      "batch loss: 0.9988466501235962 | avg loss: 1.0725768133997917\n",
      "batch loss: 1.1465173959732056 | avg loss: 1.0728192415393767\n",
      "batch loss: 1.0716480016708374 | avg loss: 1.072815413958107\n",
      "batch loss: 0.8798241019248962 | avg loss: 1.072186777762559\n",
      "batch loss: 0.9226205945014954 | avg loss: 1.0717011732714516\n",
      "batch loss: 0.827184796333313 | avg loss: 1.0709098581357295\n",
      "batch loss: 1.0210192203521729 | avg loss: 1.0707489205944922\n",
      "batch loss: 1.2778240442276 | avg loss: 1.0714147570048882\n",
      "batch loss: 0.838534951210022 | avg loss: 1.0706683473709302\n",
      "batch loss: 1.1237683296203613 | avg loss: 1.0708379958765195\n",
      "batch loss: 1.1579463481903076 | avg loss: 1.0711154110112768\n",
      "batch loss: 0.8639994859695435 | avg loss: 1.0704579001381285\n",
      "batch loss: 0.9621188640594482 | avg loss: 1.0701150550872465\n",
      "batch loss: 1.1103408336639404 | avg loss: 1.0702419502878038\n",
      "batch loss: 0.8847874999046326 | avg loss: 1.0696587601922594\n",
      "batch loss: 1.0390799045562744 | avg loss: 1.0695629017106418\n",
      "batch loss: 1.158028483390808 | avg loss: 1.0698393566533924\n",
      "batch loss: 0.957867443561554 | avg loss: 1.0694905344942278\n",
      "batch loss: 0.875968337059021 | avg loss: 1.0688895338810749\n",
      "batch loss: 1.0612132549285889 | avg loss: 1.0688657683115625\n",
      "batch loss: 1.0100425481796265 | avg loss: 1.0686842151630072\n",
      "batch loss: 0.9189239740371704 | avg loss: 1.0682234144210816\n",
      "batch loss: 1.2214760780334473 | avg loss: 1.0686935146162115\n",
      "batch loss: 1.0275352001190186 | avg loss: 1.0685676482110213\n",
      "batch loss: 0.8877481818199158 | avg loss: 1.0680163693500728\n",
      "batch loss: 0.9005908370018005 | avg loss: 1.0675074771544855\n",
      "batch loss: 0.9459255933761597 | avg loss: 1.067139047203642\n",
      "batch loss: 1.3099220991134644 | avg loss: 1.067872530744155\n",
      "batch loss: 1.3070813417434692 | avg loss: 1.0685930392110203\n",
      "batch loss: 1.0296614170074463 | avg loss: 1.0684761274326313\n",
      "batch loss: 1.0757465362548828 | avg loss: 1.0684978951237158\n",
      "batch loss: 0.9788928627967834 | avg loss: 1.0682304174152772\n",
      "batch loss: 1.055322527885437 | avg loss: 1.0681920010773909\n",
      "batch loss: 1.0022605657577515 | avg loss: 1.0679963588360863\n",
      "batch loss: 1.0849089622497559 | avg loss: 1.0680463961242923\n",
      "batch loss: 0.7532898783683777 | avg loss: 1.0671179108211777\n",
      "batch loss: 1.0450104475021362 | avg loss: 1.0670528888702393\n",
      "batch loss: 1.2178943157196045 | avg loss: 1.067495239095604\n",
      "batch loss: 1.1264979839324951 | avg loss: 1.067667761741326\n",
      "batch loss: 0.91436767578125 | avg loss: 1.0672208227152031\n",
      "batch loss: 0.8529611825942993 | avg loss: 1.066597974924154\n",
      "batch loss: 0.896427571773529 | avg loss: 1.06610472737879\n",
      "batch loss: 1.1095573902130127 | avg loss: 1.0662303131095248\n",
      "batch loss: 0.9350073337554932 | avg loss: 1.0658521489038935\n",
      "batch loss: 0.9653394818305969 | avg loss: 1.0655633194008092\n",
      "batch loss: 0.7821661233901978 | avg loss: 1.0647512930512086\n",
      "batch loss: 0.8250349760055542 | avg loss: 1.0640663892882212\n",
      "batch loss: 0.83988356590271 | avg loss: 1.0634276917856982\n",
      "batch loss: 0.9659914970397949 | avg loss: 1.063150884414261\n",
      "batch loss: 0.9182959198951721 | avg loss: 1.062740530407125\n",
      "batch loss: 1.1771799325942993 | avg loss: 1.0630638055545463\n",
      "batch loss: 1.1814594268798828 | avg loss: 1.063397314347012\n",
      "batch loss: 0.9545955061912537 | avg loss: 1.0630916912903947\n",
      "batch loss: 0.9473464488983154 | avg loss: 1.0627674749251508\n",
      "batch loss: 0.9327825307846069 | avg loss: 1.062404388489004\n",
      "batch loss: 0.8987308740615845 | avg loss: 1.0619484734070335\n",
      "batch loss: 0.9250086545944214 | avg loss: 1.0615680850214428\n",
      "batch loss: 0.8646310567855835 | avg loss: 1.0610225530872714\n",
      "batch loss: 1.0606244802474976 | avg loss: 1.061021453438543\n",
      "\n",
      "  Average training loss: 1.06\n",
      "  Training epoch took: 0:27:03\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.51\n",
      "  Validation took: 0:01:44\n",
      "\n",
      "Training complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.81      0.62       566\n",
      "           2       0.00      0.00      0.00       463\n",
      "           3       0.51      0.65      0.57       418\n",
      "\n",
      "    accuracy                           0.51      1447\n",
      "   macro avg       0.34      0.49      0.40      1447\n",
      "weighted avg       0.34      0.51      0.41      1447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Definimos la funcion para entrenar el modelo y entregar los resultados en el set de validación\n",
    "#Train model\n",
    "def training(n_epochs, training_dataloader, validation_dataloader):\n",
    "    for epoch_i in range(0, n_epochs):\n",
    "        print('======= Epoch {:} / {:} ======='.format(epoch_i + 1, epochs))\n",
    "        # Mira cuanto tiempo le cuesta entrenar un EPOCH.\n",
    "        t0 = time.time()\n",
    "        # Resetea la perdida para este EPOCH.\n",
    "        total_loss = 0\n",
    "        # Pone el modelo en modo entrenamiento.\n",
    "        model.train()\n",
    "        # Para cada batch en el training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Limpia el gradiente calculado anteriormente\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Genera un paso adelante\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # Saca el loss value fuera del output\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Genera un paso atras\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipea el los gradientes a 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Actualiza los parametros\n",
    "            # ¿take a step using the computed gradient?\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calcula el average loss sobre el training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "        #Validación\n",
    "        # Despues de completar un entrenamiento genera un paso de validacion\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Pone el modelo en modo evaluación\n",
    "        model.eval()\n",
    "\n",
    "        # Trackea las variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evalua el data para un epoch mas\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # El modelo no computa los gradientes\n",
    "            with torch.no_grad():\n",
    "                # Paso adelante \n",
    "                # Devolvemos los loggits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # Los \"logits\" son el valor de salida\n",
    "            # Prioriza aplicar la funcion de activación\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Mueve los logits y labels a la CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Guarda los logits y labels del batch\n",
    "            # Utilizamos esto en la matriz de confusión\n",
    "            predict_labels = np.argmax(logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calcula la precision para este batch\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "            # Accumula la precisión total\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #Print la matriz de confussión\"\n",
    "    conf = confusion_matrix(all_labels, all_logits, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    print(classification_report(all_labels, all_logits, target_names=target_names))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Llamamos a la funcion para entrenar el modelo\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
