{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4340, 2)\n",
      "(1447, 2)\n"
     ]
    }
   ],
   "source": [
    "#Model ROBERTA Spanish\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string \n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MAX_LEN = 32\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "df_train = pd.read_csv('/Users/nfanlo/dev/spanish-classifier-tfg/dataset/60-20-20/train.csv')\n",
    "print(df_train.shape)\n",
    "df_train.isnull().sum()\n",
    "df_train.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df_train.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df_train.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df_train.head()\n",
    "df_train['review'] = df_train['text']\n",
    "df_train.drop('text', axis=1, inplace=True)\n",
    "df_train['label'] = df_train['sentiment']\n",
    "df_train.drop('sentiment', axis=1, inplace=True)\n",
    "\n",
    "df_dev = pd.read_csv('/Users/nfanlo/dev/spanish-classifier-tfg/dataset/60-20-20/dev.csv')\n",
    "print(df_dev.shape)\n",
    "df_dev.isnull().sum()\n",
    "df_dev.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df_dev.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df_dev.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df_dev['review'] = df_dev['text']\n",
    "df_dev.drop('text', axis=1, inplace=True)\n",
    "df_dev['label'] = df_dev['sentiment']\n",
    "df_dev.drop('sentiment', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Environment stopwords for train\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df_train['review']=df_train['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment stopwords for dev\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df_dev['review']=df_dev['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 0]\n",
    "y_train = df_train.iloc[:, 1]\n",
    "X_dev = df_dev.iloc[:, 0]\n",
    "y_dev = df_dev.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 32\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base',\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=MAX_LEN,\n",
    "                   truncation=True ,pad_to_max_length=True,\n",
    "                   return_token_type_ids = False,\n",
    "                   return_attention_mask = True)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_dev_inputs, X_dev_masks = preprocessing(X_dev)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "batch_size = 8\n",
    "\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "y_dev_labels = torch.tensor(y_dev.values)\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks, y_train_labels)\n",
    "val_dataloader = dataloader(X_dev_inputs, X_dev_masks, y_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el modelo + optimizador + definimos EPOCHS + Scheduler\n",
    "#Modelo\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3,\n",
    " output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-6)\n",
    "\n",
    "epochs=4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion para formatear el tiempo y otra para calcular la exactitud\n",
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Epoch 1 / 4 =======\n",
      "======= Epoch 2 / 4 =======\n",
      "======= Epoch 3 / 4 =======\n",
      "======= Epoch 4 / 4 =======\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      1.00      0.59       599\n",
      "           2       0.00      0.00      0.00       440\n",
      "           3       0.00      0.00      0.00       408\n",
      "\n",
      "    accuracy                           0.41      1447\n",
      "   macro avg       0.14      0.33      0.20      1447\n",
      "weighted avg       0.17      0.41      0.24      1447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Definimos la funcion para entrenar el modelo y entregar los resultados en el set de validación\n",
    "#Train model\n",
    "def training(n_epochs, training_dataloader, validation_dataloader):\n",
    "    for epoch_i in range(0, n_epochs):\n",
    "        print('======= Epoch {:} / {:} ======='.format(epoch_i + 1, epochs))\n",
    "        # Mira cuanto tiempo le cuesta entrenar un EPOCH.\n",
    "        t0 = time.time()\n",
    "        # Resetea la perdida para este EPOCH.\n",
    "        total_loss = 0\n",
    "        # Pone el modelo en modo entrenamiento.\n",
    "        model.train()\n",
    "        # Para cada batch en el training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Limpia el gradiente calculado anteriormente\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Genera un paso adelante\n",
    "            outputs = model(b_input_ids,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # Saca el loss value fuera del output\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Genera un paso atras\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipea el los gradientes a 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Actualiza los parametros\n",
    "            # ¿take a step using the computed gradient?\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calcula el average loss sobre el training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        #Validación\n",
    "        # Despues de completar un entrenamiento genera un paso de validacion\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Pone el modelo en modo evaluación\n",
    "        model.eval()\n",
    "\n",
    "        # Trackea las variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evalua el data para un epoch mas\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # El modelo no computa los gradientes\n",
    "            with torch.no_grad():\n",
    "                # Paso adelante \n",
    "                # Devolvemos los loggits \n",
    "                outputs = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # Los \"logits\" son el valor de salida\n",
    "            # Prioriza aplicar la funcion de activación\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Mueve los logits y labels a la CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Guarda los logits y labels del batch\n",
    "            # Utilizamos esto en la matriz de confusión\n",
    "            predict_labels = np.argmax(logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calcula la precision para este batch\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "            # Accumula la precisión total\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #Print la matriz de confussión\"\n",
    "    conf = confusion_matrix(all_labels, all_logits, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    print(classification_report(all_labels, all_logits, target_names=target_names))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Llamamos a la funcion para entrenar el modelo\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
