{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show dev Predictions Stats from the Best Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dev_predictions.tsv` file is found in the `best_model` directory and has the following format:\n",
    "\n",
    "        text    labels  predicted_labels        loss    probabilities   label   predicted_label\n",
    "0        eso ! en no escuchar el despertador ! y te dormÃ­s nervioso ...con miedo...y peor es cuando te acostumbraste a dormir tarde     2       2       0.05242778      [0.00769282 0.04338438 0.9489229 ]      N       N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"/Users/fperez/dev/data/spanishclassfier_exp/dccuchile-distilbert-base-spanish-uncased-finetuned-with-spanish-tweets-clf-cleaned-ds/ep_2-lr_5e-5-msl_72-bs_8-ds_config_80-10-10-nl_5-do_0.2/\"\n",
    "dir_src=os.path.join(base_dir, \"best_model\", \"dev_predictions.tsv\")\n",
    "\n",
    "preds_df = pd.read_csv(dir_src, sep=\"\\t\", index_col=0)\n",
    "preds_df.head(32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats on the examples read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total examples: {preds_df.count()[0]}\n",
    "Correctly classified: {preds_df.query('label == predicted_label').count()[0]}\n",
    "Misclassified: {preds_df.query('label != predicted_label').count()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = {\n",
    "    \"padding\": \"max_length\",\n",
    "    \"truncation\": True,\n",
    "    \"max_length\": 72,\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(base_dir, \"best_model\")) #, **tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(os.path.join(base_dir, \"best_model\"))\n",
    "tc_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, padding=\"max_length\", truncation=True, max_length=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_pipe(preds_df.at[1, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df_label_str(pred_label: str):\n",
    "    if pred_label ==\"positivo\":\n",
    "        return \"P\"\n",
    "    if pred_label ==\"negativo\":\n",
    "        return \"N\"\n",
    "    if pred_label ==\"neutral\":\n",
    "        return \"NEU\"\n",
    "    raise ValueError(f\"label str not recognized: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def eval_on_tc_pipeline(df):\n",
    "    for i in range(len(df)):\n",
    "        example = df.at[i, 'text'] \n",
    "        output = tc_pipe(example, return_all_scores=True)\n",
    "        # print(output)\n",
    "        # print(example)\n",
    "        probas = {}\n",
    "        for pred in output[0]:\n",
    "            probas[pred['label']] = pred['score']\n",
    "        probas_arr = np.array([probas[\"P\"], probas[\"NEU\"], probas[\"N\"]])\n",
    "        # print(probas_arr)\n",
    "        df.at[i, 'probabilities_tc_pipe'] = str(probas_arr)\n",
    "        df.at[i, 'predicted_label_tc_pipe'] = pipe.model.config.id2label[int(np.argmax(probas_arr))]\n",
    "        # print(df)\n",
    "    return df\n",
    "\n",
    "preds_df_2 = eval_on_tc_pipeline(preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df_2.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total examples: {preds_df_2.count()[0]}\n",
    "Correctly classified: {preds_df_2.query('label == predicted_label_tc_pipe').count()[0]}\n",
    "Misclassified: {preds_df_2.query('label != predicted_label_tc_pipe').count()[0]}\n",
    "Mismatchs: {preds_df_2.query('predicted_label != predicted_label_tc_pipe').count()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.model_input_names)\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # print(f\"Batch of type {type(batch)}\\n {batch}\")\n",
    "    example = tokenizer(batch.text, padding=\"max_length\", truncation=True, max_length=72)\n",
    "    # print(f\"Tokenized example:\\n{example}\")\n",
    "    # print(\"-----\")\n",
    "    inputs = {k: torch.tensor(v).view(1,-1).to(model.device) for k, v in example.items() if k in tokenizer.model_input_names}\n",
    "    # print(f\"Selected inputs: {inputs}\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        # print(f\"Model predictions:\\n{output}\")\n",
    "        probabilities = torch.softmax(output.logits, dim=-1)\n",
    "        # print(f\"Model probabilities:\\n{probabilities}\")\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        pred_label_str = model.config.id2label[int(pred_label[0].cpu().numpy())]\n",
    "        # print(f\"Predicted label: {pred_label} -> {pred_label_str}\")\n",
    "        loss = cross_entropy(\n",
    "            output.logits[0], torch.tensor(batch['labels']), reduction=\"none\"\n",
    "        )\n",
    "    return {\n",
    "        \"loss_raw_model\": loss.cpu().numpy(),\n",
    "        \"predicted_label_raw_model\": pred_label_str,\n",
    "        \"probabilities_raw_model\": probabilities[0].cpu().numpy(),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "preds_df2 = preds_df_2.copy()\n",
    "\n",
    "preds_df2 = pd.concat([preds_df2, preds_df2[['text', 'labels']].apply(\n",
    "    forward_pass_with_label, axis=1, result_type=\"expand\"\n",
    ")], axis=1)\n",
    "\n",
    "preds_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total examples: {preds_df2.count()[0]}\n",
    "Matched classifications tc_pipe vs raw_model: {preds_df2.query('predicted_label_tc_pipe == predicted_labels_raw_model').count()[0]}\n",
    "Misclassified tc_pipe vs raw_model: {preds_df2.query('predicted_label_tc_pipe != predicted_labels_raw_model').count()[0]}\n",
    "Mismatchs raw model: {preds_df2.query('predicted_label != predicted_labels_raw_model').count()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tc_pipe and raw_model preds they don't match with original predictions and they should!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Misclassified Examples by Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_df = preds_df.query('label != predicted_label').sort_values(by=\"loss\", ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_df.drop('index', axis=1, inplace=True)\n",
    "mislabeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"El sentimiento es [MASK]\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = mislabeled_df.at[0, 'text']\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pipeline=pipeline(\"fill-mask\", model=\"dccuchile/bert-base-spanish-wwm-uncased-finetuned-mldoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def apply_dropout(m):\n",
    "    if type(m) == nn.Dropout:\n",
    "        # print(m)\n",
    "        m.eval()\n",
    "\n",
    "mask_pipeline.model.apply(apply_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_mm(df):\n",
    "    for i in range(len(df)):\n",
    "        example = df.at[i, 'text'] \n",
    "        # example = \"Hola. Que tal estas?\"\n",
    "        example = example + \" \" + prompt\n",
    "        output = mask_pipeline(example, targets=[\"negativo\", \"neutral\", \"positivo\"])\n",
    "        # print(output)\n",
    "        # print(example)\n",
    "        for element in output:\n",
    "\n",
    "            label = df.at[i, 'label']\n",
    "            \n",
    "            if label == \"P\" and element['token_str'] ==\"positivo\":\n",
    "                correct_label = \"(GT)\"\n",
    "            elif label == \"NEU\" and element['token_str'] ==\"neutral\":\n",
    "                correct_label = \"(GT)\"\n",
    "            elif label == \"N\" and element['token_str'] ==\"negativo\":\n",
    "                correct_label = \"(GT)\"\n",
    "            else:\n",
    "                correct_label = \"\"\n",
    "            # print(f\"Token {element['token_str']}\\t{element['score']:.6f}% {correct_label}\")\n",
    "\n",
    "        \n",
    "        df.at[i, 'mm_predicted_label'] = to_df_label_str(output[0]['token_str'])\n",
    "    return df\n",
    "\n",
    "mislabeled_df_ext = eval_on_mm(mislabeled_df)\n",
    "mislabeled_df_ext.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total examples: {mislabeled_df_ext.count()[0]}\n",
    "\\n\n",
    "Correctly classified: {mislabeled_df_ext.query('label == predicted_label').count()[0]}\n",
    "Misclassified: {mislabeled_df_ext.query('label != predicted_label').count()[0]}\n",
    "\\n\n",
    "Correctly classified with masked model: {mislabeled_df_ext.query('label == mm_predicted_label').count()[0]}\n",
    "Misclassified with masked model: {mislabeled_df_ext.query('label != mm_predicted_label').count()[0]}\n",
    "\\n\n",
    "Equally classified with distilbeto and masked model: {mislabeled_df_ext.query('predicted_label == mm_predicted_label').count()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_df_ext.query('label != mm_predicted_label').head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both predictions correct but misclassified!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_df_ext.query('predicted_label == mm_predicted_label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df_ext = eval_on_mm(preds_df)\n",
    "preds_df_ext.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total examples: {preds_df_ext.count()[0]}\n",
    "\\n\n",
    "Correctly classified: {preds_df_ext.query('label == predicted_label').count()[0]}\n",
    "Misclassified: {preds_df_ext.query('label != predicted_label').count()[0]}\n",
    "\\n\n",
    "Correctly classified with masked model: {preds_df_ext.query('label == mm_predicted_label').count()[0]}\n",
    "Misclassified with masked model: {preds_df_ext.query('label != mm_predicted_label').count()[0]}\n",
    "\\n\n",
    "Equally classified with distilbeto and masked model: {preds_df_ext.query('predicted_label == mm_predicted_label').count()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_pipeline = pipeline(\"zero-shot-classification\", model=\"dccuchile/bert-base-spanish-wwm-uncased-finetuned-xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dropout(m):\n",
    "    if type(m) == nn.Dropout:\n",
    "        # print(m)\n",
    "        m.train()\n",
    "\n",
    "xnli_pipeline.model.apply(apply_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels=[\"positivo\", \"neutral\", \"negativo\"]\n",
    "# all_labels=[\"P\", \"NEU\", \"N\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=mislabeled_df.at[0, 'text']\n",
    "print(mislabeled_df.at[0, 'label'])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = xnli_pipeline(example, all_labels, multi_label=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = xnli_pipeline(example, all_labels, multi_label=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_zs(df, labels):\n",
    "    for i in range(len(df)):\n",
    "        example = df.at[i, 'text'] \n",
    "        # example = \"Hola. Que tal estas?\"\n",
    "        output = xnli_pipeline(example, labels, multi_label=False)\n",
    "        # print(output)\n",
    "        # print(example)        \n",
    "        df.at[i, 'zs_predicted_label'] = to_df_label_str(output['labels'][0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_zs_df_ext = eval_on_zs(mislabeled_df_ext, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_zs_df_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total examples: {mislabeled_zs_df_ext.count()[0]}\n",
    "\\n\n",
    "Correctly classified: {mislabeled_zs_df_ext.query('label == predicted_label').count()[0]}\n",
    "Misclassified: {mislabeled_zs_df_ext.query('label != predicted_label').count()[0]}\n",
    "\\n\n",
    "Correctly classified with masked model: {mislabeled_zs_df_ext.query('label == mm_predicted_label').count()[0]}\n",
    "Misclassified with masked model: {mislabeled_zs_df_ext.query('label != mm_predicted_label').count()[0]}\n",
    "\\n\n",
    "Equally classified with distilbeto and masked model: {mislabeled_zs_df_ext.query('predicted_label == mm_predicted_label').count()[0]}\n",
    "\\n\n",
    "Correctly classified with zero shot model: {mislabeled_zs_df_ext.query('label == zs_predicted_label').count()[0]}\n",
    "Misclassified with zero shot model: {mislabeled_zs_df_ext.query('label != zs_predicted_label').count()[0]}\n",
    "\\n\n",
    "Equally classified with distilbeto and zero shot model: {mislabeled_zs_df_ext.query('predicted_label == zs_predicted_label').count()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_zs_df_ext = eval_on_zs(preds_df_ext, all_labels)\n",
    "preds_zs_df_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Total examples: {preds_zs_df_ext.count()[0]}\n",
    "\n",
    "Correctly classified: {preds_zs_df_ext.query('label == predicted_label').count()[0]}\n",
    "Misclassified: {preds_zs_df_ext.query('label != predicted_label').count()[0]}\n",
    "\n",
    "Correctly classified with masked model: {preds_zs_df_ext.query('label == mm_predicted_label').count()[0]}\n",
    "Misclassified with masked model: {preds_zs_df_ext.query('label != mm_predicted_label').count()[0]}\n",
    "\n",
    "Equally classified with distilbeto and masked model: {preds_zs_df_ext.query('predicted_label == mm_predicted_label').count()[0]}\n",
    "\n",
    "Correctly classified with zero shot model: {preds_zs_df_ext.query('label == zs_predicted_label').count()[0]}\n",
    "Misclassified with zero shot model: {preds_zs_df_ext.query('label != zs_predicted_label').count()[0]}\n",
    "\n",
    "Equally classified with distilbeto and zero shot model: {preds_zs_df_ext.query('predicted_label == zs_predicted_label').count()[0]}\n",
    "Equally classified with masked model and zero shot model: {preds_zs_df_ext.query('mm_predicted_label == zs_predicted_label').count()[0]}\n",
    "\n",
    "Correctly classified with both, masked model and zero shot model: {preds_zs_df_ext.query('label == mm_predicted_label and mm_predicted_label == zs_predicted_label').count()[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72b222e497ad478e6e6c53bf9e7b3df626e2903443cf6c47b806e88d45facfb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
