{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
    "!tar -xzvf pytorch_weights.tar.gz\n",
    "!mv config.json pytorch/.\n",
    "!mv vocab.txt pytorch/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1124, 2)\n",
      "                                                 review  label\n",
      "0     comprendo que te molen mis tattoos, pero no te...      1\n",
      "1     Mi última partida jugada, con Sona support. La...      2\n",
      "2     Tranquilos que con el.dinero de Camacho seguro...      2\n",
      "3     @daniacal aún no, pero si estará jugable en el...      2\n",
      "4     @ragnomuelle Yo a veces hecho de menos mi pelo...      0\n",
      "...                                                 ...    ...\n",
      "1119  Penoso lo de hoy, pensando en que hacer en el ...      1\n",
      "1120  q los GP de La Caixa (y otros) manipulen el Ib...      0\n",
      "1121  Os creéis que es broma que mi foto soy yo desn...      0\n",
      "1122                         @XbadetaX que es muy cara       0\n",
      "1123                 @donarfonzo yo no la veo caliente       1\n",
      "\n",
      "[1124 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Modelo BETO\n",
    "#Libreria transformers (modelo BERT predefinido para la clasificación (BertForSequenceClassification))\n",
    "#Libreria sera BERT + Capa de clasificación por encima\n",
    "#Debemos tokenizar nuestro dataset (tokens + attention mask + max_length)\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras \n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
    "from keras.models import Model\n",
    "\n",
    "MAX_LEN = 38\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "df = pd.read_csv('/Users/nfanlo/Desktop/Python/DataAnalysis/Datasets TFG/TFG/Task1-train-dev/train/train_es.csv')\n",
    "df_val_mx = pd.read_csv('/Users/nfanlo/Desktop/Python/DataAnalysis/Datasets TFG/TFG/Task1-train-dev/train/train_mx.csv')\n",
    "print(df.shape)\n",
    "df.isnull().sum()\n",
    "df.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df['review'] = df['text']\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "df['label'] = df['sentiment']\n",
    "df.drop('sentiment', axis=1, inplace=True)\n",
    "review = df['review']\n",
    "label = df['label']\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review  label\n",
      "0                                 comprendo molen hagas      1\n",
      "1        última partida sona grandes razones jugar sona      2\n",
      "2     tranquilos camacho seguro mañana empiezan esca...      2\n",
      "3                         aún si jugable tgs creo tarde      2\n",
      "4              veces hecho menos pelo largo mismo cosas      0\n",
      "...                                                 ...    ...\n",
      "1119    penoso pensando hacer gracias toda gente stream      1\n",
      "1120  q gp caixa manipulen ibex bendició gracias pod...      0\n",
      "1121                      creéis broma foto desnuda luz      0\n",
      "1122                                               cara      0\n",
      "1123                                       veo caliente      1\n",
      "\n",
      "[1124 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Environment stopwords\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(review):\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(review):\n",
    "    return re.sub('\\[[^]]*\\]', '', review)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(review):\n",
    "    return re.sub(r'http\\S+', '', review)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(review):\n",
    "    final_text = []\n",
    "    for i in review.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(review):\n",
    "    review = strip_html(review)\n",
    "    review = remove_between_square_brackets(review)\n",
    "    review = remove_stopwords(review)\n",
    "    return review\n",
    "#Apply function on review column\n",
    "df['review']=df['review'].apply(denoise_text)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  sentiment\n",
      "0    @leomall2018 Según yo era como aviso, pero aho...          1\n",
      "1    @benshorts a juzgar por mis comportamientos au...          0\n",
      "2    #BuenosDias mundo Twittero ya desperté y estoy...          2\n",
      "3    No pude resolver el rompecabezas en Los rios d...          0\n",
      "4    o sea ... me urge un Dr. @Rocktor101 (escuchó ...          0\n",
      "..                                                 ...        ...\n",
      "984  @ladelbosque29 acude al próximo llamado que ha...          1\n",
      "985  @Dianybony jajajaja claro que no amor!! te amo...          2\n",
      "986  Hoy le pedí a Dios una señal realmente obvia, ...          2\n",
      "987  El reboot de Jumanji puede romper mi corazón x...          0\n",
      "988  @Djrossana que tengan un lindo martes y que to...          2\n",
      "\n",
      "[989 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "review = df['review']\n",
    "label = df['label']\n",
    "\n",
    "df_val_mx.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df_val_mx.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df_val_mx.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "\n",
    "print(df_val_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  sentiment\n",
      "0                                  según ahora oficial          1\n",
      "1    juzgar comportamientos autodestructivos aún qu...          0\n",
      "2          mundo twittero desperté listo vivir dia mas          2\n",
      "3    pude resolver rompecabezas rios alicia ahora m...          0\n",
      "4                       urge programa males digestivos          0\n",
      "..                                                 ...        ...\n",
      "984  acude próximo llamado hagamos zapato cerrado c...          1\n",
      "985                jajajaja claro amo esperando quiero          2\n",
      "986  hoy pedí dios señal realmente poder tomar impo...          2\n",
      "987  reboot jumanji puede romper corazón puede ser ...          0\n",
      "988  lindo martes sueños hagan realidad sé copies m...          2\n",
      "\n",
      "[989 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Environment stopwords\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(review):\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(review_mx):\n",
    "    return re.sub('\\[[^]]*\\]', '', review_mx)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(review_mx):\n",
    "    return re.sub(r'http\\S+', '', review_mx)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(review_mx):\n",
    "    final_text = []\n",
    "    for i in review_mx.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(review_mx):\n",
    "    review_mx = strip_html(review_mx)\n",
    "    review_mx = remove_between_square_brackets(review_mx)\n",
    "    review_mx = remove_stopwords(review_mx)\n",
    "    return review_mx\n",
    "#Apply function on review column\n",
    "df_val_mx['text']=df_val_mx['text'].apply(denoise_text)\n",
    "\n",
    "print(df_val_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                    según ahora oficial\n",
      "1      juzgar comportamientos autodestructivos aún qu...\n",
      "2            mundo twittero desperté listo vivir dia mas\n",
      "3      pude resolver rompecabezas rios alicia ahora m...\n",
      "4                         urge programa males digestivos\n",
      "                             ...                        \n",
      "984    acude próximo llamado hagamos zapato cerrado c...\n",
      "985                  jajajaja claro amo esperando quiero\n",
      "986    hoy pedí dios señal realmente poder tomar impo...\n",
      "987    reboot jumanji puede romper corazón puede ser ...\n",
      "988    lindo martes sueños hagan realidad sé copies m...\n",
      "Name: text, Length: 989, dtype: object\n",
      "0      1\n",
      "1      0\n",
      "2      2\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "984    1\n",
      "985    2\n",
      "986    2\n",
      "987    0\n",
      "988    2\n",
      "Name: sentiment, Length: 989, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "review_mx = df_val_mx['text']\n",
    "label_mx = df_val_mx['sentiment']\n",
    "print(review_mx)\n",
    "print(label_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO',\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=MAX_LEN,\n",
    "                   truncation=True ,pad_to_max_length=True,\n",
    "                   return_token_type_ids = False,\n",
    "                   return_attention_mask = True,)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(review)\n",
    "X_val_inputs, X_val_masks = preprocessing(review_mx)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids']\n"
     ]
    }
   ],
   "source": [
    "print(['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "batch_size = 16\n",
    "\n",
    "y_train_labels = torch.tensor(label.values)\n",
    "y_val_labels = torch.tensor(label_mx.values)\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks, y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el modelo + optimizador + definimos EPOCHS + Scheduler\n",
    "#Modelo\n",
    "model = BertForSequenceClassification.from_pretrained('/Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO', num_labels=3,\n",
    " output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 4e-5, eps = 1e-6)\n",
    "\n",
    "epochs=5\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion para formatear el tiempo y otra para calcular la exactitud\n",
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Training--------------------\n",
      "\n",
      "======= Epoch 1 / 5 =======\n",
      "batch loss: 1.2199769020080566 | avg loss: 1.2199769020080566\n",
      "batch loss: 1.1200382709503174 | avg loss: 1.170007586479187\n",
      "batch loss: 0.9305777549743652 | avg loss: 1.0901976426442463\n",
      "batch loss: 0.9932152628898621 | avg loss: 1.0659520477056503\n",
      "batch loss: 0.9797075390815735 | avg loss: 1.048703145980835\n",
      "batch loss: 1.0418909788131714 | avg loss: 1.0475677847862244\n",
      "batch loss: 0.900810718536377 | avg loss: 1.0266024896076746\n",
      "batch loss: 1.0906181335449219 | avg loss: 1.0346044450998306\n",
      "batch loss: 0.9689158201217651 | avg loss: 1.0273057089911566\n",
      "batch loss: 1.196665644645691 | avg loss: 1.0442417025566102\n",
      "batch loss: 1.0740602016448975 | avg loss: 1.0469524752009998\n",
      "batch loss: 1.079559564590454 | avg loss: 1.049669732650121\n",
      "batch loss: 1.106737494468689 | avg loss: 1.0540595604823186\n",
      "batch loss: 1.2510312795639038 | avg loss: 1.0681289689881461\n",
      "batch loss: 0.9576455354690552 | avg loss: 1.06076340675354\n",
      "batch loss: 1.0899471044540405 | avg loss: 1.0625873878598213\n",
      "batch loss: 1.1172099113464355 | avg loss: 1.065800477476681\n",
      "batch loss: 1.1528242826461792 | avg loss: 1.0706351333194308\n",
      "batch loss: 1.0254977941513062 | avg loss: 1.0682594838895296\n",
      "batch loss: 1.0151084661483765 | avg loss: 1.065601933002472\n",
      "batch loss: 0.8765198588371277 | avg loss: 1.0565980247088842\n",
      "batch loss: 0.7347502708435059 | avg loss: 1.0419685813513668\n",
      "batch loss: 0.9069303870201111 | avg loss: 1.0360973555108774\n",
      "batch loss: 1.1396024227142334 | avg loss: 1.0404100666443508\n",
      "batch loss: 1.596047043800354 | avg loss: 1.0626355457305907\n",
      "batch loss: 1.3688123226165771 | avg loss: 1.074411575610821\n",
      "batch loss: 1.2541332244873047 | avg loss: 1.0810679329766169\n",
      "batch loss: 0.9332404136657715 | avg loss: 1.0757883787155151\n",
      "batch loss: 1.075906753540039 | avg loss: 1.0757924606060159\n",
      "batch loss: 1.0731909275054932 | avg loss: 1.0757057428359986\n",
      "batch loss: 1.049783706665039 | avg loss: 1.0748695481208064\n",
      "batch loss: 1.1680551767349243 | avg loss: 1.0777815990149975\n",
      "batch loss: 1.110201120376587 | avg loss: 1.0787640087532275\n",
      "batch loss: 1.1062886714935303 | avg loss: 1.079573557657354\n",
      "batch loss: 1.0753639936447144 | avg loss: 1.07945328439985\n",
      "batch loss: 1.1781142950057983 | avg loss: 1.082193868027793\n",
      "batch loss: 1.112274408340454 | avg loss: 1.0830068556038108\n",
      "batch loss: 1.146456241607666 | avg loss: 1.084676576288123\n",
      "batch loss: 1.0187655687332153 | avg loss: 1.0829865504533818\n",
      "batch loss: 1.1424586772918701 | avg loss: 1.0844733536243438\n",
      "batch loss: 1.1363332271575928 | avg loss: 1.0857382285885695\n",
      "batch loss: 1.1098967790603638 | avg loss: 1.0863134321712313\n",
      "batch loss: 1.1927582025527954 | avg loss: 1.0887888919475466\n",
      "batch loss: 1.0702053308486938 | avg loss: 1.088366538286209\n",
      "batch loss: 1.1355934143066406 | avg loss: 1.0894160244199964\n",
      "batch loss: 1.0653003454208374 | avg loss: 1.0888917705287104\n",
      "batch loss: 1.1729846000671387 | avg loss: 1.0906809796678258\n",
      "batch loss: 1.0703811645507812 | avg loss: 1.0902580668528874\n",
      "batch loss: 1.1002873182296753 | avg loss: 1.0904627454524138\n",
      "batch loss: 1.1341750621795654 | avg loss: 1.0913369917869569\n",
      "batch loss: 1.0918256044387817 | avg loss: 1.0913465724271887\n",
      "batch loss: 1.1875079870224 | avg loss: 1.0931958304001734\n",
      "batch loss: 1.1086431741714478 | avg loss: 1.0934872897166126\n",
      "batch loss: 1.0424190759658813 | avg loss: 1.092541582054562\n",
      "batch loss: 1.144323468208313 | avg loss: 1.093483070893721\n",
      "batch loss: 1.1307228803634644 | avg loss: 1.094148067491395\n",
      "batch loss: 1.1182444095611572 | avg loss: 1.0945708103347243\n",
      "batch loss: 1.1107401847839355 | avg loss: 1.0948495926528141\n",
      "batch loss: 1.0075287818908691 | avg loss: 1.0933695789110862\n",
      "batch loss: 1.025503158569336 | avg loss: 1.0922384719053904\n",
      "batch loss: 1.147503137588501 | avg loss: 1.093144450031343\n",
      "batch loss: 1.1845688819885254 | avg loss: 1.0946190376435556\n",
      "batch loss: 1.0453550815582275 | avg loss: 1.0938370700866458\n",
      "batch loss: 1.0098066329956055 | avg loss: 1.0925240945070982\n",
      "batch loss: 1.013499140739441 | avg loss: 1.0913083259875958\n",
      "batch loss: 1.1245219707489014 | avg loss: 1.0918115630294338\n",
      "batch loss: 1.040531039237976 | avg loss: 1.0910461820773225\n",
      "batch loss: 1.0372391939163208 | avg loss: 1.0902549028396606\n",
      "batch loss: 0.9093356132507324 | avg loss: 1.087632884149966\n",
      "batch loss: 1.0329632759094238 | avg loss: 1.0868518897465298\n",
      "batch loss: 0.9125679135322571 | avg loss: 1.0843971858561878\n",
      "\n",
      "  Average training loss: 1.08\n",
      "  Training epoch took: 0:05:45\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.51\n",
      "  Validation took: 0:01:26\n",
      "\n",
      "======= Epoch 2 / 5 =======\n",
      "batch loss: 1.3044495582580566 | avg loss: 1.3044495582580566\n",
      "batch loss: 1.2618095874786377 | avg loss: 1.2831295728683472\n",
      "batch loss: 0.9846364259719849 | avg loss: 1.1836318572362263\n",
      "batch loss: 0.8921846747398376 | avg loss: 1.1107700616121292\n",
      "batch loss: 0.9562727808952332 | avg loss: 1.07987060546875\n",
      "batch loss: 1.3185527324676514 | avg loss: 1.119650959968567\n",
      "batch loss: 0.9111936092376709 | avg loss: 1.0898713384355818\n",
      "batch loss: 1.1227524280548096 | avg loss: 1.0939814746379852\n",
      "batch loss: 0.9052976965904236 | avg loss: 1.0730166104104784\n",
      "batch loss: 1.1454317569732666 | avg loss: 1.0802581250667571\n",
      "batch loss: 0.9816184043884277 | avg loss: 1.0712908777323635\n",
      "batch loss: 1.0403532981872559 | avg loss: 1.0687127461036046\n",
      "batch loss: 0.9851363897323608 | avg loss: 1.062283795613509\n",
      "batch loss: 1.1899775266647339 | avg loss: 1.0714047764028822\n",
      "batch loss: 0.9667491912841797 | avg loss: 1.0644277373949687\n",
      "batch loss: 1.05238676071167 | avg loss: 1.0636751763522625\n",
      "batch loss: 1.0608593225479126 | avg loss: 1.0635095378931831\n",
      "batch loss: 1.0402613878250122 | avg loss: 1.0622179740005069\n",
      "batch loss: 0.9487005472183228 | avg loss: 1.0562433725909184\n",
      "batch loss: 1.0284948348999023 | avg loss: 1.0548559457063675\n",
      "batch loss: 0.9714338183403015 | avg loss: 1.0508834634508406\n",
      "batch loss: 0.8144841194152832 | avg loss: 1.0401380387219517\n",
      "batch loss: 0.8341103792190552 | avg loss: 1.0311803143957388\n",
      "batch loss: 0.9938384294509888 | avg loss: 1.0296244025230408\n",
      "batch loss: 1.5177205801010132 | avg loss: 1.0491482496261597\n",
      "batch loss: 1.2891652584075928 | avg loss: 1.0583796730408301\n",
      "batch loss: 1.3262081146240234 | avg loss: 1.068299244951319\n",
      "batch loss: 0.974057674407959 | avg loss: 1.0649334745747703\n",
      "batch loss: 1.1412439346313477 | avg loss: 1.067564869749135\n",
      "batch loss: 1.1650243997573853 | avg loss: 1.07081352074941\n",
      "batch loss: 0.8944762945175171 | avg loss: 1.0651252231290262\n",
      "batch loss: 1.0211454629898071 | avg loss: 1.0637508556246758\n",
      "batch loss: 0.9688950777053833 | avg loss: 1.06087643811197\n",
      "batch loss: 1.1608377695083618 | avg loss: 1.0638164772706873\n",
      "batch loss: 1.0341744422912598 | avg loss: 1.0629695619855608\n",
      "batch loss: 1.1364022493362427 | avg loss: 1.0650093588564131\n",
      "batch loss: 1.0758126974105835 | avg loss: 1.0653013409794987\n",
      "batch loss: 1.07477605342865 | avg loss: 1.0655506755176343\n",
      "batch loss: 1.142703652381897 | avg loss: 1.0675289569756923\n",
      "batch loss: 1.1939771175384521 | avg loss: 1.0706901609897614\n",
      "batch loss: 1.1205708980560303 | avg loss: 1.071906764332841\n",
      "batch loss: 1.1074243783950806 | avg loss: 1.0727524218105136\n",
      "batch loss: 1.0327202081680298 | avg loss: 1.0718214400978976\n",
      "batch loss: 1.0492430925369263 | avg loss: 1.0713082958351483\n",
      "batch loss: 1.0280499458312988 | avg loss: 1.070346999168396\n",
      "batch loss: 1.0507785081863403 | avg loss: 1.0699215971905252\n",
      "batch loss: 1.0817208290100098 | avg loss: 1.0701726446760462\n",
      "batch loss: 1.068701982498169 | avg loss: 1.0701420058806737\n",
      "batch loss: 1.0900248289108276 | avg loss: 1.0705477777792483\n",
      "batch loss: 1.0246262550354004 | avg loss: 1.0696293473243714\n",
      "batch loss: 1.0055606365203857 | avg loss: 1.0683730980929207\n",
      "batch loss: 1.1188199520111084 | avg loss: 1.0693432298990397\n",
      "batch loss: 1.0077098608016968 | avg loss: 1.068180336142486\n",
      "batch loss: 0.9462534785270691 | avg loss: 1.06592243137183\n",
      "batch loss: 1.0646848678588867 | avg loss: 1.0658999302170493\n",
      "batch loss: 1.134853720664978 | avg loss: 1.0671312479036195\n",
      "batch loss: 1.087570071220398 | avg loss: 1.0674898237512822\n",
      "batch loss: 0.9825272560119629 | avg loss: 1.0660249518937077\n",
      "batch loss: 0.959388792514801 | avg loss: 1.064217559361862\n",
      "batch loss: 1.0143868923187256 | avg loss: 1.0633870482444763\n",
      "batch loss: 1.0360864400863647 | avg loss: 1.0629394972910646\n",
      "batch loss: 1.1924152374267578 | avg loss: 1.06502781568035\n",
      "batch loss: 0.9757736325263977 | avg loss: 1.0636110826144143\n",
      "batch loss: 0.9335663914680481 | avg loss: 1.0615791343152523\n",
      "batch loss: 0.9876292943954468 | avg loss: 1.0604414444703323\n",
      "batch loss: 1.0207246541976929 | avg loss: 1.0598396749207468\n",
      "batch loss: 0.9244392514228821 | avg loss: 1.057818773077495\n",
      "batch loss: 1.0362356901168823 | avg loss: 1.0575013747986626\n",
      "batch loss: 0.9034671783447266 | avg loss: 1.05526899513991\n",
      "batch loss: 0.9559522271156311 | avg loss: 1.0538501841681345\n",
      "batch loss: 0.5703722238540649 | avg loss: 1.0470406354313166\n",
      "\n",
      "  Average training loss: 1.05\n",
      "  Training epoch took: 0:05:26\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.50\n",
      "  Validation took: 0:01:15\n",
      "\n",
      "======= Epoch 3 / 5 =======\n",
      "batch loss: 1.462057113647461 | avg loss: 1.462057113647461\n",
      "batch loss: 1.2848074436187744 | avg loss: 1.3734322786331177\n",
      "batch loss: 0.8613044619560242 | avg loss: 1.20272300640742\n",
      "batch loss: 0.7730391621589661 | avg loss: 1.0953020453453064\n",
      "batch loss: 0.906053900718689 | avg loss: 1.057452416419983\n",
      "batch loss: 1.2919923067092896 | avg loss: 1.0965423981348674\n",
      "batch loss: 0.7495737671852112 | avg loss: 1.0469754508563451\n",
      "batch loss: 0.8754310011863708 | avg loss: 1.0255323946475983\n",
      "batch loss: 0.7501257061958313 | avg loss: 0.9949316514862908\n",
      "batch loss: 1.1213796138763428 | avg loss: 1.007576447725296\n",
      "batch loss: 0.858831524848938 | avg loss: 0.9940541820092634\n",
      "batch loss: 0.8452571034431458 | avg loss: 0.981654425462087\n",
      "batch loss: 0.8132415413856506 | avg loss: 0.968699588225438\n",
      "batch loss: 1.132093906402588 | avg loss: 0.9803706109523773\n",
      "batch loss: 0.7930938601493835 | avg loss: 0.9678854942321777\n",
      "batch loss: 0.929804265499115 | avg loss: 0.9655054174363613\n",
      "batch loss: 0.728262722492218 | avg loss: 0.9515499647925881\n",
      "batch loss: 0.852879524230957 | avg loss: 0.9460682736502753\n",
      "batch loss: 0.8839800357818604 | avg loss: 0.9428004716572008\n",
      "batch loss: 0.8802057504653931 | avg loss: 0.9396707355976105\n",
      "batch loss: 0.7649750709533691 | avg loss: 0.9313518944240752\n",
      "batch loss: 0.6658000349998474 | avg loss: 0.9192813553593375\n",
      "batch loss: 0.7172738909721375 | avg loss: 0.9104984221251115\n",
      "batch loss: 0.7586921453475952 | avg loss: 0.9041731605927149\n",
      "batch loss: 1.4527901411056519 | avg loss: 0.9261178398132324\n",
      "batch loss: 1.0747840404510498 | avg loss: 0.9318357706069946\n",
      "batch loss: 1.2343242168426514 | avg loss: 0.9430390463935004\n",
      "batch loss: 0.8971105813980103 | avg loss: 0.9413987440722329\n",
      "batch loss: 1.1476163864135742 | avg loss: 0.9485096972564171\n",
      "batch loss: 0.9833038449287415 | avg loss: 0.949669502178828\n",
      "batch loss: 0.7994127869606018 | avg loss: 0.9448225113653368\n",
      "batch loss: 1.0102993249893188 | avg loss: 0.9468686617910862\n",
      "batch loss: 0.8286054730415344 | avg loss: 0.9432849287986755\n",
      "batch loss: 1.221602439880371 | avg loss: 0.9514707379481372\n",
      "batch loss: 1.1498162746429443 | avg loss: 0.9571377532822746\n",
      "batch loss: 1.0969518423080444 | avg loss: 0.9610214779774348\n",
      "batch loss: 0.8268702030181885 | avg loss: 0.9573957678434011\n",
      "batch loss: 1.0362781286239624 | avg loss: 0.9594716194428896\n",
      "batch loss: 0.9354655742645264 | avg loss: 0.9588560798229315\n",
      "batch loss: 1.0629959106445312 | avg loss: 0.9614595755934715\n",
      "batch loss: 1.0586479902267456 | avg loss: 0.9638300247308684\n",
      "batch loss: 0.8204747438430786 | avg loss: 0.9604168037573496\n",
      "batch loss: 0.9273975491523743 | avg loss: 0.9596489141153735\n",
      "batch loss: 0.9604116678237915 | avg loss: 0.9596662494269285\n",
      "batch loss: 0.7341036200523376 | avg loss: 0.9546537465519376\n",
      "batch loss: 0.8379182815551758 | avg loss: 0.9521160190520079\n",
      "batch loss: 1.1499890089035034 | avg loss: 0.9563260826658695\n",
      "batch loss: 0.7957083582878113 | avg loss: 0.95297988007466\n",
      "batch loss: 0.8892403244972229 | avg loss: 0.9516790728179776\n",
      "batch loss: 0.8907324075698853 | avg loss: 0.9504601395130158\n",
      "batch loss: 0.9751574993133545 | avg loss: 0.9509444014698851\n",
      "batch loss: 1.0190010070800781 | avg loss: 0.9522531823470042\n",
      "batch loss: 0.7950391173362732 | avg loss: 0.9492868792335942\n",
      "batch loss: 0.8465760946273804 | avg loss: 0.9473848276668124\n",
      "batch loss: 0.9146155118942261 | avg loss: 0.9467890219254927\n",
      "batch loss: 1.0387253761291504 | avg loss: 0.9484307425362724\n",
      "batch loss: 0.9310287237167358 | avg loss: 0.948125443960491\n",
      "batch loss: 0.7933405637741089 | avg loss: 0.9454567391296913\n",
      "batch loss: 0.9074043035507202 | avg loss: 0.9448117825944545\n",
      "batch loss: 0.9382435083389282 | avg loss: 0.9447023113568623\n",
      "batch loss: 0.8483437299728394 | avg loss: 0.9431226624817145\n",
      "batch loss: 1.1188099384307861 | avg loss: 0.9459563282228285\n",
      "batch loss: 0.7423759698867798 | avg loss: 0.9427248939635262\n",
      "batch loss: 0.8522025346755981 | avg loss: 0.9413104820996523\n",
      "batch loss: 0.8498084545135498 | avg loss: 0.9399027585983276\n",
      "batch loss: 0.8149266242980957 | avg loss: 0.9380091808058999\n",
      "batch loss: 0.8013962507247925 | avg loss: 0.9359701818494655\n",
      "batch loss: 0.8410559892654419 | avg loss: 0.9345743848997004\n",
      "batch loss: 0.807183563709259 | avg loss: 0.9327281411143317\n",
      "batch loss: 0.9519091844558716 | avg loss: 0.9330021560192108\n",
      "batch loss: 0.6805139183998108 | avg loss: 0.9294459836583742\n",
      "\n",
      "  Average training loss: 0.93\n",
      "  Training epoch took: 0:05:31\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.49\n",
      "  Validation took: 0:01:19\n",
      "\n",
      "======= Epoch 4 / 5 =======\n",
      "batch loss: 1.3042138814926147 | avg loss: 1.3042138814926147\n",
      "batch loss: 0.9222031235694885 | avg loss: 1.1132085025310516\n",
      "batch loss: 0.5984679460525513 | avg loss: 0.9416283170382181\n",
      "batch loss: 0.7111364006996155 | avg loss: 0.8840053379535675\n",
      "batch loss: 0.8242365717887878 | avg loss: 0.8720515847206116\n",
      "batch loss: 1.030957579612732 | avg loss: 0.8985359172026316\n",
      "batch loss: 0.703220009803772 | avg loss: 0.8706336447170803\n",
      "batch loss: 0.6487295031547546 | avg loss: 0.8428956270217896\n",
      "batch loss: 0.714451014995575 | avg loss: 0.8286240034633212\n",
      "batch loss: 1.191146969795227 | avg loss: 0.8648763000965118\n",
      "batch loss: 0.5331888794898987 | avg loss: 0.8347228982231834\n",
      "batch loss: 0.6170026659965515 | avg loss: 0.8165795455376307\n",
      "batch loss: 0.6132909059524536 | avg loss: 0.8009419578772324\n",
      "batch loss: 0.7433104515075684 | avg loss: 0.7968254217079708\n",
      "batch loss: 0.6366633772850037 | avg loss: 0.786147952079773\n",
      "batch loss: 0.5956016182899475 | avg loss: 0.7742388062179089\n",
      "batch loss: 0.38098233938217163 | avg loss: 0.7511060728746302\n",
      "batch loss: 0.6699674129486084 | avg loss: 0.7465983695454068\n",
      "batch loss: 0.8580785989761353 | avg loss: 0.7524657500417609\n",
      "batch loss: 0.9182258248329163 | avg loss: 0.7607537537813187\n",
      "batch loss: 0.8151742815971375 | avg loss: 0.7633452074868339\n",
      "batch loss: 0.7464748620986938 | avg loss: 0.7625783736055548\n",
      "batch loss: 0.6609346866607666 | avg loss: 0.7581590828688248\n",
      "batch loss: 0.4913267195224762 | avg loss: 0.7470410677293936\n",
      "batch loss: 1.1959794759750366 | avg loss: 0.7649986040592194\n",
      "batch loss: 0.8708595037460327 | avg loss: 0.7690701771240968\n",
      "batch loss: 1.0615705251693726 | avg loss: 0.779903523347996\n",
      "batch loss: 0.6951450109481812 | avg loss: 0.776876433619431\n",
      "batch loss: 0.9039928317070007 | avg loss: 0.7812597576914162\n",
      "batch loss: 0.7280810475349426 | avg loss: 0.7794871340195338\n",
      "batch loss: 0.5079952478408813 | avg loss: 0.7707293312395772\n",
      "batch loss: 0.8333264589309692 | avg loss: 0.7726854914799333\n",
      "batch loss: 0.7066720724105835 | avg loss: 0.7706850848414681\n",
      "batch loss: 1.1442115306854248 | avg loss: 0.7816711567780551\n",
      "batch loss: 0.8368229269981384 | avg loss: 0.783246921641486\n",
      "batch loss: 1.079972743988037 | avg loss: 0.7914893055955569\n",
      "batch loss: 0.6106956601142883 | avg loss: 0.7866029908528199\n",
      "batch loss: 0.4864546060562134 | avg loss: 0.7787043491476461\n",
      "batch loss: 0.6226440072059631 | avg loss: 0.7747028019183722\n",
      "batch loss: 1.0228726863861084 | avg loss: 0.7809070490300656\n",
      "batch loss: 0.836654543876648 | avg loss: 0.7822667440263237\n",
      "batch loss: 0.525857150554657 | avg loss: 0.7761617537055697\n",
      "batch loss: 0.7016335725784302 | avg loss: 0.7744285401909851\n",
      "batch loss: 0.8081791996955872 | avg loss: 0.7751956006342714\n",
      "batch loss: 0.42191943526268005 | avg loss: 0.7673450191815694\n",
      "batch loss: 0.557427704334259 | avg loss: 0.7627815992935844\n",
      "batch loss: 0.8256399035453796 | avg loss: 0.7641190100223461\n",
      "batch loss: 0.49760228395462036 | avg loss: 0.7585665782292684\n",
      "batch loss: 0.5432742834091187 | avg loss: 0.7541728579268163\n",
      "batch loss: 0.6745153069496155 | avg loss: 0.7525797069072724\n",
      "batch loss: 0.7346758842468262 | avg loss: 0.7522286515609891\n",
      "batch loss: 0.7376261949539185 | avg loss: 0.7519478350877762\n",
      "batch loss: 0.6944485306739807 | avg loss: 0.7508629425516669\n",
      "batch loss: 0.4387342035770416 | avg loss: 0.7450827807188034\n",
      "batch loss: 0.6704263091087341 | avg loss: 0.743725390325893\n",
      "batch loss: 0.6443440914154053 | avg loss: 0.7419507242739201\n",
      "batch loss: 0.8213755488395691 | avg loss: 0.743344142248756\n",
      "batch loss: 0.814757764339447 | avg loss: 0.7445754115951473\n",
      "batch loss: 0.6571245789527893 | avg loss: 0.7430931940927343\n",
      "batch loss: 0.7916584014892578 | avg loss: 0.7439026142160098\n",
      "batch loss: 0.48246464133262634 | avg loss: 0.7396167458080855\n",
      "batch loss: 0.8501604795455933 | avg loss: 0.7413997092554646\n",
      "batch loss: 0.6179165244102478 | avg loss: 0.7394396587023659\n",
      "batch loss: 0.6592426300048828 | avg loss: 0.7381865801289678\n",
      "batch loss: 0.5985514521598816 | avg loss: 0.7360383473909818\n",
      "batch loss: 0.6566022634506226 | avg loss: 0.7348347703615824\n",
      "batch loss: 0.8095645308494568 | avg loss: 0.7359501399211029\n",
      "batch loss: 0.5724748373031616 | avg loss: 0.733546091353192\n",
      "batch loss: 0.8509671092033386 | avg loss: 0.7352478452350782\n",
      "batch loss: 0.6245807409286499 | avg loss: 0.7336668866021293\n",
      "batch loss: 0.3234747052192688 | avg loss: 0.727889531934765\n",
      "\n",
      "  Average training loss: 0.73\n",
      "  Training epoch took: 0:05:42\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.46\n",
      "  Validation took: 0:01:20\n",
      "\n",
      "======= Epoch 5 / 5 =======\n",
      "batch loss: 0.9858624339103699 | avg loss: 0.9858624339103699\n",
      "batch loss: 0.5146769881248474 | avg loss: 0.7502697110176086\n",
      "batch loss: 0.34322458505630493 | avg loss: 0.6145880023638407\n",
      "batch loss: 0.4602697491645813 | avg loss: 0.5760084390640259\n",
      "batch loss: 0.6547553539276123 | avg loss: 0.5917578220367432\n",
      "batch loss: 0.6805917024612427 | avg loss: 0.6065634687741598\n",
      "batch loss: 0.5201713442802429 | avg loss: 0.5942217367036002\n",
      "batch loss: 0.5225529670715332 | avg loss: 0.5852631404995918\n",
      "batch loss: 0.5278430581092834 | avg loss: 0.5788831313451132\n",
      "batch loss: 0.992347240447998 | avg loss: 0.6202295422554016\n",
      "batch loss: 0.2639164626598358 | avg loss: 0.5878374441103502\n",
      "batch loss: 0.45698001980781555 | avg loss: 0.5769326587518057\n",
      "batch loss: 0.5133454203605652 | avg loss: 0.5720413327217102\n",
      "batch loss: 0.549514651298523 | avg loss: 0.5704322840486254\n",
      "batch loss: 0.41136735677719116 | avg loss: 0.5598279555638631\n",
      "batch loss: 0.446561336517334 | avg loss: 0.552748791873455\n",
      "batch loss: 0.20351828634738922 | avg loss: 0.532205820960157\n",
      "batch loss: 0.543456494808197 | avg loss: 0.5328308583961593\n",
      "batch loss: 0.6793683767318726 | avg loss: 0.5405433593611968\n",
      "batch loss: 0.5589818358421326 | avg loss: 0.5414652831852436\n",
      "batch loss: 0.45662546157836914 | avg loss: 0.5374252916801543\n",
      "batch loss: 0.5798729062080383 | avg loss: 0.539354728704149\n",
      "batch loss: 0.49567079544067383 | avg loss: 0.5374554272579111\n",
      "batch loss: 0.5774950385093689 | avg loss: 0.5391237443933884\n",
      "batch loss: 0.8671699166297913 | avg loss: 0.5522455912828446\n",
      "batch loss: 0.6523972749710083 | avg loss: 0.5560975791170046\n",
      "batch loss: 0.897446870803833 | avg loss: 0.5687401454757761\n",
      "batch loss: 0.888124406337738 | avg loss: 0.5801467262208462\n",
      "batch loss: 0.8766344785690308 | avg loss: 0.5903704418190594\n",
      "batch loss: 0.7795813083648682 | avg loss: 0.5966774707039197\n",
      "batch loss: 0.5411953926086426 | avg loss: 0.5948877262492334\n",
      "batch loss: 0.8477573394775391 | avg loss: 0.6027899016626179\n",
      "batch loss: 0.455166757106781 | avg loss: 0.5983164730397138\n",
      "batch loss: 0.9894695281982422 | avg loss: 0.6098209746620235\n",
      "batch loss: 0.6544871926307678 | avg loss: 0.6110971523182732\n",
      "batch loss: 0.7676995396614075 | avg loss: 0.6154472186333604\n",
      "batch loss: 0.4383467435836792 | avg loss: 0.6106607193076933\n",
      "batch loss: 0.5004894733428955 | avg loss: 0.6077614759928301\n",
      "batch loss: 0.46872735023498535 | avg loss: 0.6041964984092957\n",
      "batch loss: 0.8015674948692322 | avg loss: 0.6091307733207941\n",
      "batch loss: 0.6802144050598145 | avg loss: 0.61086452043638\n",
      "batch loss: 0.5636873841285706 | avg loss: 0.609741255286194\n",
      "batch loss: 0.7839607000350952 | avg loss: 0.6137928702803546\n",
      "batch loss: 0.6012685298919678 | avg loss: 0.6135082261806185\n",
      "batch loss: 0.2862696945667267 | avg loss: 0.6062362588114208\n",
      "batch loss: 0.6126872301101685 | avg loss: 0.6063764973179154\n",
      "batch loss: 0.6863419413566589 | avg loss: 0.6080778897442716\n",
      "batch loss: 0.44386938214302063 | avg loss: 0.6046568791692456\n",
      "batch loss: 0.48840755224227905 | avg loss: 0.6022844439258381\n",
      "batch loss: 0.4541911780834198 | avg loss: 0.5993225786089897\n",
      "batch loss: 0.5766938924789429 | avg loss: 0.5988788788809496\n",
      "batch loss: 0.581358015537262 | avg loss: 0.5985419392012633\n",
      "batch loss: 0.7237051129341125 | avg loss: 0.6009035085169774\n",
      "batch loss: 0.4061465859413147 | avg loss: 0.5972968988396503\n",
      "batch loss: 0.5266107320785522 | avg loss: 0.5960116958076304\n",
      "batch loss: 0.44549331068992615 | avg loss: 0.5933238675019571\n",
      "batch loss: 0.6296067833900452 | avg loss: 0.5939604098859587\n",
      "batch loss: 0.4672377109527588 | avg loss: 0.5917755357664207\n",
      "batch loss: 0.46537402272224426 | avg loss: 0.5896331372402482\n",
      "batch loss: 0.5161277651786804 | avg loss: 0.5884080477058887\n",
      "batch loss: 0.4103427529335022 | avg loss: 0.5854889445128988\n",
      "batch loss: 0.4744906425476074 | avg loss: 0.5836986493199102\n",
      "batch loss: 0.34139326214790344 | avg loss: 0.5798525320632117\n",
      "batch loss: 0.517551839351654 | avg loss: 0.5788790837395936\n",
      "batch loss: 0.417567640542984 | avg loss: 0.5763973692288765\n",
      "batch loss: 0.5308428406715393 | avg loss: 0.5757071490992199\n",
      "batch loss: 0.5976112484931946 | avg loss: 0.5760340759558464\n",
      "batch loss: 0.47898101806640625 | avg loss: 0.5746068251045311\n",
      "batch loss: 0.5423992872238159 | avg loss: 0.5741400491932164\n",
      "batch loss: 0.5868288278579712 | avg loss: 0.5743213174598557\n",
      "batch loss: 0.26374080777168274 | avg loss: 0.569946944083966\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:05:46\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.42\n",
      "  Validation took: 0:01:21\n",
      "\n",
      "Training complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.42      0.49       504\n",
      "           2       0.20      0.24      0.22       172\n",
      "           3       0.38      0.52      0.44       313\n",
      "\n",
      "    accuracy                           0.42       989\n",
      "   macro avg       0.39      0.39      0.38       989\n",
      "weighted avg       0.46      0.42      0.43       989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Definimos la funcion para entrenar el modelo y entregar los resultados en el set de validación\n",
    "#Train model\n",
    "def training(n_epochs, training_dataloader, validation_dataloader):\n",
    "    print('-----------------Training--------------------')\n",
    "    for epoch_i in range(0, n_epochs):\n",
    "        # Perform one full pass over the training set\n",
    "        print(\"\")\n",
    "        print('======= Epoch {:} / {:} ======='.format(\n",
    "             epoch_i + 1, epochs))\n",
    "        # Mira cuanto tiempo le cuesta entrenar un EPOCH.\n",
    "        t0 = time.time()\n",
    "        # Resetea la perdida para este EPOCH.\n",
    "        total_loss = 0\n",
    "        # Pone el modelo en modo entrenamiento.\n",
    "        model.train()\n",
    "        # Para cada batch en el training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Limpia el gradiente calculado anteriormente\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Genera un paso adelante\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # Saca el loss value fuera del output\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Genera un paso atras\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipea el los gradientes a 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Actualiza los parametros\n",
    "            # ¿take a step using the computed gradient?\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            print('batch loss: {0} | avg loss: {1}'.format(\n",
    "                  batch_loss, total_loss/(step+1)))\n",
    "        # Calcula el average loss sobre el training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".\n",
    "             format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(\n",
    "              format_time(time.time() - t0)))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validación\n",
    "        # ========================================\n",
    "        # Despues de completar un entrenamiento genera un paso de validacion\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"======= Validation =======\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Pone el modelo en modo evaluación\n",
    "        model.eval()\n",
    "\n",
    "        # Trackea las variables\n",
    "        eval_accuracy = 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evalua el data para un epoch mas\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # El modelo no computa los gradientes\n",
    "            with torch.no_grad():\n",
    "                # Paso adelante \n",
    "                # Devolvemos los loggits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # Los \"logits\" son el valor de salida\n",
    "            # Prioriza aplicar la funcion de activación\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Mueve los logits y labels a la CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Guarda los logits y labels del batch\n",
    "            # Utilizamos esto en la matriz de confusión\n",
    "            predict_labels = np.argmax(logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calcula la precision para este batch\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "            # Accumula la precisión total\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Informe de la precisión final de esta ejecución de validación\n",
    "        print(\"  Accuracy: {0:.2f}\".\n",
    "            format(eval_accuracy / (step+1)))\n",
    "        print(\"  Validation took: {:}\".format(\n",
    "            format_time(time.time() - t0)))\n",
    "    print(\"\")\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "    #Print la matriz de confussión\"\n",
    "    conf = confusion_matrix(all_labels, all_logits, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    print(classification_report(all_labels, all_logits, target_names=target_names))\n",
    "\n",
    "    \n",
    "#Llamamos a la funcion para entrenar el modelo\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
