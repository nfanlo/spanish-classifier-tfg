{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras \n",
    "from keras import backend as K\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
    "from keras.models import Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7234, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@dianalaa32 Es una escena de uno de los docume...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qué feo es tener que terminar con alguien; y m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oído en McDonalds \"el mejor mannequin challeng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tengo que aceptar que me esta hundiendo el con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mmm no quiero hacer spoiler pero hoy va a ver ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  @dianalaa32 Es una escena de uno de los docume...          2\n",
       "1  Qué feo es tener que terminar con alguien; y m...          0\n",
       "2  Oído en McDonalds \"el mejor mannequin challeng...          0\n",
       "3  Tengo que aceptar que me esta hundiendo el con...          1\n",
       "4  Mmm no quiero hacer spoiler pero hoy va a ver ...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/nfanlo/Desktop/Python/DataAnalysis/Datasets TFG/TFG/Task1-train-dev/train_dev/train_dev_all.csv')\n",
    "print(df.shape)\n",
    "df.isnull().sum()\n",
    "df.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review  label\n",
      "0     @dianalaa32 Es una escena de uno de los docume...      2\n",
      "1     Qué feo es tener que terminar con alguien; y m...      0\n",
      "2     Oído en McDonalds \"el mejor mannequin challeng...      0\n",
      "3     Tengo que aceptar que me esta hundiendo el con...      1\n",
      "4     Mmm no quiero hacer spoiler pero hoy va a ver ...      1\n",
      "...                                                 ...    ...\n",
      "7229  @sebatramp Acá también, Seba ???? Para peor el...      0\n",
      "7230  @Phoyu_Agustina no soy hack pero es imposible ...      1\n",
      "7231  Nadie te vende un The Last of Us Remastered po...      0\n",
      "7232  Me propuse dejar las redes, las salidas &amp; ...      1\n",
      "7233  @irenichus siii! Voy como en media hora. Me va...      2\n",
      "\n",
      "[7234 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df['review'] = df['text']\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "df['label'] = df['sentiment']\n",
    "df.drop('sentiment', axis=1, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Environment stopwords\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['review']=df['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7229        acá seba peor sismógrafo da datos tiempo real\n",
      "7230          hack imposible sacarme acc q gano confianza\n",
      "7231    nadie vende the last of us remastered menos dó...\n",
      "7232    propuse dejar salidas estudiar historia ay rar...\n",
      "7233    voy media vas reconocer musculosa havaianas ro...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['review'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo Base de Referencia: Naive Bayes\n",
    "#Nos sirve para evaluar los resultados con BERT\n",
    "#Busca el parametro alpha y utiliza la validacion cruzada para calcular\n",
    "#la exactitud del modelo\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#Funcion que CREA + FIT el Naive Bayes\n",
    "def naive_bayes(file):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        df['review'], \n",
    "        df['label'],\n",
    "        stratify = df['label'],\n",
    "        test_size=0.2)\n",
    "\n",
    "    count_vect = CountVectorizer()\n",
    "    x_train_count = count_vect.fit_transform(x_train)\n",
    "    x_test_count = count_vect.transform(x_test)\n",
    "\n",
    "    param_grid = [{'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}]\n",
    "\n",
    "    mln = MultinomialNB()\n",
    "    clf = GridSearchCV(mln, param_grid, cv=5)\n",
    "    clf.fit(x_train_count, y_train)\n",
    "\n",
    "    print(\"optimun alpha:{}\".format(clf.best_params_['alpha']))\n",
    "\n",
    "    print(\"accuracy: {}\".format(clf.score(x_test_count, y_test)))\n",
    "\n",
    "    y_test_pred = clf.predict(x_test_count)\n",
    "    conf_matrix = plot_confusion_matrix(clf, x_test_count, y_test, cmap=plt.cm.summer, normalize='true')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimun alpha:0.9\n",
      "accuracy: 0.5176226675881134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "naive_bayes(df)\n",
    "#Este modelo nos da una exactitud de 0.52 y la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7234, 2)\n",
      "                                                 review  label\n",
      "0     @dianalaa32 Es una escena de uno de los docume...      2\n",
      "1     Qué feo es tener que terminar con alguien; y m...      0\n",
      "2     Oído en McDonalds \"el mejor mannequin challeng...      0\n",
      "3     Tengo que aceptar que me esta hundiendo el con...      1\n",
      "4     Mmm no quiero hacer spoiler pero hoy va a ver ...      1\n",
      "...                                                 ...    ...\n",
      "7229  @sebatramp Acá también, Seba ???? Para peor el...      0\n",
      "7230  @Phoyu_Agustina no soy hack pero es imposible ...      1\n",
      "7231  Nadie te vende un The Last of Us Remastered po...      0\n",
      "7232  Me propuse dejar las redes, las salidas &amp; ...      1\n",
      "7233  @irenichus siii! Voy como en media hora. Me va...      2\n",
      "\n",
      "[7234 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Modelo BETO\n",
    "#Libreria transformers (modelo BERT predefinido para la clasificación (BertForSequenceClassification))\n",
    "#Libreria sera BERT + Capa de clasificación por encima\n",
    "#Debemos tokenizar nuestro dataset (tokens + attention mask + max_length)\n",
    "\n",
    "import torch\n",
    "from transformers import  BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "MAX_LEN = 38\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "df = pd.read_csv('/Users/nfanlo/Desktop/Python/DataAnalysis/Datasets TFG/TFG/Task1-train-dev/train_dev/train_dev_all.csv')\n",
    "print(df.shape)\n",
    "df.isnull().sum()\n",
    "df.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df['review'] = df['text']\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "df['label'] = df['sentiment']\n",
    "df.drop('sentiment', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review  label\n",
      "0        escena documentales respetados naturaleza dudo      2\n",
      "1      feo tener terminar dos personas aún aman entorno      0\n",
      "2     oído mcdonalds mejor mannequin challenge visto...      0\n",
      "3     aceptar hundiendo consumismo nueva macbook gop...      1\n",
      "4     mmm quiero hacer spoiler hoy va ver nuevo podr...      1\n",
      "...                                                 ...    ...\n",
      "7229      acá seba peor sismógrafo da datos tiempo real      0\n",
      "7230        hack imposible sacarme acc q gano confianza      1\n",
      "7231  nadie vende the last of us remastered menos dó...      0\n",
      "7232  propuse dejar salidas estudiar historia ay rar...      1\n",
      "7233  voy media vas reconocer musculosa havaianas ro...      2\n",
      "\n",
      "[7234 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(review):\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(review):\n",
    "    return re.sub('\\[[^]]*\\]', '', review)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(review):\n",
    "    return re.sub(r'http\\S+', '', review)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(review):\n",
    "    final_text = []\n",
    "    for i in review.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(review):\n",
    "    review = strip_html(review)\n",
    "    review = remove_between_square_brackets(review)\n",
    "    review = remove_stopwords(review)\n",
    "    return review\n",
    "#Apply function on review column\n",
    "df['review']=df['review'].apply(denoise_text)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df['review']\n",
    "label = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 5787\n",
      "Validation set length : 1447\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(review, label, stratify=label, test_size=0.2)\n",
    "\n",
    "# Report datasets lenghts\n",
    "print('Training set length : {}'.format(len(X_train)))\n",
    "print('Validation set length : {}'.format(len(X_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798     0\n",
      "4101    1\n",
      "6129    0\n",
      "2575    2\n",
      "6455    0\n",
      "       ..\n",
      "6462    2\n",
      "6123    0\n",
      "3241    0\n",
      "2362    2\n",
      "4015    0\n",
      "Name: label, Length: 5787, dtype: int64\n",
      "1451    0\n",
      "6480    1\n",
      "2918    0\n",
      "4520    1\n",
      "399     1\n",
      "       ..\n",
      "5529    2\n",
      "1394    2\n",
      "6646    0\n",
      "4929    1\n",
      "4722    2\n",
      "Name: label, Length: 1447, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 38\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO',\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=MAX_LEN,\n",
    "                   truncation=True ,pad_to_max_length=True,\n",
    "                   return_token_type_ids = False,\n",
    "                   return_attention_mask = True,)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_val_inputs, X_val_masks = preprocessing(X_val)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 13433, 13578,  ...,     0,     0,     0],\n",
      "        [  101,  4013,  9048,  ...,     0,     0,     0],\n",
      "        [  101, 11530,  2015,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  7570,  2100,  ...,     0,     0,     0],\n",
      "        [  101, 11320, 20265,  ...,     0,     0,     0],\n",
      "        [  101,  2852, 15669,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "batch_size = 16\n",
    "\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "y_val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks, y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el modelo + optimizador + definimos EPOCHS + Scheduler\n",
    "#Modelo\n",
    "model = BertForSequenceClassification.from_pretrained('/Users/nfanlo/Python/TFG Decisión multicriterio/Pruebas/BETO', num_labels=3,\n",
    " output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 4e-5, eps = 1e-6)\n",
    "\n",
    "epochs=5\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion para formatear el tiempo y otra para calcular la exactitud\n",
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Training--------------------\n",
      "\n",
      "======= Epoch 1 / 5 =======\n",
      "batch loss: 1.0353275537490845 | avg loss: 1.0353275537490845\n",
      "batch loss: 1.3424614667892456 | avg loss: 1.188894510269165\n",
      "batch loss: 0.9227267503738403 | avg loss: 1.1001719236373901\n",
      "batch loss: 1.224654197692871 | avg loss: 1.1312924921512604\n",
      "batch loss: 1.2329151630401611 | avg loss: 1.1516170263290406\n",
      "batch loss: 1.169697642326355 | avg loss: 1.154630462328593\n",
      "batch loss: 1.0714036226272583 | avg loss: 1.1427409137998308\n",
      "batch loss: 1.066037893295288 | avg loss: 1.133153036236763\n",
      "batch loss: 1.2109342813491821 | avg loss: 1.1417953968048096\n",
      "batch loss: 1.0714315176010132 | avg loss: 1.13475900888443\n",
      "batch loss: 1.1132203340530396 | avg loss: 1.1328009475361218\n",
      "batch loss: 1.086066722869873 | avg loss: 1.1289064288139343\n",
      "batch loss: 1.1252245903015137 | avg loss: 1.128623210466825\n",
      "batch loss: 1.1501150131225586 | avg loss: 1.1301583392279488\n",
      "batch loss: 1.0171935558319092 | avg loss: 1.122627353668213\n",
      "batch loss: 1.0977466106414795 | avg loss: 1.121072307229042\n",
      "batch loss: 1.1616110801696777 | avg loss: 1.1234569409314323\n",
      "batch loss: 1.3256688117980957 | avg loss: 1.134690933757358\n",
      "batch loss: 1.0837409496307373 | avg loss: 1.1320093556454307\n",
      "batch loss: 1.1218420267105103 | avg loss: 1.1315009891986847\n",
      "batch loss: 1.1113264560699463 | avg loss: 1.1305402971449352\n",
      "batch loss: 1.059456467628479 | avg loss: 1.1273092139850964\n",
      "batch loss: 1.0658001899719238 | avg loss: 1.1246349085932192\n",
      "batch loss: 1.102399230003357 | avg loss: 1.1237084219853084\n",
      "batch loss: 1.0670740604400635 | avg loss: 1.1214430475234984\n",
      "batch loss: 1.0547423362731934 | avg loss: 1.1188776355523329\n",
      "batch loss: 1.0980772972106934 | avg loss: 1.1181072526507907\n",
      "batch loss: 1.1064882278442383 | avg loss: 1.1176922874791282\n",
      "batch loss: 1.049397587776184 | avg loss: 1.115337297834199\n",
      "batch loss: 1.1879189014434814 | avg loss: 1.117756684621175\n",
      "batch loss: 1.2340326309204102 | avg loss: 1.1215075215985697\n",
      "batch loss: 1.073339581489563 | avg loss: 1.1200022734701633\n",
      "batch loss: 1.1095834970474243 | avg loss: 1.1196865529725046\n",
      "batch loss: 1.0749233961105347 | avg loss: 1.1183699895353878\n",
      "batch loss: 1.2066465616226196 | avg loss: 1.1208921773093088\n",
      "batch loss: 1.1527098417282104 | avg loss: 1.1217760013209448\n",
      "batch loss: 1.0625900030136108 | avg loss: 1.120176379745071\n",
      "batch loss: 1.1159313917160034 | avg loss: 1.1200646695337797\n",
      "batch loss: 1.0134141445159912 | avg loss: 1.1173300406871698\n",
      "batch loss: 1.0687528848648071 | avg loss: 1.1161156117916107\n",
      "batch loss: 1.178960919380188 | avg loss: 1.1176484241718199\n",
      "batch loss: 1.0746413469314575 | avg loss: 1.1166244461422874\n",
      "batch loss: 1.1144800186157227 | avg loss: 1.116574575734693\n",
      "batch loss: 1.1195240020751953 | avg loss: 1.1166416081515225\n",
      "batch loss: 1.149289846420288 | avg loss: 1.1173671245574952\n",
      "batch loss: 1.1103495359420776 | avg loss: 1.1172145682832468\n",
      "batch loss: 1.1154108047485352 | avg loss: 1.1171761903356998\n",
      "batch loss: 1.1214700937271118 | avg loss: 1.1172656466563542\n",
      "batch loss: 1.1120834350585938 | avg loss: 1.1171598872359918\n",
      "batch loss: 1.1322969198226929 | avg loss: 1.1174626278877258\n",
      "batch loss: 1.2025090456008911 | avg loss: 1.119130204705631\n",
      "batch loss: 1.08989417552948 | avg loss: 1.1185679733753204\n",
      "batch loss: 1.1036083698272705 | avg loss: 1.1182857167046025\n",
      "batch loss: 1.087028980255127 | avg loss: 1.1177068882518344\n",
      "batch loss: 1.117388367652893 | avg loss: 1.1177010969682173\n",
      "batch loss: 1.117233157157898 | avg loss: 1.117692740900176\n",
      "batch loss: 1.2614470720291138 | avg loss: 1.1202147467094554\n",
      "batch loss: 1.039490818977356 | avg loss: 1.1188229548520054\n",
      "batch loss: 1.0979286432266235 | avg loss: 1.1184688139769992\n",
      "batch loss: 1.0466442108154297 | avg loss: 1.1172717372576395\n",
      "batch loss: 1.0727205276489258 | avg loss: 1.116541389559136\n",
      "batch loss: 1.1068812608718872 | avg loss: 1.1163855810319223\n",
      "batch loss: 1.0034575462341309 | avg loss: 1.1145930725430686\n",
      "batch loss: 1.0983973741531372 | avg loss: 1.1143400147557259\n",
      "batch loss: 1.1507614850997925 | avg loss: 1.1149003450687116\n",
      "batch loss: 1.1582865715026855 | avg loss: 1.1155577121358928\n",
      "batch loss: 1.1544702053070068 | avg loss: 1.1161384956160587\n",
      "batch loss: 1.0372867584228516 | avg loss: 1.1149789112455704\n",
      "batch loss: 1.1200991868972778 | avg loss: 1.1150531181390735\n",
      "batch loss: 1.0799164772033691 | avg loss: 1.1145511661257064\n",
      "batch loss: 1.112797498703003 | avg loss: 1.1145264665845414\n",
      "batch loss: 1.109420895576477 | avg loss: 1.114455555876096\n",
      "batch loss: 1.0648514032363892 | avg loss: 1.1137760469358262\n",
      "batch loss: 1.0862473249435425 | avg loss: 1.1134040371791736\n",
      "batch loss: 1.0986584424972534 | avg loss: 1.1132074292500813\n",
      "batch loss: 1.0397562980651855 | avg loss: 1.1122409669976485\n",
      "batch loss: 1.0487910509109497 | avg loss: 1.1114169421134057\n",
      "batch loss: 1.098732352256775 | avg loss: 1.1112543191665258\n",
      "batch loss: 1.164830207824707 | avg loss: 1.1119324949723255\n",
      "batch loss: 1.135825276374817 | avg loss: 1.1122311547398567\n",
      "batch loss: 1.0421568155288696 | avg loss: 1.1113660394409557\n",
      "batch loss: 1.1164159774780273 | avg loss: 1.1114276240511638\n",
      "batch loss: 0.9848486185073853 | avg loss: 1.1099025757916003\n",
      "batch loss: 1.101001501083374 | avg loss: 1.1097966106165023\n",
      "batch loss: 1.0685423612594604 | avg loss: 1.1093112665064195\n",
      "batch loss: 1.1737515926361084 | avg loss: 1.1100605726242065\n",
      "batch loss: 1.0628540515899658 | avg loss: 1.1095179689341579\n",
      "batch loss: 1.0836944580078125 | avg loss: 1.1092245199463584\n",
      "batch loss: 1.0418462753295898 | avg loss: 1.10846746101808\n",
      "batch loss: 1.1492151021957397 | avg loss: 1.1089202125867208\n",
      "batch loss: 1.2080440521240234 | avg loss: 1.110009485548669\n",
      "batch loss: 1.3132636547088623 | avg loss: 1.1122187699960626\n",
      "batch loss: 1.1560171842575073 | avg loss: 1.1126897206870459\n",
      "batch loss: 1.1672515869140625 | avg loss: 1.1132701660724396\n",
      "batch loss: 1.0874817371368408 | avg loss: 1.1129987089257491\n",
      "batch loss: 1.0881472826004028 | avg loss: 1.1127398399015267\n",
      "batch loss: 1.0549321174621582 | avg loss: 1.11214388400009\n",
      "batch loss: 1.159061312675476 | avg loss: 1.1126226332722877\n",
      "batch loss: 1.0910847187042236 | avg loss: 1.112405078579681\n",
      "batch loss: 1.1358758211135864 | avg loss: 1.1126397860050201\n",
      "batch loss: 0.9932323694229126 | avg loss: 1.1114575343556923\n",
      "batch loss: 1.1047931909561157 | avg loss: 1.1113921976556964\n",
      "batch loss: 0.9711049795150757 | avg loss: 1.1100301858290886\n",
      "batch loss: 1.0941109657287598 | avg loss: 1.109877116405047\n",
      "batch loss: 1.1375205516815186 | avg loss: 1.1101403872172038\n",
      "batch loss: 1.0507681369781494 | avg loss: 1.1095802716489107\n",
      "batch loss: 1.0511460304260254 | avg loss: 1.1090341572449587\n",
      "batch loss: 1.140832543373108 | avg loss: 1.109328586746145\n",
      "batch loss: 1.1189141273498535 | avg loss: 1.1094165274856287\n",
      "batch loss: 0.9363282918930054 | avg loss: 1.1078429980711504\n",
      "batch loss: 1.2031813859939575 | avg loss: 1.1087019024668514\n",
      "batch loss: 1.0542104244232178 | avg loss: 1.1082153714128904\n",
      "batch loss: 1.1298613548278809 | avg loss: 1.108406928788244\n",
      "batch loss: 1.2550015449523926 | avg loss: 1.1096928464738947\n",
      "batch loss: 1.189391851425171 | avg loss: 1.110385881299558\n",
      "batch loss: 1.1675591468811035 | avg loss: 1.110878754278709\n",
      "batch loss: 1.1285301446914673 | avg loss: 1.1110296208634336\n",
      "batch loss: 1.0548492670059204 | avg loss: 1.1105535161697258\n",
      "batch loss: 1.1345293521881104 | avg loss: 1.1107549937833257\n",
      "batch loss: 1.0614567995071411 | avg loss: 1.1103441754976908\n",
      "batch loss: 1.1027143001556396 | avg loss: 1.110281118676682\n",
      "batch loss: 1.13649320602417 | avg loss: 1.1104959718516616\n",
      "batch loss: 1.0718533992767334 | avg loss: 1.1101818045949547\n",
      "batch loss: 1.0866563320159912 | avg loss: 1.1099920830418986\n",
      "batch loss: 1.170797348022461 | avg loss: 1.110478525161743\n",
      "batch loss: 1.1935378313064575 | avg loss: 1.1111377260041615\n",
      "batch loss: 1.1368712186813354 | avg loss: 1.1113403519307534\n",
      "batch loss: 0.9839741587638855 | avg loss: 1.1103453035466373\n",
      "batch loss: 1.0830107927322388 | avg loss: 1.1101334081139675\n",
      "batch loss: 1.0590181350708008 | avg loss: 1.1097402137059431\n",
      "batch loss: 1.063761830329895 | avg loss: 1.1093892336801718\n",
      "batch loss: 1.0652045011520386 | avg loss: 1.109054500857989\n",
      "batch loss: 1.1227803230285645 | avg loss: 1.1091577025284445\n",
      "batch loss: 1.041978359222412 | avg loss: 1.1086563641455636\n",
      "batch loss: 1.209421157836914 | avg loss: 1.1094027700247588\n",
      "batch loss: 0.9437494874000549 | avg loss: 1.1081847311819302\n",
      "batch loss: 1.152427315711975 | avg loss: 1.1085076697551421\n",
      "batch loss: 1.0791043043136597 | avg loss: 1.108294601889624\n",
      "batch loss: 1.0108155012130737 | avg loss: 1.1075933133955482\n",
      "batch loss: 1.070683240890503 | avg loss: 1.1073296700205122\n",
      "batch loss: 0.996180534362793 | avg loss: 1.106541378278259\n",
      "batch loss: 1.029099941253662 | avg loss: 1.1059960160456912\n",
      "batch loss: 1.1039983034133911 | avg loss: 1.1059820460272836\n",
      "batch loss: 1.1178510189056396 | avg loss: 1.1060644694500499\n",
      "batch loss: 1.2339587211608887 | avg loss: 1.1069464987721938\n",
      "batch loss: 1.1687097549438477 | avg loss: 1.1073695347733694\n",
      "batch loss: 0.9254947304725647 | avg loss: 1.1061322912066973\n",
      "batch loss: 1.0247968435287476 | avg loss: 1.1055827273710355\n",
      "batch loss: 1.0016318559646606 | avg loss: 1.104885070515959\n",
      "batch loss: 1.0797641277313232 | avg loss: 1.1047175975640615\n",
      "batch loss: 0.9938466548919678 | avg loss: 1.103983352910604\n",
      "batch loss: 1.13292396068573 | avg loss: 1.1041737516459667\n",
      "batch loss: 1.2166447639465332 | avg loss: 1.104908856301526\n",
      "batch loss: 0.9535990357398987 | avg loss: 1.1039263249991775\n",
      "batch loss: 1.2324340343475342 | avg loss: 1.1047554069949734\n",
      "batch loss: 1.2354775667190552 | avg loss: 1.1055933695573072\n",
      "batch loss: 1.0131036043167114 | avg loss: 1.1050042627723353\n",
      "batch loss: 1.0938704013824463 | avg loss: 1.104933795295184\n",
      "batch loss: 1.0521737337112427 | avg loss: 1.1046019710084927\n",
      "batch loss: 1.0842639207839966 | avg loss: 1.1044748581945896\n",
      "batch loss: 1.0191032886505127 | avg loss: 1.1039445999986637\n",
      "batch loss: 1.12489914894104 | avg loss: 1.1040739490662093\n",
      "batch loss: 0.9845213294029236 | avg loss: 1.1033404974118333\n",
      "batch loss: 1.1008265018463135 | avg loss: 1.10332516817058\n",
      "batch loss: 1.237743616104126 | avg loss: 1.1041398254307833\n",
      "batch loss: 1.1174802780151367 | avg loss: 1.1042201896029782\n",
      "batch loss: 1.3512744903564453 | avg loss: 1.1056995566733583\n",
      "batch loss: 1.1733890771865845 | avg loss: 1.106102470485937\n",
      "batch loss: 1.162018060684204 | avg loss: 1.1064333319664001\n",
      "batch loss: 1.0041669607162476 | avg loss: 1.1058317650766933\n",
      "batch loss: 1.0880740880966187 | avg loss: 1.1057279190124825\n",
      "batch loss: 1.0924837589263916 | avg loss: 1.1056509180817493\n",
      "batch loss: 1.0636886358261108 | avg loss: 1.105408361536919\n",
      "batch loss: 1.1751271486282349 | avg loss: 1.105809044221352\n",
      "batch loss: 1.0644108057022095 | avg loss: 1.1055724828583853\n",
      "batch loss: 1.1010448932647705 | avg loss: 1.1055467579175124\n",
      "batch loss: 1.0428900718688965 | avg loss: 1.1051927653409668\n",
      "batch loss: 1.0974961519241333 | avg loss: 1.1051495259397486\n",
      "batch loss: 1.062442660331726 | avg loss: 1.104910940098363\n",
      "batch loss: 1.0646657943725586 | avg loss: 1.1046873559554418\n",
      "batch loss: 1.0875942707061768 | avg loss: 1.104592919020363\n",
      "batch loss: 1.0430961847305298 | avg loss: 1.1042550248759133\n",
      "batch loss: 1.073330044746399 | avg loss: 1.104086035913457\n",
      "batch loss: 1.0192995071411133 | avg loss: 1.1036252395614334\n",
      "batch loss: 1.0798295736312866 | avg loss: 1.1034966143401894\n",
      "batch loss: 1.0693087577819824 | avg loss: 1.1033128086597688\n",
      "batch loss: 1.1787043809890747 | avg loss: 1.1037159721481609\n",
      "batch loss: 1.158766269683838 | avg loss: 1.1040087928797335\n",
      "batch loss: 0.9844775199890137 | avg loss: 1.1033763522824283\n",
      "batch loss: 1.0430243015289307 | avg loss: 1.1030587099100415\n",
      "batch loss: 1.3283926248550415 | avg loss: 1.104238468627031\n",
      "batch loss: 1.1407829523086548 | avg loss: 1.1044288044795394\n",
      "batch loss: 1.1160274744033813 | avg loss: 1.1044889012148962\n",
      "batch loss: 0.9946901798248291 | avg loss: 1.1039229284242256\n",
      "batch loss: 1.0260541439056396 | avg loss: 1.1035236013241303\n",
      "batch loss: 1.2060019969940186 | avg loss: 1.1040464502816298\n",
      "batch loss: 1.058210015296936 | avg loss: 1.103813778022824\n",
      "batch loss: 1.0646634101867676 | avg loss: 1.103616048892339\n",
      "batch loss: 1.0856599807739258 | avg loss: 1.1035258173942566\n",
      "batch loss: 0.9284743070602417 | avg loss: 1.1026505598425864\n",
      "batch loss: 1.0441149473190308 | avg loss: 1.102359337889733\n",
      "batch loss: 1.15520441532135 | avg loss: 1.102620947183949\n",
      "batch loss: 1.072646141052246 | avg loss: 1.102473288040443\n",
      "batch loss: 1.00398850440979 | avg loss: 1.101990519493234\n",
      "batch loss: 1.1974143981933594 | avg loss: 1.1024560018283565\n",
      "batch loss: 0.9886857867240906 | avg loss: 1.101903719230763\n",
      "batch loss: 1.158815860748291 | avg loss: 1.10217865711249\n",
      "batch loss: 0.9966821074485779 | avg loss: 1.1016714621621828\n",
      "batch loss: 1.193398356437683 | avg loss: 1.102110346823788\n",
      "batch loss: 1.0479364395141602 | avg loss: 1.1018523758365995\n",
      "batch loss: 1.2283852100372314 | avg loss: 1.1024520575152754\n",
      "batch loss: 1.1955643892288208 | avg loss: 1.1028912666271318\n",
      "batch loss: 1.1628936529159546 | avg loss: 1.1031729679712108\n",
      "batch loss: 1.1117125749588013 | avg loss: 1.1032128726767603\n",
      "batch loss: 1.0578817129135132 | avg loss: 1.1030020300732102\n",
      "batch loss: 0.9671317934989929 | avg loss: 1.1023730012001816\n",
      "batch loss: 1.0398415327072144 | avg loss: 1.1020848377509052\n",
      "batch loss: 1.1265500783920288 | avg loss: 1.1021970636254057\n",
      "batch loss: 1.0059555768966675 | avg loss: 1.1017576047818955\n",
      "batch loss: 1.1065181493759155 | avg loss: 1.1017792436209592\n",
      "batch loss: 1.0817410945892334 | avg loss: 1.1016885732633497\n",
      "batch loss: 1.1198636293411255 | avg loss: 1.1017704428853214\n",
      "batch loss: 1.1396982669830322 | avg loss: 1.101940522814011\n",
      "batch loss: 1.074625015258789 | avg loss: 1.1018185785838537\n",
      "batch loss: 0.9937567114830017 | avg loss: 1.101338303618961\n",
      "batch loss: 1.0977147817611694 | avg loss: 1.1013222703364043\n",
      "batch loss: 1.0856250524520874 | avg loss: 1.1012531195968258\n",
      "batch loss: 1.1076589822769165 | avg loss: 1.1012812154857736\n",
      "batch loss: 1.0070374011993408 | avg loss: 1.1008696704452214\n",
      "batch loss: 1.094356656074524 | avg loss: 1.1008413529914358\n",
      "batch loss: 1.125178575515747 | avg loss: 1.1009467089330995\n",
      "batch loss: 1.2774766683578491 | avg loss: 1.1017076139306199\n",
      "batch loss: 0.8672373294830322 | avg loss: 1.1007013036969393\n",
      "batch loss: 1.1856558322906494 | avg loss: 1.1010643572379382\n",
      "batch loss: 1.0642130374908447 | avg loss: 1.1009075431113546\n",
      "batch loss: 1.0056108236312866 | avg loss: 1.1005037434525409\n",
      "batch loss: 1.0309646129608154 | avg loss: 1.100210329399833\n",
      "batch loss: 1.0774043798446655 | avg loss: 1.1001145060823745\n",
      "batch loss: 1.023777723312378 | avg loss: 1.0997951053176465\n",
      "batch loss: 0.9794450402259827 | avg loss: 1.0992936467130978\n",
      "batch loss: 0.9365437030792236 | avg loss: 1.0986183357436627\n",
      "batch loss: 1.1418253183364868 | avg loss: 1.0987968769940464\n",
      "batch loss: 1.0563688278198242 | avg loss: 1.098622275968638\n",
      "batch loss: 1.0229958295822144 | avg loss: 1.0983123315162346\n",
      "batch loss: 1.003013014793396 | avg loss: 1.0979233547132843\n",
      "batch loss: 1.0819227695465088 | avg loss: 1.097858311684151\n",
      "batch loss: 1.2256144285202026 | avg loss: 1.0983755429264022\n",
      "batch loss: 1.0259449481964111 | avg loss: 1.0980834840766844\n",
      "batch loss: 0.882652759552002 | avg loss: 1.0972183004440552\n",
      "batch loss: 0.9308599829673767 | avg loss: 1.0965528671741485\n",
      "batch loss: 1.1152844429016113 | avg loss: 1.0966274949658914\n",
      "batch loss: 0.9670699834823608 | avg loss: 1.0961133778568297\n",
      "batch loss: 1.1030340194702148 | avg loss: 1.0961407321715073\n",
      "batch loss: 1.213581919670105 | avg loss: 1.096603099051423\n",
      "batch loss: 1.035326600074768 | avg loss: 1.096362799055436\n",
      "batch loss: 1.1397124528884888 | avg loss: 1.0965321336407214\n",
      "batch loss: 1.0979803800582886 | avg loss: 1.0965377688407898\n",
      "batch loss: 0.9304264783859253 | avg loss: 1.0958939266297245\n",
      "batch loss: 0.9783652424812317 | avg loss: 1.0954401479264484\n",
      "batch loss: 1.0484727621078491 | avg loss: 1.0952595041348383\n",
      "batch loss: 1.2422335147857666 | avg loss: 1.0958226229495929\n",
      "batch loss: 1.0418434143066406 | avg loss: 1.0956165954356885\n",
      "batch loss: 1.0570141077041626 | avg loss: 1.0954698179157967\n",
      "batch loss: 1.2144616842269897 | avg loss: 1.095920544682127\n",
      "batch loss: 0.9691753387451172 | avg loss: 1.0954422608861383\n",
      "batch loss: 1.0051636695861816 | avg loss: 1.0951028676857626\n",
      "batch loss: 1.0842375755310059 | avg loss: 1.095062173707655\n",
      "batch loss: 1.0418356657028198 | avg loss: 1.0948635673345024\n",
      "batch loss: 1.1673365831375122 | avg loss: 1.0951329837501271\n",
      "batch loss: 0.8716726899147034 | avg loss: 1.094305353032218\n",
      "batch loss: 1.122036099433899 | avg loss: 1.0944076805097152\n",
      "batch loss: 1.0312001705169678 | avg loss: 1.0941752999582712\n",
      "batch loss: 1.0104554891586304 | avg loss: 1.0938686339846462\n",
      "batch loss: 0.9616576433181763 | avg loss: 1.0933861121208999\n",
      "batch loss: 1.1761075258255005 | avg loss: 1.093686917261644\n",
      "batch loss: 0.960259199142456 | avg loss: 1.0932034835003424\n",
      "batch loss: 0.9634841084480286 | avg loss: 1.092735182507374\n",
      "batch loss: 1.1363554000854492 | avg loss: 1.0928920897648489\n",
      "batch loss: 1.3664618730545044 | avg loss: 1.0938726266225178\n",
      "batch loss: 1.0657579898834229 | avg loss: 1.0937722172055926\n",
      "batch loss: 1.2264583110809326 | avg loss: 1.094244409710487\n",
      "batch loss: 1.110996961593628 | avg loss: 1.0943038159228387\n",
      "batch loss: 1.0731778144836426 | avg loss: 1.0942291657410745\n",
      "batch loss: 1.1434545516967773 | avg loss: 1.0944024945648623\n",
      "batch loss: 1.1191596984863281 | avg loss: 1.0944893619470428\n",
      "batch loss: 1.1187655925750732 | avg loss: 1.0945742438723158\n",
      "batch loss: 1.0789517164230347 | avg loss: 1.0945198099787643\n",
      "batch loss: 1.059119462966919 | avg loss: 1.0943968921071954\n",
      "batch loss: 1.1309432983398438 | avg loss: 1.0945233502602494\n",
      "batch loss: 1.1194566488265991 | avg loss: 1.0946093271518575\n",
      "batch loss: 1.0056333541870117 | avg loss: 1.09430356779459\n",
      "batch loss: 1.0561487674713135 | avg loss: 1.0941729006701952\n",
      "batch loss: 1.0079158544540405 | avg loss: 1.0938785080209934\n",
      "batch loss: 1.0656929016113281 | avg loss: 1.0937826386114367\n",
      "batch loss: 1.0146903991699219 | avg loss: 1.0935145293251942\n",
      "batch loss: 1.1609110832214355 | avg loss: 1.0937422203856546\n",
      "batch loss: 0.9845306873321533 | avg loss: 1.0933745047861478\n",
      "batch loss: 1.0577996969223022 | avg loss: 1.093255126236269\n",
      "batch loss: 1.1455540657043457 | avg loss: 1.0934300390772995\n",
      "batch loss: 1.054353952407837 | avg loss: 1.093299785455068\n",
      "batch loss: 1.0370975732803345 | avg loss: 1.0931130671421951\n",
      "batch loss: 1.0077383518218994 | avg loss: 1.0928303694093464\n",
      "batch loss: 1.0112123489379883 | avg loss: 1.0925610030051505\n",
      "batch loss: 0.9962939023971558 | avg loss: 1.0922443349110453\n",
      "batch loss: 1.0396275520324707 | avg loss: 1.0920718208688205\n",
      "batch loss: 0.9419086575508118 | avg loss: 1.0915810915769315\n",
      "batch loss: 1.1727286577224731 | avg loss: 1.0918454158966238\n",
      "batch loss: 1.121259331703186 | avg loss: 1.0919409156232684\n",
      "batch loss: 1.002276062965393 | avg loss: 1.0916507381065763\n",
      "batch loss: 0.8291988968849182 | avg loss: 1.090804119263926\n",
      "batch loss: 1.1172542572021484 | avg loss: 1.0908891679389683\n",
      "batch loss: 1.17166006565094 | avg loss: 1.0911480490213785\n",
      "batch loss: 1.1417431831359863 | avg loss: 1.0913096948172718\n",
      "batch loss: 0.9624541401863098 | avg loss: 1.090899326808893\n",
      "batch loss: 0.9715868234634399 | avg loss: 1.0905205569570027\n",
      "batch loss: 1.0428869724273682 | avg loss: 1.0903698177654533\n",
      "batch loss: 0.8838108777999878 | avg loss: 1.0897182122766031\n",
      "batch loss: 1.0907707214355469 | avg loss: 1.0897215220538325\n",
      "batch loss: 1.057563066482544 | avg loss: 1.08962071184828\n",
      "batch loss: 1.1357802152633667 | avg loss: 1.089764960296452\n",
      "batch loss: 1.123858094215393 | avg loss: 1.0898711694363865\n",
      "batch loss: 1.0209451913833618 | avg loss: 1.0896571136039237\n",
      "batch loss: 1.000730276107788 | avg loss: 1.089381798317558\n",
      "batch loss: 1.181051254272461 | avg loss: 1.0896647287371717\n",
      "batch loss: 1.0981166362762451 | avg loss: 1.0896907346065228\n",
      "batch loss: 0.9260879755020142 | avg loss: 1.0891888856522145\n",
      "batch loss: 1.0703672170639038 | avg loss: 1.0891313270326783\n",
      "batch loss: 1.066371202468872 | avg loss: 1.0890619364090082\n",
      "batch loss: 1.1650099754333496 | avg loss: 1.0892927815124256\n",
      "batch loss: 1.0871305465698242 | avg loss: 1.089286229285327\n",
      "batch loss: 1.0133929252624512 | avg loss: 1.0890569443789133\n",
      "batch loss: 1.131317138671875 | avg loss: 1.0891842341207596\n",
      "batch loss: 0.9633943438529968 | avg loss: 1.0888064867025382\n",
      "batch loss: 1.0867722034454346 | avg loss: 1.0888003960341037\n",
      "batch loss: 0.9992958903312683 | avg loss: 1.08853321840514\n",
      "batch loss: 1.0355939865112305 | avg loss: 1.0883756611673605\n",
      "batch loss: 1.1946418285369873 | avg loss: 1.0886909910408609\n",
      "batch loss: 1.0520296096801758 | avg loss: 1.0885825254155335\n",
      "batch loss: 0.975654125213623 | avg loss: 1.088249403291044\n",
      "batch loss: 1.0814167261123657 | avg loss: 1.0882293071816949\n",
      "batch loss: 1.1157819032669067 | avg loss: 1.0883101065837044\n",
      "batch loss: 1.0309473276138306 | avg loss: 1.088142379159816\n",
      "batch loss: 1.0360642671585083 | avg loss: 1.0879905479294913\n",
      "batch loss: 1.054601788520813 | avg loss: 1.0878934875823731\n",
      "batch loss: 1.058469533920288 | avg loss: 1.0878082007601642\n",
      "batch loss: 0.9334200024604797 | avg loss: 1.087361992094558\n",
      "batch loss: 1.1980055570602417 | avg loss: 1.0876808496304822\n",
      "batch loss: 1.1583929061889648 | avg loss: 1.0878840451953056\n",
      "batch loss: 1.1468098163604736 | avg loss: 1.0880528869464952\n",
      "batch loss: 1.2147738933563232 | avg loss: 1.0884149469648088\n",
      "batch loss: 1.242771863937378 | avg loss: 1.088854710261027\n",
      "batch loss: 1.1726070642471313 | avg loss: 1.089092643084851\n",
      "batch loss: 1.0655198097229004 | avg loss: 1.0890258645200865\n",
      "batch loss: 1.018836259841919 | avg loss: 1.0888275888006567\n",
      "batch loss: 1.0792713165283203 | avg loss: 1.088800669723833\n",
      "batch loss: 1.1074509620666504 | avg loss: 1.0888530581854703\n",
      "batch loss: 1.0737401247024536 | avg loss: 1.0888107250384589\n",
      "batch loss: 1.1352778673171997 | avg loss: 1.0889405215252712\n",
      "batch loss: 1.0982933044433594 | avg loss: 1.0889665738453773\n",
      "batch loss: 0.9607911109924316 | avg loss: 1.0886105308930079\n",
      "batch loss: 1.207693099975586 | avg loss: 1.088940399505425\n",
      "batch loss: 1.0645350217819214 | avg loss: 1.088872981334918\n",
      "\n",
      "  Average training loss: 1.09\n",
      "  Training epoch took: 0:23:29\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.44\n",
      "  Validation took: 0:01:40\n",
      "\n",
      "======= Epoch 2 / 5 =======\n",
      "batch loss: 1.0091627836227417 | avg loss: 1.0091627836227417\n",
      "batch loss: 1.032874584197998 | avg loss: 1.0210186839103699\n",
      "batch loss: 1.0436867475509644 | avg loss: 1.0285747051239014\n",
      "batch loss: 1.0806504487991333 | avg loss: 1.0415936410427094\n",
      "batch loss: 1.0629324913024902 | avg loss: 1.0458614110946656\n",
      "batch loss: 0.9246433973312378 | avg loss: 1.025658408800761\n",
      "batch loss: 1.0670591592788696 | avg loss: 1.031572801726205\n",
      "batch loss: 0.9749274849891663 | avg loss: 1.0244921371340752\n",
      "batch loss: 0.9968228340148926 | avg loss: 1.0214177701208327\n",
      "batch loss: 1.0203778743743896 | avg loss: 1.0213137805461883\n",
      "batch loss: 1.2690801620483398 | avg loss: 1.043837997046384\n",
      "batch loss: 1.042203426361084 | avg loss: 1.043701782822609\n",
      "batch loss: 1.3300737142562866 | avg loss: 1.0657303929328918\n",
      "batch loss: 1.1570677757263184 | avg loss: 1.0722544917038508\n",
      "batch loss: 1.1407114267349243 | avg loss: 1.0768182873725891\n",
      "batch loss: 1.0546094179153442 | avg loss: 1.0754302330315113\n",
      "batch loss: 0.9237908124923706 | avg loss: 1.0665102671174442\n",
      "batch loss: 0.9458920359611511 | avg loss: 1.0598092542754278\n",
      "batch loss: 0.9564187526702881 | avg loss: 1.054367648927789\n",
      "batch loss: 1.043591022491455 | avg loss: 1.0538288176059722\n",
      "batch loss: 0.7820713520050049 | avg loss: 1.040887985910688\n",
      "batch loss: 1.0628912448883057 | avg loss: 1.0418881340460344\n",
      "batch loss: 0.8826342225074768 | avg loss: 1.0349640509356623\n",
      "batch loss: 0.8247786164283752 | avg loss: 1.0262063244978588\n",
      "batch loss: 0.8456431031227112 | avg loss: 1.0189837956428527\n",
      "batch loss: 1.079432487487793 | avg loss: 1.0213087453291967\n",
      "batch loss: 0.8874961733818054 | avg loss: 1.01635272414596\n",
      "batch loss: 1.0676825046539307 | avg loss: 1.0181859305926733\n",
      "batch loss: 1.0054965019226074 | avg loss: 1.0177483640868088\n",
      "batch loss: 0.8391925096511841 | avg loss: 1.011796502272288\n",
      "batch loss: 0.9784446954727173 | avg loss: 1.010720637536818\n",
      "batch loss: 0.6233117580413818 | avg loss: 0.9986141100525856\n",
      "batch loss: 0.9904319047927856 | avg loss: 0.9983661644386522\n",
      "batch loss: 1.0601807832717896 | avg loss: 1.0001842414631563\n",
      "batch loss: 1.1233296394348145 | avg loss: 1.0037026814052037\n",
      "batch loss: 1.2760958671569824 | avg loss: 1.0112691587871976\n",
      "batch loss: 1.1708574295043945 | avg loss: 1.0155823552930676\n",
      "batch loss: 1.0170565843582153 | avg loss: 1.015621150794782\n",
      "batch loss: 0.7877911329269409 | avg loss: 1.0097793554648375\n",
      "batch loss: 1.108436107635498 | avg loss: 1.012245774269104\n",
      "batch loss: 1.2157536745071411 | avg loss: 1.017209381591983\n",
      "batch loss: 1.0340931415557861 | avg loss: 1.0176113758768355\n",
      "batch loss: 1.1213791370391846 | avg loss: 1.020024579624797\n",
      "batch loss: 1.060607671737671 | avg loss: 1.0209469226273624\n",
      "batch loss: 0.8756246566772461 | avg loss: 1.017717538939582\n",
      "batch loss: 1.1141541004180908 | avg loss: 1.0198139859282451\n",
      "batch loss: 1.120815634727478 | avg loss: 1.0219629571792928\n",
      "batch loss: 0.8683984875679016 | avg loss: 1.0187636973957221\n",
      "batch loss: 1.1519097089767456 | avg loss: 1.021480962938192\n",
      "batch loss: 0.8679363131523132 | avg loss: 1.0184100699424743\n",
      "batch loss: 1.0151256322860718 | avg loss: 1.0183456692041135\n",
      "batch loss: 1.2167162895202637 | avg loss: 1.022160488825578\n",
      "batch loss: 0.9204183220863342 | avg loss: 1.020240825302196\n",
      "batch loss: 1.0057605504989624 | avg loss: 1.019972672065099\n",
      "batch loss: 1.0489710569381714 | avg loss: 1.0204999154264276\n",
      "batch loss: 0.9571225643157959 | avg loss: 1.0193681770137377\n",
      "batch loss: 1.3122063875198364 | avg loss: 1.0245056894787572\n",
      "batch loss: 0.8534498810768127 | avg loss: 1.0215564514028614\n",
      "batch loss: 0.804334819316864 | avg loss: 1.0178747288251326\n",
      "batch loss: 0.8650510311126709 | avg loss: 1.0153276671965916\n",
      "batch loss: 1.1592166423797607 | avg loss: 1.0176865028553321\n",
      "batch loss: 0.8515586256980896 | avg loss: 1.0150070209656992\n",
      "batch loss: 0.8261435031890869 | avg loss: 1.0120091873501975\n",
      "batch loss: 0.845094621181488 | avg loss: 1.0094011472538114\n",
      "batch loss: 1.1726619005203247 | avg loss: 1.0119128511502193\n",
      "batch loss: 1.1457377672195435 | avg loss: 1.013940501393694\n",
      "batch loss: 1.2663421630859375 | avg loss: 1.017707690374175\n",
      "batch loss: 0.7802898287773132 | avg loss: 1.0142162512330448\n",
      "batch loss: 0.7578087449073792 | avg loss: 1.0105002004167307\n",
      "batch loss: 1.1028543710708618 | avg loss: 1.0118195457117898\n",
      "batch loss: 1.1968055963516235 | avg loss: 1.0144249830447452\n",
      "batch loss: 1.1222491264343262 | avg loss: 1.0159225405918226\n",
      "batch loss: 0.938971757888794 | avg loss: 1.0148684202808222\n",
      "batch loss: 0.9550692439079285 | avg loss: 1.0140603233028103\n",
      "batch loss: 1.1167545318603516 | avg loss: 1.0154295794169108\n",
      "batch loss: 1.0010093450546265 | avg loss: 1.0152398394910913\n",
      "batch loss: 0.8576694130897522 | avg loss: 1.013193470317048\n",
      "batch loss: 1.0236302614212036 | avg loss: 1.0133272753312037\n",
      "batch loss: 0.9790688157081604 | avg loss: 1.0128936239435702\n",
      "batch loss: 0.839139997959137 | avg loss: 1.0107217036187648\n",
      "batch loss: 0.972427487373352 | avg loss: 1.0102489355169697\n",
      "batch loss: 1.0173003673553467 | avg loss: 1.0103349285881693\n",
      "batch loss: 0.9422810077667236 | avg loss: 1.0095150018312844\n",
      "batch loss: 0.8704521656036377 | avg loss: 1.0078594918761934\n",
      "batch loss: 0.8981461524963379 | avg loss: 1.0065687467070188\n",
      "batch loss: 1.0200427770614624 | avg loss: 1.006725421478582\n",
      "batch loss: 0.9497513771057129 | avg loss: 1.0060705474053306\n",
      "batch loss: 1.0566110610961914 | avg loss: 1.0066448714245448\n",
      "batch loss: 0.7811553478240967 | avg loss: 1.004111281271731\n",
      "batch loss: 1.219299554824829 | avg loss: 1.0065022620889876\n",
      "batch loss: 1.2401121854782104 | avg loss: 1.0090694041042538\n",
      "batch loss: 0.9477911591529846 | avg loss: 1.0084033362243487\n",
      "batch loss: 0.990927517414093 | avg loss: 1.0082154241941308\n",
      "batch loss: 1.090038776397705 | avg loss: 1.009085885387786\n",
      "batch loss: 1.0628100633621216 | avg loss: 1.0096514030506736\n",
      "batch loss: 0.8789770007133484 | avg loss: 1.00829021135966\n",
      "batch loss: 0.9260936975479126 | avg loss: 1.0074428246193325\n",
      "batch loss: 1.0189398527145386 | avg loss: 1.007560141232549\n",
      "batch loss: 0.9270076751708984 | avg loss: 1.006746479959199\n",
      "batch loss: 1.2627384662628174 | avg loss: 1.0093063998222351\n",
      "batch loss: 0.8743993043899536 | avg loss: 1.007970686006074\n",
      "batch loss: 1.0163840055465698 | avg loss: 1.0080531695309807\n",
      "batch loss: 0.8882942795753479 | avg loss: 1.0068904618615087\n",
      "batch loss: 1.029465913772583 | avg loss: 1.0071075335144997\n",
      "batch loss: 1.0235095024108887 | avg loss: 1.0072637427420843\n",
      "batch loss: 0.8944573402404785 | avg loss: 1.0061995313977294\n",
      "batch loss: 1.1042842864990234 | avg loss: 1.0071162113519472\n",
      "batch loss: 1.3548107147216797 | avg loss: 1.010335604901667\n",
      "batch loss: 1.0699928998947144 | avg loss: 1.0108829195346307\n",
      "batch loss: 0.9087006449699402 | avg loss: 1.009953989765861\n",
      "batch loss: 0.9597088694572449 | avg loss: 1.0095013310243417\n",
      "batch loss: 0.9733377695083618 | avg loss: 1.0091784420822347\n",
      "batch loss: 1.0434224605560303 | avg loss: 1.0094814864935073\n",
      "batch loss: 1.000511884689331 | avg loss: 1.0094028057759268\n",
      "batch loss: 0.8492358326911926 | avg loss: 1.0080100494882336\n",
      "batch loss: 1.0531197786331177 | avg loss: 1.0083989264636204\n",
      "batch loss: 0.9022876024246216 | avg loss: 1.0074919920701246\n",
      "batch loss: 0.9915405511856079 | avg loss: 1.0073568103677135\n",
      "batch loss: 1.1468493938446045 | avg loss: 1.008529016951553\n",
      "batch loss: 0.9118510484695435 | avg loss: 1.007723367214203\n",
      "batch loss: 1.0988874435424805 | avg loss: 1.0084767893326183\n",
      "batch loss: 1.1177308559417725 | avg loss: 1.009372314468759\n",
      "batch loss: 0.945645809173584 | avg loss: 1.0088542127996925\n",
      "batch loss: 1.074213981628418 | avg loss: 1.0093813077096017\n",
      "batch loss: 1.0899109840393066 | avg loss: 1.0100255451202393\n",
      "batch loss: 1.2183349132537842 | avg loss: 1.01167879407368\n",
      "batch loss: 1.0264474153518677 | avg loss: 1.0117950824302013\n",
      "batch loss: 1.017075538635254 | avg loss: 1.0118363359943032\n",
      "batch loss: 0.9599507451057434 | avg loss: 1.0114341221114462\n",
      "batch loss: 0.9793753027915955 | avg loss: 1.0111875158089858\n",
      "batch loss: 0.9366660118103027 | avg loss: 1.0106186493662477\n",
      "batch loss: 0.9778063893318176 | avg loss: 1.0103700716387143\n",
      "batch loss: 1.0609835386276245 | avg loss: 1.0107506240220894\n",
      "batch loss: 0.8496124744415283 | avg loss: 1.009548100517757\n",
      "batch loss: 0.9902501106262207 | avg loss: 1.0094051524444863\n",
      "batch loss: 0.7332865595817566 | avg loss: 1.0073748686734367\n",
      "batch loss: 1.036678433418274 | avg loss: 1.0075887633066107\n",
      "batch loss: 1.1354542970657349 | avg loss: 1.008515325145445\n",
      "batch loss: 0.8728859424591064 | avg loss: 1.007539574190867\n",
      "batch loss: 0.9324933290481567 | avg loss: 1.007003529582705\n",
      "batch loss: 0.8493099212646484 | avg loss: 1.0058851351974705\n",
      "batch loss: 0.8189973831176758 | avg loss: 1.004569024267331\n",
      "batch loss: 0.9770189523696899 | avg loss: 1.004376366421893\n",
      "batch loss: 0.901565670967102 | avg loss: 1.0036624032590125\n",
      "batch loss: 1.0697665214538574 | avg loss: 1.0041182937293218\n",
      "batch loss: 0.8791242837905884 | avg loss: 1.003262170373577\n",
      "batch loss: 0.9500802755355835 | avg loss: 1.0029003887760395\n",
      "batch loss: 0.9965068101882935 | avg loss: 1.002857188920717\n",
      "batch loss: 0.9751497507095337 | avg loss: 1.0026712329595682\n",
      "batch loss: 0.9179385900497437 | avg loss: 1.0021063486735027\n",
      "batch loss: 1.0356498956680298 | avg loss: 1.0023284913688306\n",
      "batch loss: 0.9749270677566528 | avg loss: 1.0021482188450663\n",
      "batch loss: 1.2426977157592773 | avg loss: 1.003720437779146\n",
      "batch loss: 0.7183041572570801 | avg loss: 1.0018670853082237\n",
      "batch loss: 1.0146410465240479 | avg loss: 1.001949497961229\n",
      "batch loss: 1.122783899307251 | avg loss: 1.0027240774570367\n",
      "batch loss: 0.910161554813385 | avg loss: 1.0021345072491155\n",
      "batch loss: 0.9257432818412781 | avg loss: 1.0016510184807113\n",
      "batch loss: 1.1723065376281738 | avg loss: 1.0027243236325822\n",
      "batch loss: 1.0051634311676025 | avg loss: 1.002739568054676\n",
      "batch loss: 0.906454861164093 | avg loss: 1.0021415263969706\n",
      "batch loss: 1.0588648319244385 | avg loss: 1.0024916702582511\n",
      "batch loss: 0.8688631057739258 | avg loss: 1.0016718631141757\n",
      "batch loss: 1.0440882444381714 | avg loss: 1.0019304995856635\n",
      "batch loss: 1.3959835767745972 | avg loss: 1.004318700053475\n",
      "batch loss: 1.062734603881836 | avg loss: 1.0046706030885857\n",
      "batch loss: 1.359786868095398 | avg loss: 1.0067970477892254\n",
      "batch loss: 1.1608456373214722 | avg loss: 1.0077140036792982\n",
      "batch loss: 0.8947577476501465 | avg loss: 1.007045623466108\n",
      "batch loss: 0.8294342756271362 | avg loss: 1.006000850831761\n",
      "batch loss: 1.015869140625 | avg loss: 1.0060585601287975\n",
      "batch loss: 0.9528699517250061 | avg loss: 1.0057493240334268\n",
      "batch loss: 1.0383317470550537 | avg loss: 1.005937661738754\n",
      "batch loss: 1.1981834173202515 | avg loss: 1.0070425224030155\n",
      "batch loss: 0.7848197817802429 | avg loss: 1.0057726781708853\n",
      "batch loss: 1.0075175762176514 | avg loss: 1.0057825923643329\n",
      "batch loss: 0.9691542387008667 | avg loss: 1.005575652513127\n",
      "batch loss: 1.0788003206253052 | avg loss: 1.0059870270530828\n",
      "batch loss: 1.0401121377944946 | avg loss: 1.0061776701298506\n",
      "batch loss: 1.0814831256866455 | avg loss: 1.0065960337718327\n",
      "batch loss: 1.0844062566757202 | avg loss: 1.0070259245061084\n",
      "batch loss: 1.0931224822998047 | avg loss: 1.007498982515964\n",
      "batch loss: 1.0870970487594604 | avg loss: 1.007933944626584\n",
      "batch loss: 0.9636010527610779 | avg loss: 1.0076930049968802\n",
      "batch loss: 1.0061413049697876 | avg loss: 1.0076846174291663\n",
      "batch loss: 1.004168152809143 | avg loss: 1.0076657117054026\n",
      "batch loss: 1.0331575870513916 | avg loss: 1.0078020318944187\n",
      "batch loss: 0.9851502180099487 | avg loss: 1.0076815435226927\n",
      "batch loss: 1.0502469539642334 | avg loss: 1.0079067573345528\n",
      "batch loss: 1.0091605186462402 | avg loss: 1.0079133560782985\n",
      "batch loss: 1.1674436330795288 | avg loss: 1.008748593130661\n",
      "batch loss: 0.9548654556274414 | avg loss: 1.0084679517894983\n",
      "batch loss: 0.9131338000297546 | avg loss: 1.0079739924539557\n",
      "batch loss: 0.8987550139427185 | avg loss: 1.00741100802864\n",
      "batch loss: 0.8629393577575684 | avg loss: 1.0066701277708396\n",
      "batch loss: 1.0914146900177002 | avg loss: 1.0071024979863847\n",
      "batch loss: 0.8843408823013306 | avg loss: 1.006479342576816\n",
      "batch loss: 0.8433443903923035 | avg loss: 1.0056554286768942\n",
      "batch loss: 0.8491816520690918 | avg loss: 1.0048691282919304\n",
      "batch loss: 0.85261470079422 | avg loss: 1.0041078561544419\n",
      "batch loss: 0.8582364320755005 | avg loss: 1.0033821276764372\n",
      "batch loss: 1.3201003074645996 | avg loss: 1.0049500394575666\n",
      "batch loss: 0.9597816467285156 | avg loss: 1.0047275350598865\n",
      "batch loss: 1.0322219133377075 | avg loss: 1.0048623114239936\n",
      "batch loss: 1.1765409708023071 | avg loss: 1.0056997682990096\n",
      "batch loss: 0.9036410450935364 | avg loss: 1.0052043376038375\n",
      "batch loss: 0.8313599824905396 | avg loss: 1.0043645098013578\n",
      "batch loss: 0.7369248867034912 | avg loss: 1.0030787423826182\n",
      "batch loss: 1.2954926490783691 | avg loss: 1.0044778519840332\n",
      "batch loss: 0.9872990250587463 | avg loss: 1.0043960480462937\n",
      "batch loss: 1.2279698848724365 | avg loss: 1.0054556396900196\n",
      "batch loss: 1.2241290807724 | avg loss: 1.0064871181856911\n",
      "batch loss: 1.1062283515930176 | avg loss: 1.006955386887134\n",
      "batch loss: 1.2863824367523193 | avg loss: 1.0082611207650087\n",
      "batch loss: 0.8704825043678284 | avg loss: 1.0076202899910682\n",
      "batch loss: 0.8737495541572571 | avg loss: 1.0070005180659118\n",
      "batch loss: 0.6745988130569458 | avg loss: 1.0054687129737045\n",
      "batch loss: 1.0773744583129883 | avg loss: 1.0057985558422333\n",
      "batch loss: 0.921882152557373 | avg loss: 1.0054153759185582\n",
      "batch loss: 0.9868030548095703 | avg loss: 1.005330774458972\n",
      "batch loss: 1.1195517778396606 | avg loss: 1.0058476115783415\n",
      "batch loss: 1.0996942520141602 | avg loss: 1.0062703441929173\n",
      "batch loss: 1.1826578378677368 | avg loss: 1.0070613195008762\n",
      "batch loss: 0.9039925932884216 | avg loss: 1.0066011912588562\n",
      "batch loss: 0.7514716386795044 | avg loss: 1.0054672821362813\n",
      "batch loss: 0.9618386030197144 | avg loss: 1.005274234883553\n",
      "batch loss: 0.9347453117370605 | avg loss: 1.0049635347815862\n",
      "batch loss: 1.019365906715393 | avg loss: 1.0050267030795415\n",
      "batch loss: 1.0308977365493774 | avg loss: 1.0051396770248247\n",
      "batch loss: 0.9436824917793274 | avg loss: 1.0048724718715834\n",
      "batch loss: 1.0155996084213257 | avg loss: 1.004918909692145\n",
      "batch loss: 0.9533557295799255 | avg loss: 1.0046966546054543\n",
      "batch loss: 0.7442179322242737 | avg loss: 1.0035787201746338\n",
      "batch loss: 1.0436506271362305 | avg loss: 1.0037499676402817\n",
      "batch loss: 0.9615700244903564 | avg loss: 1.0035704785204949\n",
      "batch loss: 0.9858527183532715 | avg loss: 1.003495403265549\n",
      "batch loss: 0.9163877964019775 | avg loss: 1.003127860620555\n",
      "batch loss: 0.880308985710144 | avg loss: 1.0026118149276542\n",
      "batch loss: 0.8650671243667603 | avg loss: 1.0020363141303281\n",
      "batch loss: 0.9785801768302917 | avg loss: 1.0019385802249114\n",
      "batch loss: 1.0016841888427734 | avg loss: 1.0019375246590103\n",
      "batch loss: 1.1007332801818848 | avg loss: 1.0023457715826587\n",
      "batch loss: 0.8731397390365601 | avg loss: 1.0018140595145677\n",
      "batch loss: 0.8993409872055054 | avg loss: 1.0013940879067436\n",
      "batch loss: 1.0411113500595093 | avg loss: 1.0015561991808366\n",
      "batch loss: 0.9220784306526184 | avg loss: 1.0012331188209658\n",
      "batch loss: 1.0610624551773071 | avg loss: 1.0014753428547971\n",
      "batch loss: 0.769359827041626 | avg loss: 1.00053939319426\n",
      "batch loss: 0.7099132537841797 | avg loss: 0.9993722199436172\n",
      "batch loss: 0.529802143573761 | avg loss: 0.9974939396381378\n",
      "batch loss: 0.8993695974349976 | avg loss: 0.9971030060038624\n",
      "batch loss: 0.7628486752510071 | avg loss: 0.9961734253262716\n",
      "batch loss: 0.9365920424461365 | avg loss: 0.995937925789196\n",
      "batch loss: 1.21098792552948 | avg loss: 0.9967845793314806\n",
      "batch loss: 0.9763849377632141 | avg loss: 0.9967045807370952\n",
      "batch loss: 0.9584575891494751 | avg loss: 0.9965551784262061\n",
      "batch loss: 1.209742546081543 | avg loss: 0.9973847012575499\n",
      "batch loss: 0.5776920318603516 | avg loss: 0.9957579854846925\n",
      "batch loss: 0.8048531413078308 | avg loss: 0.9950209011442412\n",
      "batch loss: 1.072705864906311 | avg loss: 0.995319689466403\n",
      "batch loss: 1.2855671644210815 | avg loss: 0.9964317487574171\n",
      "batch loss: 1.0401127338409424 | avg loss: 0.9965984700745298\n",
      "batch loss: 0.9834224581718445 | avg loss: 0.9965483711699569\n",
      "batch loss: 1.1252706050872803 | avg loss: 0.9970359553893408\n",
      "batch loss: 0.7635812163352966 | avg loss: 0.9961549941098915\n",
      "batch loss: 0.8321090936660767 | avg loss: 0.9955382801984486\n",
      "batch loss: 0.9902485013008118 | avg loss: 0.9955184682924649\n",
      "batch loss: 0.7262921929359436 | avg loss: 0.9945138926381496\n",
      "batch loss: 0.7717501521110535 | avg loss: 0.9936857746436251\n",
      "batch loss: 0.6115677952766418 | avg loss: 0.9922705228681917\n",
      "batch loss: 1.0617390871047974 | avg loss: 0.9925268644336405\n",
      "batch loss: 0.7808388471603394 | avg loss: 0.9917485996642533\n",
      "batch loss: 0.7897551655769348 | avg loss: 0.9910086969752888\n",
      "batch loss: 0.8283221125602722 | avg loss: 0.9904149503168398\n",
      "batch loss: 0.884105920791626 | avg loss: 0.9900283720276573\n",
      "batch loss: 0.8848419189453125 | avg loss: 0.9896472616904024\n",
      "batch loss: 0.8836724162101746 | avg loss: 0.9892646810207987\n",
      "batch loss: 1.133223056793213 | avg loss: 0.9897825169048721\n",
      "batch loss: 1.6643152236938477 | avg loss: 0.9922001968575208\n",
      "batch loss: 0.6021580100059509 | avg loss: 0.9908071890473366\n",
      "batch loss: 0.9771727919578552 | avg loss: 0.990758668061253\n",
      "batch loss: 1.0527575016021729 | avg loss: 0.9909785220809016\n",
      "batch loss: 0.9084507822990417 | avg loss: 0.9906869046258421\n",
      "batch loss: 1.2445647716522217 | avg loss: 0.9915808407773434\n",
      "batch loss: 0.9648155570030212 | avg loss: 0.9914869275009423\n",
      "batch loss: 1.0063585042953491 | avg loss: 0.9915389260212024\n",
      "batch loss: 1.0448975563049316 | avg loss: 0.9917248445936196\n",
      "batch loss: 1.051356554031372 | avg loss: 0.9919318991402785\n",
      "batch loss: 1.0192437171936035 | avg loss: 0.9920264037010167\n",
      "batch loss: 1.0468791723251343 | avg loss: 0.9922155511790308\n",
      "batch loss: 0.9501552581787109 | avg loss: 0.9920710140896827\n",
      "batch loss: 1.0074634552001953 | avg loss: 0.9921237279291022\n",
      "batch loss: 0.8045017719268799 | avg loss: 0.9914833799563983\n",
      "batch loss: 0.9133720993995667 | avg loss: 0.9912176953286541\n",
      "batch loss: 0.9036707878112793 | avg loss: 0.9909209261506291\n",
      "batch loss: 1.0437583923339844 | avg loss: 0.9910994311039513\n",
      "batch loss: 0.8616178631782532 | avg loss: 0.9906634662287805\n",
      "batch loss: 0.9456546306610107 | avg loss: 0.9905124298678148\n",
      "batch loss: 1.0510997772216797 | avg loss: 0.9907150631365569\n",
      "batch loss: 0.8277628421783447 | avg loss: 0.9901718890666962\n",
      "batch loss: 1.0613524913787842 | avg loss: 0.9904083694730487\n",
      "batch loss: 0.9138208031654358 | avg loss: 0.9901547682601095\n",
      "batch loss: 0.8594465255737305 | avg loss: 0.9897233879212106\n",
      "batch loss: 0.9139004945755005 | avg loss: 0.9894739705088892\n",
      "batch loss: 0.9954395294189453 | avg loss: 0.9894935297184303\n",
      "batch loss: 0.9461179971694946 | avg loss: 0.9893517796120612\n",
      "batch loss: 1.0692611932754517 | avg loss: 0.9896120708617792\n",
      "batch loss: 0.9883774518966675 | avg loss: 0.9896080623586456\n",
      "batch loss: 0.7796401977539062 | avg loss: 0.9889285547062031\n",
      "batch loss: 0.7198000550270081 | avg loss: 0.9880603982556251\n",
      "batch loss: 0.9312264919281006 | avg loss: 0.9878776525761154\n",
      "batch loss: 0.9972649812698364 | avg loss: 0.9879077401680824\n",
      "batch loss: 1.070745825767517 | avg loss: 0.9881723985885279\n",
      "batch loss: 0.8433592915534973 | avg loss: 0.9877112103495628\n",
      "batch loss: 0.7691382169723511 | avg loss: 0.987017327830905\n",
      "batch loss: 0.8849390149116516 | avg loss: 0.9866942951950846\n",
      "batch loss: 1.0581763982772827 | avg loss: 0.9869197907884039\n",
      "batch loss: 1.03646719455719 | avg loss: 0.9870756002342176\n",
      "batch loss: 0.9501393437385559 | avg loss: 0.9869598125963002\n",
      "batch loss: 0.7802112102508545 | avg loss: 0.9863137232139707\n",
      "batch loss: 1.1805208921432495 | avg loss: 0.9869187299707597\n",
      "batch loss: 0.9323917031288147 | avg loss: 0.9867493913780828\n",
      "batch loss: 0.8751288056373596 | avg loss: 0.9864038168092261\n",
      "batch loss: 0.9897536039352417 | avg loss: 0.9864141556583805\n",
      "batch loss: 0.8773983120918274 | avg loss: 0.9860787222935603\n",
      "batch loss: 0.6752635836601257 | avg loss: 0.9851253016229056\n",
      "batch loss: 0.9772617220878601 | avg loss: 0.9851012539790676\n",
      "batch loss: 1.0735458135604858 | avg loss: 0.9853709020265718\n",
      "batch loss: 1.5622367858886719 | avg loss: 0.9871242937708335\n",
      "batch loss: 1.0872691869735718 | avg loss: 0.9874277631441752\n",
      "batch loss: 0.7640873789787292 | avg loss: 0.9867530187811376\n",
      "batch loss: 1.3429604768753052 | avg loss: 0.9878259328115417\n",
      "batch loss: 0.8690929412841797 | avg loss: 0.9874693772814296\n",
      "batch loss: 0.8024829030036926 | avg loss: 0.9869155255620351\n",
      "batch loss: 0.6591745018959045 | avg loss: 0.9859371941481063\n",
      "batch loss: 0.7488974928855896 | avg loss: 0.9852317188467298\n",
      "batch loss: 1.0816712379455566 | avg loss: 0.9855178895265483\n",
      "batch loss: 0.9444189071655273 | avg loss: 0.9853962949041785\n",
      "batch loss: 0.7854463458061218 | avg loss: 0.9848064720454821\n",
      "batch loss: 0.9212319850921631 | avg loss: 0.9846194882603253\n",
      "batch loss: 0.9611691832542419 | avg loss: 0.9845507190374335\n",
      "batch loss: 0.9593065977096558 | avg loss: 0.9844769058171768\n",
      "batch loss: 1.1419551372528076 | avg loss: 0.9849360260254441\n",
      "batch loss: 0.742321252822876 | avg loss: 0.9842307505219482\n",
      "batch loss: 0.9484154582023621 | avg loss: 0.9841269380804422\n",
      "batch loss: 0.7436391115188599 | avg loss: 0.9834318865585878\n",
      "batch loss: 1.1421934366226196 | avg loss: 0.9838894126394641\n",
      "batch loss: 0.8993241190910339 | avg loss: 0.9836464089223709\n",
      "batch loss: 1.0424139499664307 | avg loss: 0.9838147972921246\n",
      "batch loss: 1.146921157836914 | avg loss: 0.9842808154651097\n",
      "batch loss: 1.439178228378296 | avg loss: 0.9855768194905034\n",
      "batch loss: 1.1185075044631958 | avg loss: 0.9859544634819031\n",
      "batch loss: 0.9356911182403564 | avg loss: 0.9858120744018987\n",
      "batch loss: 0.8808994293212891 | avg loss: 0.9855157109977162\n",
      "batch loss: 0.9358805418014526 | avg loss: 0.9853758936196986\n",
      "batch loss: 1.081995964050293 | avg loss: 0.9856472983119193\n",
      "batch loss: 0.9283649921417236 | avg loss: 0.9854868436727872\n",
      "batch loss: 1.1006662845611572 | avg loss: 0.9858085739545982\n",
      "batch loss: 1.2440499067306519 | avg loss: 0.9865279091433894\n",
      "batch loss: 1.0209732055664062 | avg loss: 0.9866235905223423\n",
      "batch loss: 1.062995195388794 | avg loss: 0.9868351462144931\n",
      "batch loss: 1.1385668516159058 | avg loss: 0.9872542945719556\n",
      "\n",
      "  Average training loss: 0.99\n",
      "  Training epoch took: 0:27:36\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.48\n",
      "  Validation took: 0:01:48\n",
      "\n",
      "======= Epoch 3 / 5 =======\n",
      "batch loss: 0.9047167301177979 | avg loss: 0.9047167301177979\n",
      "batch loss: 0.9278622269630432 | avg loss: 0.9162894785404205\n",
      "batch loss: 0.8474758863449097 | avg loss: 0.8933516144752502\n",
      "batch loss: 1.0391626358032227 | avg loss: 0.9298043698072433\n",
      "batch loss: 0.9344816207885742 | avg loss: 0.9307398200035095\n",
      "batch loss: 0.8582838773727417 | avg loss: 0.9186638295650482\n",
      "batch loss: 0.9845878481864929 | avg loss: 0.9280815465109689\n",
      "batch loss: 0.8159236907958984 | avg loss: 0.9140618145465851\n",
      "batch loss: 0.9226263165473938 | avg loss: 0.9150134258800082\n",
      "batch loss: 0.8619495034217834 | avg loss: 0.9097070336341858\n",
      "batch loss: 0.9612457156181335 | avg loss: 0.9143923683599993\n",
      "batch loss: 0.9556926488876343 | avg loss: 0.9178340584039688\n",
      "batch loss: 1.0060149431228638 | avg loss: 0.9246172033823453\n",
      "batch loss: 0.802405834197998 | avg loss: 0.9158878198691777\n",
      "batch loss: 1.0257024765014648 | avg loss: 0.9232087969779968\n",
      "batch loss: 0.9330067038536072 | avg loss: 0.9238211661577225\n",
      "batch loss: 0.8862016201019287 | avg loss: 0.9216082516838523\n",
      "batch loss: 0.7366918325424194 | avg loss: 0.911335117287106\n",
      "batch loss: 0.819483757019043 | avg loss: 0.9065008351677343\n",
      "batch loss: 0.9927507042884827 | avg loss: 0.9108133286237716\n",
      "batch loss: 0.7707953453063965 | avg loss: 0.9041458056086585\n",
      "batch loss: 0.9461953043937683 | avg loss: 0.9060571464625272\n",
      "batch loss: 0.730428159236908 | avg loss: 0.8984211035396742\n",
      "batch loss: 0.7360456585884094 | avg loss: 0.8916554600000381\n",
      "batch loss: 0.6682259440422058 | avg loss: 0.8827182793617249\n",
      "batch loss: 0.8324706554412842 | avg loss: 0.8807856784417079\n",
      "batch loss: 0.6032254099845886 | avg loss: 0.8705056684988516\n",
      "batch loss: 0.8462086915969849 | avg loss: 0.869637919323785\n",
      "batch loss: 0.9693025350570679 | avg loss: 0.8730746302111395\n",
      "batch loss: 0.5666079521179199 | avg loss: 0.8628590742746989\n",
      "batch loss: 0.615373432636261 | avg loss: 0.8548756664799105\n",
      "batch loss: 0.4814102351665497 | avg loss: 0.843204871751368\n",
      "batch loss: 0.7950191497802734 | avg loss: 0.8417446983583046\n",
      "batch loss: 1.0335148572921753 | avg loss: 0.8473849971504772\n",
      "batch loss: 0.9853681325912476 | avg loss: 0.851327372448785\n",
      "batch loss: 1.1386349201202393 | avg loss: 0.859308137661881\n",
      "batch loss: 0.9937553405761719 | avg loss: 0.8629418458487537\n",
      "batch loss: 1.1439592838287354 | avg loss: 0.870337041585069\n",
      "batch loss: 0.5614125728607178 | avg loss: 0.8624159013613676\n",
      "batch loss: 0.7545406818389893 | avg loss: 0.8597190208733082\n",
      "batch loss: 0.7348200678825378 | avg loss: 0.856672704946704\n",
      "batch loss: 1.2367732524871826 | avg loss: 0.8657227179833821\n",
      "batch loss: 1.1336325407028198 | avg loss: 0.8719531789768574\n",
      "batch loss: 1.0083885192871094 | avg loss: 0.8750539821657267\n",
      "batch loss: 0.8150861859321594 | avg loss: 0.8737213644716475\n",
      "batch loss: 1.0335460901260376 | avg loss: 0.8771958150293516\n",
      "batch loss: 1.052008032798767 | avg loss: 0.8809152239180625\n",
      "batch loss: 0.6693017482757568 | avg loss: 0.8765066098421812\n",
      "batch loss: 0.9999537467956543 | avg loss: 0.8790259391677623\n",
      "batch loss: 0.7713190913200378 | avg loss: 0.8768718022108078\n",
      "batch loss: 1.1865558624267578 | avg loss: 0.8829440386856303\n",
      "batch loss: 1.0143582820892334 | avg loss: 0.8854712356741612\n",
      "batch loss: 0.7963888049125671 | avg loss: 0.8837904350937538\n",
      "batch loss: 0.6829603314399719 | avg loss: 0.8800713591001652\n",
      "batch loss: 1.1014398336410522 | avg loss: 0.8840962404554541\n",
      "batch loss: 0.8023732304573059 | avg loss: 0.8826369009912014\n",
      "batch loss: 1.3275806903839111 | avg loss: 0.8904429323840559\n",
      "batch loss: 0.5968768000602722 | avg loss: 0.8853814473439907\n",
      "batch loss: 0.6563047170639038 | avg loss: 0.8814987908985655\n",
      "batch loss: 0.7812967896461487 | avg loss: 0.8798287575443585\n",
      "batch loss: 0.7782784700393677 | avg loss: 0.8781639987328014\n",
      "batch loss: 0.8319659233093262 | avg loss: 0.8774188684840356\n",
      "batch loss: 0.5908634066581726 | avg loss: 0.8728703690899743\n",
      "batch loss: 0.9141543507575989 | avg loss: 0.8735154313035309\n",
      "batch loss: 1.2682240009307861 | avg loss: 0.8795878708362579\n",
      "batch loss: 1.0370275974273682 | avg loss: 0.8819733212391535\n",
      "batch loss: 1.4395313262939453 | avg loss: 0.8902950825086281\n",
      "batch loss: 0.8413803577423096 | avg loss: 0.889575748320888\n",
      "batch loss: 0.5913730263710022 | avg loss: 0.8852539697419042\n",
      "batch loss: 0.8892476558685303 | avg loss: 0.885311022400856\n",
      "batch loss: 0.943496584892273 | avg loss: 0.8861305373655238\n",
      "batch loss: 1.206458330154419 | avg loss: 0.8905795344875919\n",
      "batch loss: 0.7028471231460571 | avg loss: 0.8880078576198996\n",
      "batch loss: 0.8688645362854004 | avg loss: 0.8877491640883524\n",
      "batch loss: 1.0047467947006226 | avg loss: 0.8893091324965159\n",
      "batch loss: 0.9545584321022034 | avg loss: 0.8901676759123802\n",
      "batch loss: 0.5438973307609558 | avg loss: 0.8856706584428812\n",
      "batch loss: 0.8544766306877136 | avg loss: 0.8852707350101227\n",
      "batch loss: 0.9290611743927002 | avg loss: 0.8858250443693958\n",
      "batch loss: 0.7255339622497559 | avg loss: 0.8838214058429003\n",
      "batch loss: 0.726186215877533 | avg loss: 0.8818752923865377\n",
      "batch loss: 0.8794896602630615 | avg loss: 0.8818461993118611\n",
      "batch loss: 0.5400620698928833 | avg loss: 0.8777283182345241\n",
      "batch loss: 0.6521320343017578 | avg loss: 0.8750426481877055\n",
      "batch loss: 0.7398433089256287 | avg loss: 0.8734520677257986\n",
      "batch loss: 0.8893179893493652 | avg loss: 0.8736365551865378\n",
      "batch loss: 0.7864397764205933 | avg loss: 0.8726342933616419\n",
      "batch loss: 0.9094398617744446 | avg loss: 0.873052538457242\n",
      "batch loss: 0.576759397983551 | avg loss: 0.8697234020474252\n",
      "batch loss: 1.0819265842437744 | avg loss: 0.8720812151829401\n",
      "batch loss: 1.1461598873138428 | avg loss: 0.8750930687228402\n",
      "batch loss: 0.689221203327179 | avg loss: 0.8730727223598439\n",
      "batch loss: 0.9016653895378113 | avg loss: 0.8733801703940156\n",
      "batch loss: 0.7672486305236816 | avg loss: 0.8722511114592247\n",
      "batch loss: 1.0650098323822021 | avg loss: 0.8742801506268351\n",
      "batch loss: 0.5046711564064026 | avg loss: 0.8704300569370389\n",
      "batch loss: 0.8784606456756592 | avg loss: 0.8705128465116638\n",
      "batch loss: 1.0830187797546387 | avg loss: 0.8726812744018982\n",
      "batch loss: 0.6064770221710205 | avg loss: 0.8699923425611823\n",
      "batch loss: 1.240694522857666 | avg loss: 0.8736993643641472\n",
      "batch loss: 0.7034834027290344 | avg loss: 0.8720140578133044\n",
      "batch loss: 0.9402422904968262 | avg loss: 0.8726829620552998\n",
      "batch loss: 0.7550202012062073 | avg loss: 0.8715406051538523\n",
      "batch loss: 0.6603937745094299 | avg loss: 0.8695103471668867\n",
      "batch loss: 0.8076406717300415 | avg loss: 0.8689211121627263\n",
      "batch loss: 0.9622402787208557 | avg loss: 0.8698014816585576\n",
      "batch loss: 0.9381588697433472 | avg loss: 0.870440335752808\n",
      "batch loss: 1.0369302034378052 | avg loss: 0.8719819086017432\n",
      "batch loss: 1.0047008991241455 | avg loss: 0.8731995140193799\n",
      "batch loss: 0.9844402074813843 | avg loss: 0.8742107930508527\n",
      "batch loss: 0.7071629166603088 | avg loss: 0.8727058572275145\n",
      "batch loss: 0.6928682923316956 | avg loss: 0.8711001646838018\n",
      "batch loss: 0.7629947066307068 | avg loss: 0.8701434792143053\n",
      "batch loss: 0.9933686852455139 | avg loss: 0.8712244020742282\n",
      "batch loss: 0.6442277431488037 | avg loss: 0.8692505180835723\n",
      "batch loss: 1.0027064085006714 | avg loss: 0.8704009998975129\n",
      "batch loss: 0.5827356576919556 | avg loss: 0.8679423217590039\n",
      "batch loss: 0.8346902132034302 | avg loss: 0.8676605242288719\n",
      "batch loss: 1.0249607563018799 | avg loss: 0.868982374918561\n",
      "batch loss: 0.676982581615448 | avg loss: 0.8673823766410351\n",
      "batch loss: 1.1329597234725952 | avg loss: 0.869577230747081\n",
      "batch loss: 0.8965301513671875 | avg loss: 0.8697981563259344\n",
      "batch loss: 0.9224457740783691 | avg loss: 0.8702261857385558\n",
      "batch loss: 1.0103192329406738 | avg loss: 0.8713559683772826\n",
      "batch loss: 0.7772350311279297 | avg loss: 0.8706030008792878\n",
      "batch loss: 1.0449702739715576 | avg loss: 0.8719868681260518\n",
      "batch loss: 0.9027398824691772 | avg loss: 0.872229017845289\n",
      "batch loss: 0.7837928533554077 | avg loss: 0.8715381103102118\n",
      "batch loss: 0.927420973777771 | avg loss: 0.8719713108022084\n",
      "batch loss: 0.7372533082962036 | avg loss: 0.8709350184752391\n",
      "batch loss: 0.8495403528213501 | avg loss: 0.8707717004168125\n",
      "batch loss: 0.6803290843963623 | avg loss: 0.8693289533257484\n",
      "batch loss: 0.8072931170463562 | avg loss: 0.8688625184665049\n",
      "batch loss: 0.6763461828231812 | avg loss: 0.8674258293945398\n",
      "batch loss: 0.8061355948448181 | avg loss: 0.8669718276571344\n",
      "batch loss: 0.589046835899353 | avg loss: 0.8649282615412685\n",
      "batch loss: 0.7314284443855286 | avg loss: 0.8639538103211535\n",
      "batch loss: 1.0904266834259033 | avg loss: 0.8655949180972748\n",
      "batch loss: 0.781255304813385 | avg loss: 0.8649881582894772\n",
      "batch loss: 0.6992672085762024 | avg loss: 0.8638044372200966\n",
      "batch loss: 0.6504819393157959 | avg loss: 0.8622915117030449\n",
      "batch loss: 0.5770837068557739 | avg loss: 0.8602830060351063\n",
      "batch loss: 0.690599262714386 | avg loss: 0.8590964064314649\n",
      "batch loss: 0.7198018431663513 | avg loss: 0.8581290830754571\n",
      "batch loss: 1.0444042682647705 | avg loss: 0.8594137395250386\n",
      "batch loss: 0.8314540982246399 | avg loss: 0.8592222351325701\n",
      "batch loss: 0.8800039291381836 | avg loss: 0.8593636072006355\n",
      "batch loss: 0.8949785828590393 | avg loss: 0.8596042489280572\n",
      "batch loss: 0.7446834444999695 | avg loss: 0.8588329683614258\n",
      "batch loss: 0.692899763584137 | avg loss: 0.8577267469962438\n",
      "batch loss: 0.7956427931785583 | avg loss: 0.8573155949842062\n",
      "batch loss: 0.8276522755622864 | avg loss: 0.8571204415669567\n",
      "batch loss: 0.8801254630088806 | avg loss: 0.8572708011842242\n",
      "batch loss: 0.6804649233818054 | avg loss: 0.856122711068624\n",
      "batch loss: 0.8766940832138062 | avg loss: 0.8562554295985929\n",
      "batch loss: 0.8287445902824402 | avg loss: 0.856079078064515\n",
      "batch loss: 0.8169263601303101 | avg loss: 0.8558296976955073\n",
      "batch loss: 0.665840208530426 | avg loss: 0.8546272325742094\n",
      "batch loss: 0.8019279837608337 | avg loss: 0.8542957907577731\n",
      "batch loss: 0.7566644549369812 | avg loss: 0.8536855949088931\n",
      "batch loss: 0.6281803846359253 | avg loss: 0.8522849414289367\n",
      "batch loss: 0.931297242641449 | avg loss: 0.852772671683335\n",
      "batch loss: 0.7022140026092529 | avg loss: 0.8518489988669296\n",
      "batch loss: 0.9140665531158447 | avg loss: 0.8522283741977157\n",
      "batch loss: 1.4570391178131104 | avg loss: 0.8558938938559908\n",
      "batch loss: 1.1872516870498657 | avg loss: 0.8578900251402912\n",
      "batch loss: 1.3588409423828125 | avg loss: 0.8608897312315639\n",
      "batch loss: 1.3306697607040405 | avg loss: 0.8636860409308047\n",
      "batch loss: 0.5364881753921509 | avg loss: 0.8617499588861973\n",
      "batch loss: 0.4771559536457062 | avg loss: 0.8594876412083121\n",
      "batch loss: 0.9941338300704956 | avg loss: 0.8602750458215412\n",
      "batch loss: 0.6186780333518982 | avg loss: 0.8588704120281131\n",
      "batch loss: 0.8615702986717224 | avg loss: 0.85888601830929\n",
      "batch loss: 1.013765811920166 | avg loss: 0.8597761320656744\n",
      "batch loss: 0.4685803949832916 | avg loss: 0.857540727853775\n",
      "batch loss: 0.8794781565666199 | avg loss: 0.857665372335098\n",
      "batch loss: 0.9520888924598694 | avg loss: 0.8581988385504922\n",
      "batch loss: 1.2304896116256714 | avg loss: 0.8602903597475438\n",
      "batch loss: 1.0432194471359253 | avg loss: 0.8613123099564174\n",
      "batch loss: 1.2294209003448486 | avg loss: 0.8633573576807976\n",
      "batch loss: 1.041369080543518 | avg loss: 0.8643408478623595\n",
      "batch loss: 0.9072586894035339 | avg loss: 0.8645766601785199\n",
      "batch loss: 0.9258285760879517 | avg loss: 0.8649113701015222\n",
      "batch loss: 0.9642162322998047 | avg loss: 0.8654510704395564\n",
      "batch loss: 0.6869672536849976 | avg loss: 0.8644862930516939\n",
      "batch loss: 0.8046890497207642 | avg loss: 0.8641648024961512\n",
      "batch loss: 0.8886539936065674 | avg loss: 0.8642957607373835\n",
      "batch loss: 0.7770476341247559 | avg loss: 0.863831674957529\n",
      "batch loss: 0.887387752532959 | avg loss: 0.863956310288616\n",
      "batch loss: 0.8073849081993103 | avg loss: 0.8636585660670933\n",
      "batch loss: 0.9080961346626282 | avg loss: 0.863891223494295\n",
      "batch loss: 0.6460371613502502 | avg loss: 0.8627565669206282\n",
      "batch loss: 0.6151372790336609 | avg loss: 0.8614735654289858\n",
      "batch loss: 0.676095724105835 | avg loss: 0.8605180095458768\n",
      "batch loss: 0.823390007019043 | avg loss: 0.8603276095329186\n",
      "batch loss: 1.171433925628662 | avg loss: 0.8619148866558561\n",
      "batch loss: 0.5308429002761841 | avg loss: 0.8602343181970761\n",
      "batch loss: 0.5599207282066345 | avg loss: 0.8587175828940941\n",
      "batch loss: 0.5228147506713867 | avg loss: 0.8570296289633267\n",
      "batch loss: 0.7052026987075806 | avg loss: 0.8562704943120479\n",
      "batch loss: 0.6083577275276184 | avg loss: 0.8550370974623742\n",
      "batch loss: 1.3436508178710938 | avg loss: 0.8574559772663778\n",
      "batch loss: 0.5882880687713623 | avg loss: 0.8561300269782249\n",
      "batch loss: 0.8260166049003601 | avg loss: 0.8559824121641177\n",
      "batch loss: 0.8803593516349792 | avg loss: 0.8561013240639757\n",
      "batch loss: 0.8285548090934753 | avg loss: 0.8559676031175169\n",
      "batch loss: 0.6714139580726624 | avg loss: 0.8550760396148848\n",
      "batch loss: 0.6306779980659485 | avg loss: 0.8539972028766687\n",
      "batch loss: 1.0110746622085571 | avg loss: 0.8547487696677304\n",
      "batch loss: 0.8859397172927856 | avg loss: 0.8548972979897544\n",
      "batch loss: 1.0925586223602295 | avg loss: 0.8560236549772923\n",
      "batch loss: 1.0204534530639648 | avg loss: 0.8567992672324181\n",
      "batch loss: 1.0454237461090088 | avg loss: 0.857684828166111\n",
      "batch loss: 1.0602970123291016 | avg loss: 0.8586316140734147\n",
      "batch loss: 0.6364141702651978 | avg loss: 0.8575980445673299\n",
      "batch loss: 0.6485628485679626 | avg loss: 0.8566302890302958\n",
      "batch loss: 0.4841758906841278 | avg loss: 0.8549139093144149\n",
      "batch loss: 0.9880914688110352 | avg loss: 0.8555248155506379\n",
      "batch loss: 0.8885293006896973 | avg loss: 0.8556755209622318\n",
      "batch loss: 0.8403878211975098 | avg loss: 0.8556060314178466\n",
      "batch loss: 0.8823574781417847 | avg loss: 0.8557270786880907\n",
      "batch loss: 0.9239403605461121 | avg loss: 0.8560343457234872\n",
      "batch loss: 1.1295469999313354 | avg loss: 0.857260859867917\n",
      "batch loss: 0.6336811184883118 | avg loss: 0.8562627360224724\n",
      "batch loss: 0.4827544689178467 | avg loss: 0.8546026992797852\n",
      "batch loss: 0.9355389475822449 | avg loss: 0.8549608242722739\n",
      "batch loss: 0.5911876559257507 | avg loss: 0.8537988279359456\n",
      "batch loss: 0.7436367273330688 | avg loss: 0.8533156608280382\n",
      "batch loss: 0.8350092172622681 | avg loss: 0.853235720026441\n",
      "batch loss: 1.030869483947754 | avg loss: 0.8540080407391424\n",
      "batch loss: 0.7724184393882751 | avg loss: 0.8536548390016927\n",
      "batch loss: 0.7542182207107544 | avg loss: 0.8532262328883697\n",
      "batch loss: 0.5686925053596497 | avg loss: 0.8520050580921091\n",
      "batch loss: 0.8672349452972412 | avg loss: 0.8520701430801653\n",
      "batch loss: 0.8838127851486206 | avg loss: 0.8522052181527969\n",
      "batch loss: 0.6693166494369507 | avg loss: 0.8514302665904417\n",
      "batch loss: 0.7128720879554749 | avg loss: 0.8508456329253152\n",
      "batch loss: 0.6363188624382019 | avg loss: 0.8499442599400753\n",
      "batch loss: 0.6226369142532349 | avg loss: 0.8489931831798793\n",
      "batch loss: 0.8453636765480042 | avg loss: 0.8489780602355798\n",
      "batch loss: 0.876937747001648 | avg loss: 0.8490940755333644\n",
      "batch loss: 0.7889047861099243 | avg loss: 0.8488453594613666\n",
      "batch loss: 0.6503276228904724 | avg loss: 0.8480284140433794\n",
      "batch loss: 0.6822799444198608 | avg loss: 0.8473491170367257\n",
      "batch loss: 0.860625147819519 | avg loss: 0.8474033049174717\n",
      "batch loss: 0.7310648560523987 | avg loss: 0.84693038439363\n",
      "batch loss: 0.7841167449951172 | avg loss: 0.8466760781612473\n",
      "batch loss: 0.4396526515483856 | avg loss: 0.8450348546668407\n",
      "batch loss: 0.45695236325263977 | avg loss: 0.8434762904442936\n",
      "batch loss: 0.33979514241218567 | avg loss: 0.8414615658521652\n",
      "batch loss: 0.7924891710281372 | avg loss: 0.84126645670944\n",
      "batch loss: 0.44696271419525146 | avg loss: 0.8397017593185107\n",
      "batch loss: 0.8149100542068481 | avg loss: 0.8396037683892156\n",
      "batch loss: 1.0612585544586182 | avg loss: 0.8404764250272841\n",
      "batch loss: 0.8546515703201294 | avg loss: 0.8405320138323541\n",
      "batch loss: 0.7936596870422363 | avg loss: 0.8403489188058302\n",
      "batch loss: 1.179540991783142 | avg loss: 0.8416687323193606\n",
      "batch loss: 0.48567700386047363 | avg loss: 0.8402889194183571\n",
      "batch loss: 0.6444275975227356 | avg loss: 0.8395326980982968\n",
      "batch loss: 0.8380826711654663 | avg loss: 0.8395271210716321\n",
      "batch loss: 1.3240615129470825 | avg loss: 0.8413835746803503\n",
      "batch loss: 0.7761868238449097 | avg loss: 0.8411347321199096\n",
      "batch loss: 0.6025900840759277 | avg loss: 0.84022771824902\n",
      "batch loss: 0.8567317724227905 | avg loss: 0.8402902336057388\n",
      "batch loss: 0.6167883276939392 | avg loss: 0.8394468301872038\n",
      "batch loss: 0.5659612417221069 | avg loss: 0.8384186888771846\n",
      "batch loss: 0.9160857200622559 | avg loss: 0.838709576634432\n",
      "batch loss: 0.44325587153434753 | avg loss: 0.8372340031079392\n",
      "batch loss: 0.6770102977752686 | avg loss: 0.8366383759505687\n",
      "batch loss: 0.3200742304325104 | avg loss: 0.834725175411613\n",
      "batch loss: 0.8477674126625061 | avg loss: 0.8347733017483321\n",
      "batch loss: 0.5720988512039185 | avg loss: 0.8338075868566247\n",
      "batch loss: 0.6701304912567139 | avg loss: 0.8332080370558924\n",
      "batch loss: 0.7313212156295776 | avg loss: 0.8328361873426576\n",
      "batch loss: 0.7910096645355225 | avg loss: 0.8326840908960863\n",
      "batch loss: 0.9154483079910278 | avg loss: 0.8329839612478795\n",
      "batch loss: 0.6042488217353821 | avg loss: 0.8321582026214807\n",
      "batch loss: 0.9898056983947754 | avg loss: 0.8327252799444061\n",
      "batch loss: 1.5879592895507812 | avg loss: 0.8354322118784792\n",
      "batch loss: 0.3304300904273987 | avg loss: 0.8336286328732967\n",
      "batch loss: 0.8019568920135498 | avg loss: 0.8335159220517318\n",
      "batch loss: 0.6793795228004456 | avg loss: 0.8329693390756634\n",
      "batch loss: 0.9808439612388611 | avg loss: 0.8334918642423178\n",
      "batch loss: 1.0950323343276978 | avg loss: 0.8344127813905058\n",
      "batch loss: 0.8430149555206299 | avg loss: 0.834442964457629\n",
      "batch loss: 0.8118173480033875 | avg loss: 0.8343638539105862\n",
      "batch loss: 0.7596666812896729 | avg loss: 0.8341035850164367\n",
      "batch loss: 0.8089213371276855 | avg loss: 0.8340161466557119\n",
      "batch loss: 0.8537340760231018 | avg loss: 0.8340843747850107\n",
      "batch loss: 1.2076776027679443 | avg loss: 0.8353726272952968\n",
      "batch loss: 0.6365448832511902 | avg loss: 0.8346893704429115\n",
      "batch loss: 0.900505006313324 | avg loss: 0.8349147664561664\n",
      "batch loss: 0.6975162029266357 | avg loss: 0.8344458293792738\n",
      "batch loss: 0.9034686088562012 | avg loss: 0.8346806007380388\n",
      "batch loss: 0.8213155269622803 | avg loss: 0.8346352954032057\n",
      "batch loss: 0.8930009603500366 | avg loss: 0.8348324767037018\n",
      "batch loss: 0.6726993918418884 | avg loss: 0.8342865740610694\n",
      "batch loss: 0.8497490882873535 | avg loss: 0.8343384616927012\n",
      "batch loss: 0.9315268993377686 | avg loss: 0.834663506634658\n",
      "batch loss: 0.7808164954185486 | avg loss: 0.834484016597271\n",
      "batch loss: 0.7421913146972656 | avg loss: 0.834177396325178\n",
      "batch loss: 0.5854986310005188 | avg loss: 0.8333539567048976\n",
      "batch loss: 0.7396233081817627 | avg loss: 0.8330446146305638\n",
      "batch loss: 0.9286585450172424 | avg loss: 0.8333591341384148\n",
      "batch loss: 0.8713502883911133 | avg loss: 0.833483695299899\n",
      "batch loss: 0.7066096067428589 | avg loss: 0.8330690740954643\n",
      "batch loss: 0.7805756330490112 | avg loss: 0.8328980856881468\n",
      "batch loss: 0.6276776790618896 | avg loss: 0.8322317856666329\n",
      "batch loss: 0.6883806586265564 | avg loss: 0.8317662480386715\n",
      "batch loss: 0.6282495260238647 | avg loss: 0.8311097424837851\n",
      "batch loss: 0.7346291542053223 | avg loss: 0.8307995155118286\n",
      "batch loss: 0.9448515176773071 | avg loss: 0.8311650668008205\n",
      "batch loss: 1.2284327745437622 | avg loss: 0.8324342927041526\n",
      "batch loss: 0.6656789779663086 | avg loss: 0.8319032248228219\n",
      "batch loss: 0.8617516756057739 | avg loss: 0.8319979818094344\n",
      "batch loss: 0.86758953332901 | avg loss: 0.832110613301585\n",
      "batch loss: 0.7022053003311157 | avg loss: 0.831700817361615\n",
      "batch loss: 0.7383311986923218 | avg loss: 0.831407202208567\n",
      "batch loss: 0.7861452698707581 | avg loss: 0.8312653152733387\n",
      "batch loss: 0.5188997387886047 | avg loss: 0.8302891728468239\n",
      "batch loss: 0.925154983997345 | avg loss: 0.8305847049687882\n",
      "batch loss: 0.7652713060379028 | avg loss: 0.8303818683261457\n",
      "batch loss: 0.7765944600105286 | avg loss: 0.8302153438421964\n",
      "batch loss: 0.9362252354621887 | avg loss: 0.8305425348657148\n",
      "batch loss: 1.2285538911819458 | avg loss: 0.8317671851928418\n",
      "batch loss: 0.8393963575363159 | avg loss: 0.8317905875619935\n",
      "batch loss: 0.7823076844215393 | avg loss: 0.8316392636991786\n",
      "batch loss: 0.7027115225791931 | avg loss: 0.8312461913177153\n",
      "batch loss: 1.2751200199127197 | avg loss: 0.8325953518909524\n",
      "batch loss: 0.7880938053131104 | avg loss: 0.8324604987195043\n",
      "batch loss: 0.6816586256027222 | avg loss: 0.8320049039366743\n",
      "batch loss: 1.020965814590454 | avg loss: 0.8325740633061133\n",
      "batch loss: 0.7727524638175964 | avg loss: 0.8323944188632049\n",
      "batch loss: 0.6043522357940674 | avg loss: 0.8317116578360517\n",
      "batch loss: 0.5054517388343811 | avg loss: 0.8307377476300766\n",
      "batch loss: 0.5805940628051758 | avg loss: 0.8299932723776215\n",
      "batch loss: 0.6509061455726624 | avg loss: 0.8294618565710786\n",
      "batch loss: 0.6866775155067444 | avg loss: 0.8290394176921901\n",
      "batch loss: 0.6077516078948975 | avg loss: 0.8283866512916082\n",
      "batch loss: 0.799409031867981 | avg loss: 0.8283014229991856\n",
      "batch loss: 0.8014066815376282 | avg loss: 0.828222552789621\n",
      "batch loss: 0.7650538682937622 | avg loss: 0.8280378490337852\n",
      "batch loss: 0.8143091797828674 | avg loss: 0.8279978237590011\n",
      "batch loss: 0.5273286700248718 | avg loss: 0.8271237855214019\n",
      "batch loss: 0.6741191744804382 | avg loss: 0.8266802938951963\n",
      "batch loss: 0.5884801745414734 | avg loss: 0.8259918542438849\n",
      "batch loss: 0.7900013327598572 | avg loss: 0.8258881351617984\n",
      "batch loss: 0.5095720887184143 | avg loss: 0.8249791810053518\n",
      "batch loss: 0.751613199710846 | avg loss: 0.8247689632939063\n",
      "batch loss: 0.7762155532836914 | avg loss: 0.8246302392653057\n",
      "batch loss: 1.3569751977920532 | avg loss: 0.8261468915688006\n",
      "batch loss: 1.1206307411193848 | avg loss: 0.8269834934141148\n",
      "batch loss: 0.6671885848045349 | avg loss: 0.8265308166191868\n",
      "batch loss: 0.7575830817222595 | avg loss: 0.8263360490064836\n",
      "batch loss: 0.7475530505180359 | avg loss: 0.8261141250670796\n",
      "batch loss: 0.6931331157684326 | avg loss: 0.8257405829061283\n",
      "batch loss: 0.7269040942192078 | avg loss: 0.8254637299966412\n",
      "batch loss: 1.0433375835418701 | avg loss: 0.8260723161797284\n",
      "batch loss: 1.2168943881988525 | avg loss: 0.8271609570488624\n",
      "batch loss: 0.9522687196731567 | avg loss: 0.8275084786117077\n",
      "batch loss: 1.0662596225738525 | avg loss: 0.8281698391212982\n",
      "batch loss: 0.8290238976478577 | avg loss: 0.8281721983989958\n",
      "\n",
      "  Average training loss: 0.83\n",
      "  Training epoch took: 0:26:51\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.48\n",
      "  Validation took: 0:01:46\n",
      "\n",
      "======= Epoch 4 / 5 =======\n",
      "batch loss: 0.6918219923973083 | avg loss: 0.6918219923973083\n",
      "batch loss: 0.6609712839126587 | avg loss: 0.6763966381549835\n",
      "batch loss: 0.7164040207862854 | avg loss: 0.6897324323654175\n",
      "batch loss: 0.740150511264801 | avg loss: 0.7023369520902634\n",
      "batch loss: 0.619074285030365 | avg loss: 0.6856844186782837\n",
      "batch loss: 0.6584768295288086 | avg loss: 0.6811498204867045\n",
      "batch loss: 0.8493808507919312 | avg loss: 0.7051828248160226\n",
      "batch loss: 0.5108703374862671 | avg loss: 0.6808937638998032\n",
      "batch loss: 0.6879259347915649 | avg loss: 0.68167511622111\n",
      "batch loss: 0.6266549825668335 | avg loss: 0.6761731028556823\n",
      "batch loss: 0.7573598623275757 | avg loss: 0.6835537173531272\n",
      "batch loss: 0.9852161407470703 | avg loss: 0.7086922526359558\n",
      "batch loss: 0.8259792923927307 | avg loss: 0.7177143326172462\n",
      "batch loss: 0.5766677260398865 | avg loss: 0.7076395750045776\n",
      "batch loss: 0.7607342600822449 | avg loss: 0.7111792206764221\n",
      "batch loss: 0.7592686414718628 | avg loss: 0.7141848094761372\n",
      "batch loss: 0.6207332611083984 | avg loss: 0.7086876595721525\n",
      "batch loss: 0.508075475692749 | avg loss: 0.697542538245519\n",
      "batch loss: 0.5387782454490662 | avg loss: 0.6891865228351793\n",
      "batch loss: 0.8026556372642517 | avg loss: 0.694859978556633\n",
      "batch loss: 0.7021671533584595 | avg loss: 0.6952079392614818\n",
      "batch loss: 0.7251482009887695 | avg loss: 0.6965688602490858\n",
      "batch loss: 0.6920551061630249 | avg loss: 0.696372610071431\n",
      "batch loss: 0.8248105049133301 | avg loss: 0.7017241890231768\n",
      "batch loss: 0.5598200559616089 | avg loss: 0.6960480237007141\n",
      "batch loss: 0.7824687957763672 | avg loss: 0.6993718995497777\n",
      "batch loss: 0.44068044424057007 | avg loss: 0.6897907345383255\n",
      "batch loss: 0.6544240117073059 | avg loss: 0.6885276372943606\n",
      "batch loss: 0.647191047668457 | avg loss: 0.687102237652088\n",
      "batch loss: 0.5435093641281128 | avg loss: 0.6823158085346221\n",
      "batch loss: 0.5892367959022522 | avg loss: 0.6793132597400297\n",
      "batch loss: 0.44903209805488586 | avg loss: 0.6721169734373689\n",
      "batch loss: 0.7643740177154541 | avg loss: 0.6749126414457957\n",
      "batch loss: 0.6480993032455444 | avg loss: 0.6741240138516706\n",
      "batch loss: 0.8061648011207581 | avg loss: 0.6778966077736446\n",
      "batch loss: 0.8664064407348633 | avg loss: 0.6831329920225673\n",
      "batch loss: 0.9396610260009766 | avg loss: 0.6900661821300919\n",
      "batch loss: 0.9191100001335144 | avg loss: 0.6960936510249188\n",
      "batch loss: 0.35111433267593384 | avg loss: 0.6872480274775089\n",
      "batch loss: 0.9538059234619141 | avg loss: 0.6939119748771191\n",
      "batch loss: 0.5832281708717346 | avg loss: 0.691212369901378\n",
      "batch loss: 0.92820805311203 | avg loss: 0.6968551242635364\n",
      "batch loss: 0.8387340903282166 | avg loss: 0.7001546351022498\n",
      "batch loss: 0.9721560478210449 | avg loss: 0.7063364853913133\n",
      "batch loss: 0.6294264793395996 | avg loss: 0.7046273741457197\n",
      "batch loss: 0.8057993650436401 | avg loss: 0.7068267652521962\n",
      "batch loss: 0.5294050574302673 | avg loss: 0.7030518352985382\n",
      "batch loss: 0.624922513961792 | avg loss: 0.7014241411040226\n",
      "batch loss: 0.9792817234992981 | avg loss: 0.7070947040100487\n",
      "batch loss: 0.39965254068374634 | avg loss: 0.7009458607435226\n",
      "batch loss: 0.6356686353683472 | avg loss: 0.699665915147931\n",
      "batch loss: 0.8249793648719788 | avg loss: 0.7020757891810857\n",
      "batch loss: 0.48784172534942627 | avg loss: 0.6980336370333186\n",
      "batch loss: 0.6554921865463257 | avg loss: 0.6972458323946705\n",
      "batch loss: 0.9273961186408997 | avg loss: 0.7014303830536929\n",
      "batch loss: 0.6538221836090088 | avg loss: 0.7005802366350379\n",
      "batch loss: 1.159480094909668 | avg loss: 0.7086311113416103\n",
      "batch loss: 0.4525758624076843 | avg loss: 0.7042163656703357\n",
      "batch loss: 0.6632660627365112 | avg loss: 0.7035222927392539\n",
      "batch loss: 0.45565924048423767 | avg loss: 0.699391241868337\n",
      "batch loss: 0.6834046840667725 | avg loss: 0.6991291671502785\n",
      "batch loss: 0.6886433362960815 | avg loss: 0.6989600408461786\n",
      "batch loss: 0.4083727300167084 | avg loss: 0.6943475438488854\n",
      "batch loss: 0.6791500449180603 | avg loss: 0.6941100829280913\n",
      "batch loss: 1.217612385749817 | avg loss: 0.702163964509964\n",
      "batch loss: 0.8903324604034424 | avg loss: 0.7050150023265318\n",
      "batch loss: 1.382175326347351 | avg loss: 0.7151218728343053\n",
      "batch loss: 0.6616307497024536 | avg loss: 0.7143352386706016\n",
      "batch loss: 0.36519983410835266 | avg loss: 0.7092753052711487\n",
      "batch loss: 0.5819107890129089 | avg loss: 0.7074558121817452\n",
      "batch loss: 0.8954331874847412 | avg loss: 0.7101033808479846\n",
      "batch loss: 1.1166560649871826 | avg loss: 0.7157499459054735\n",
      "batch loss: 0.5912448763847351 | avg loss: 0.7140443970079291\n",
      "batch loss: 0.5622177124023438 | avg loss: 0.7119926850537996\n",
      "batch loss: 0.8052431344985962 | avg loss: 0.7132360243797302\n",
      "batch loss: 0.6851404309272766 | avg loss: 0.7128663455185137\n",
      "batch loss: 0.3707892596721649 | avg loss: 0.7084237859620677\n",
      "batch loss: 0.6832858920097351 | avg loss: 0.7081015052703711\n",
      "batch loss: 0.5842795372009277 | avg loss: 0.7065341385859477\n",
      "batch loss: 0.5310641527175903 | avg loss: 0.7043407637625932\n",
      "batch loss: 0.7601146697998047 | avg loss: 0.7050293305037935\n",
      "batch loss: 0.6546440124511719 | avg loss: 0.7044148754055907\n",
      "batch loss: 0.3244363069534302 | avg loss: 0.6998368203639984\n",
      "batch loss: 0.3678708076477051 | avg loss: 0.6958848440221378\n",
      "batch loss: 0.425697922706604 | avg loss: 0.6927061743596021\n",
      "batch loss: 0.6539502739906311 | avg loss: 0.6922555243553117\n",
      "batch loss: 0.4953784644603729 | avg loss: 0.6899925696438757\n",
      "batch loss: 0.8009707927703857 | avg loss: 0.6912536858157678\n",
      "batch loss: 0.39828670024871826 | avg loss: 0.687961921932992\n",
      "batch loss: 0.6475808024406433 | avg loss: 0.6875132428275215\n",
      "batch loss: 1.2865331172943115 | avg loss: 0.6940958788106729\n",
      "batch loss: 0.42059582471847534 | avg loss: 0.6911230521357578\n",
      "batch loss: 0.7009809017181396 | avg loss: 0.691229050518364\n",
      "batch loss: 0.5554770231246948 | avg loss: 0.689784880014176\n",
      "batch loss: 0.988537609577179 | avg loss: 0.6929296455885234\n",
      "batch loss: 0.3756835162639618 | avg loss: 0.6896249984080592\n",
      "batch loss: 0.7092244625091553 | avg loss: 0.6898270547389984\n",
      "batch loss: 0.911747932434082 | avg loss: 0.692091553490989\n",
      "batch loss: 0.523334801197052 | avg loss: 0.6903869398314544\n",
      "batch loss: 1.1628087759017944 | avg loss: 0.6951111581921577\n",
      "batch loss: 0.31228142976760864 | avg loss: 0.6913207648414197\n",
      "batch loss: 0.8100926280021667 | avg loss: 0.6924851948724073\n",
      "batch loss: 0.6605600118637085 | avg loss: 0.6921752416393132\n",
      "batch loss: 0.4754788279533386 | avg loss: 0.6900916222769481\n",
      "batch loss: 0.5056275725364685 | avg loss: 0.6883348218032292\n",
      "batch loss: 0.4809868037700653 | avg loss: 0.6863787084255578\n",
      "batch loss: 0.7090343832969666 | avg loss: 0.6865904437047299\n",
      "batch loss: 0.8797568082809448 | avg loss: 0.6883790211545097\n",
      "batch loss: 0.7120596766471863 | avg loss: 0.6885962748746259\n",
      "batch loss: 0.8994138836860657 | avg loss: 0.6905127985910936\n",
      "batch loss: 0.7760917544364929 | avg loss: 0.6912837801752864\n",
      "batch loss: 0.4873451590538025 | avg loss: 0.6894628996295589\n",
      "batch loss: 0.6988785862922668 | avg loss: 0.6895462242902908\n",
      "batch loss: 0.6150320768356323 | avg loss: 0.6888925914178815\n",
      "batch loss: 0.46833884716033936 | avg loss: 0.6869747327721637\n",
      "batch loss: 1.0376877784729004 | avg loss: 0.6899981210971701\n",
      "batch loss: 0.4195099472999573 | avg loss: 0.6876862563638606\n",
      "batch loss: 0.5550603866577148 | avg loss: 0.6865623083155034\n",
      "batch loss: 1.1118249893188477 | avg loss: 0.6901359442903214\n",
      "batch loss: 0.35401368141174316 | avg loss: 0.6873349254329999\n",
      "batch loss: 0.8524075746536255 | avg loss: 0.6886991622034183\n",
      "batch loss: 0.5516241192817688 | avg loss: 0.687575596277831\n",
      "batch loss: 0.9067949056625366 | avg loss: 0.6893578670858368\n",
      "batch loss: 0.8877182006835938 | avg loss: 0.6909575471954961\n",
      "batch loss: 0.5660793781280518 | avg loss: 0.6899585218429566\n",
      "batch loss: 0.7524672150611877 | avg loss: 0.6904546225827838\n",
      "batch loss: 0.7487195730209351 | avg loss: 0.6909134017200921\n",
      "batch loss: 0.6851115226745605 | avg loss: 0.6908680745400488\n",
      "batch loss: 0.7159895300865173 | avg loss: 0.6910628145055253\n",
      "batch loss: 0.3732728362083435 | avg loss: 0.6886182762109316\n",
      "batch loss: 0.5287218689918518 | avg loss: 0.6873976929497173\n",
      "batch loss: 0.5646153092384338 | avg loss: 0.6864675233761469\n",
      "batch loss: 0.655038595199585 | avg loss: 0.6862312156454962\n",
      "batch loss: 0.4599940776824951 | avg loss: 0.684542878795026\n",
      "batch loss: 0.4204416573047638 | avg loss: 0.6825865734506537\n",
      "batch loss: 0.2977176308631897 | avg loss: 0.6797566547551576\n",
      "batch loss: 0.424038290977478 | avg loss: 0.6778900973553205\n",
      "batch loss: 0.8956308364868164 | avg loss: 0.6794679287983023\n",
      "batch loss: 0.48451414704322815 | avg loss: 0.6780653836058198\n",
      "batch loss: 0.5401676893234253 | avg loss: 0.6770804000752313\n",
      "batch loss: 0.5154788494110107 | avg loss: 0.6759342897868326\n",
      "batch loss: 0.29600095748901367 | avg loss: 0.6732587029396648\n",
      "batch loss: 0.6356005072593689 | avg loss: 0.6729953589139285\n",
      "batch loss: 0.7112313508987427 | avg loss: 0.6732608866360452\n",
      "batch loss: 0.9326688647270203 | avg loss: 0.6750499071746037\n",
      "batch loss: 0.5952386260032654 | avg loss: 0.6745032545638411\n",
      "batch loss: 0.6944832801818848 | avg loss: 0.6746391731054604\n",
      "batch loss: 0.663560688495636 | avg loss: 0.6745643184797184\n",
      "batch loss: 0.5434414744377136 | avg loss: 0.6736842993921881\n",
      "batch loss: 0.38434451818466187 | avg loss: 0.6717553675174713\n",
      "batch loss: 0.6574507355690002 | avg loss: 0.6716606348555609\n",
      "batch loss: 0.5202019810676575 | avg loss: 0.6706641963437984\n",
      "batch loss: 0.7228743433952332 | avg loss: 0.6710054391349842\n",
      "batch loss: 0.481275349855423 | avg loss: 0.6697734255682338\n",
      "batch loss: 0.8993580937385559 | avg loss: 0.6712546169757843\n",
      "batch loss: 0.9643187522888184 | avg loss: 0.673133233227791\n",
      "batch loss: 0.6282203197479248 | avg loss: 0.6728471637151803\n",
      "batch loss: 0.41323500871658325 | avg loss: 0.6712040488101259\n",
      "batch loss: 0.5535814166069031 | avg loss: 0.6704642838277157\n",
      "batch loss: 0.5730271935462952 | avg loss: 0.6698553020134568\n",
      "batch loss: 0.7249573469161987 | avg loss: 0.6701975507395608\n",
      "batch loss: 0.6948972344398499 | avg loss: 0.670350017922896\n",
      "batch loss: 0.4653782248497009 | avg loss: 0.6690925222598701\n",
      "batch loss: 0.7137183547019958 | avg loss: 0.6693646309942733\n",
      "batch loss: 1.2073652744293213 | avg loss: 0.6726252409544858\n",
      "batch loss: 0.8039270639419556 | avg loss: 0.6734162157917597\n",
      "batch loss: 1.0077424049377441 | avg loss: 0.6754181690201788\n",
      "batch loss: 1.1087398529052734 | avg loss: 0.67799746475759\n",
      "batch loss: 0.4345792233943939 | avg loss: 0.6765571201341392\n",
      "batch loss: 0.2799450159072876 | avg loss: 0.6742241077563342\n",
      "batch loss: 0.7612834572792053 | avg loss: 0.674733226759392\n",
      "batch loss: 0.44058382511138916 | avg loss: 0.6733718930288802\n",
      "batch loss: 0.6908766627311707 | avg loss: 0.6734730766687779\n",
      "batch loss: 0.5154249668121338 | avg loss: 0.6725647541983374\n",
      "batch loss: 0.4031378924846649 | avg loss: 0.6710251721314021\n",
      "batch loss: 0.5690264105796814 | avg loss: 0.6704456337134946\n",
      "batch loss: 0.7193151712417603 | avg loss: 0.6707217327955752\n",
      "batch loss: 0.8596723675727844 | avg loss: 0.6717832532156719\n",
      "batch loss: 0.6819232702255249 | avg loss: 0.6718399013553917\n",
      "batch loss: 1.0347614288330078 | avg loss: 0.6738561320636007\n",
      "batch loss: 0.7104744911193848 | avg loss: 0.6740584434395995\n",
      "batch loss: 0.804949164390564 | avg loss: 0.6747776232250444\n",
      "batch loss: 0.8472371101379395 | avg loss: 0.6757200247928744\n",
      "batch loss: 0.9935647249221802 | avg loss: 0.6774474416414032\n",
      "batch loss: 0.4960140287876129 | avg loss: 0.6764667204908422\n",
      "batch loss: 0.7985475063323975 | avg loss: 0.6771230688018184\n",
      "batch loss: 0.7988772392272949 | avg loss: 0.6777741606222755\n",
      "batch loss: 0.7545807957649231 | avg loss: 0.6781827065538852\n",
      "batch loss: 0.6080321669578552 | avg loss: 0.6778115396777158\n",
      "batch loss: 0.8510714173316956 | avg loss: 0.6787234337706315\n",
      "batch loss: 0.6573246717453003 | avg loss: 0.6786113983673575\n",
      "batch loss: 0.5054767727851868 | avg loss: 0.6777096555257837\n",
      "batch loss: 0.5283195376396179 | avg loss: 0.6769356134641974\n",
      "batch loss: 0.44253867864608765 | avg loss: 0.6757273818414236\n",
      "batch loss: 0.6518838405609131 | avg loss: 0.6756051072707543\n",
      "batch loss: 1.41526198387146 | avg loss: 0.6793788668452477\n",
      "batch loss: 0.255175918340683 | avg loss: 0.6772255523858337\n",
      "batch loss: 0.476002961397171 | avg loss: 0.6762092766737697\n",
      "batch loss: 0.45178961753845215 | avg loss: 0.6750815396931902\n",
      "batch loss: 0.43291452527046204 | avg loss: 0.6738707046210766\n",
      "batch loss: 0.6049143671989441 | avg loss: 0.6735276382657426\n",
      "batch loss: 1.0227311849594116 | avg loss: 0.6752563686949191\n",
      "batch loss: 0.337446928024292 | avg loss: 0.6735922827802855\n",
      "batch loss: 0.5889698266983032 | avg loss: 0.6731774668190994\n",
      "batch loss: 0.7366579174995422 | avg loss: 0.6734871275541259\n",
      "batch loss: 0.684393048286438 | avg loss: 0.6735400689169041\n",
      "batch loss: 0.4739006459712982 | avg loss: 0.6725756272601621\n",
      "batch loss: 0.6350502967834473 | avg loss: 0.6723952170174855\n",
      "batch loss: 0.7560274004936218 | avg loss: 0.6727953710054096\n",
      "batch loss: 0.6379204988479614 | avg loss: 0.6726293001856123\n",
      "batch loss: 1.0005918741226196 | avg loss: 0.6741836251805744\n",
      "batch loss: 0.6395723819732666 | avg loss: 0.6740203645994078\n",
      "batch loss: 0.7384803891181946 | avg loss: 0.6743229938225007\n",
      "batch loss: 0.8144396543502808 | avg loss: 0.6749777445726306\n",
      "batch loss: 0.5140302181243896 | avg loss: 0.6742291514263596\n",
      "batch loss: 0.4746527671813965 | avg loss: 0.6733051866844848\n",
      "batch loss: 0.32583844661712646 | avg loss: 0.6717039574675846\n",
      "batch loss: 1.014449119567871 | avg loss: 0.673276182981806\n",
      "batch loss: 0.7777387499809265 | avg loss: 0.6737531810046331\n",
      "batch loss: 0.6982545256614685 | avg loss: 0.6738645507530733\n",
      "batch loss: 0.5345079898834229 | avg loss: 0.673233978079455\n",
      "batch loss: 1.0523555278778076 | avg loss: 0.6749417328082763\n",
      "batch loss: 1.155797004699707 | avg loss: 0.677098034475951\n",
      "batch loss: 0.6039446592330933 | avg loss: 0.6767714569079024\n",
      "batch loss: 0.33680492639541626 | avg loss: 0.6752604945500692\n",
      "batch loss: 0.6954056024551392 | avg loss: 0.6753496321956668\n",
      "batch loss: 0.5623617768287659 | avg loss: 0.674851888339425\n",
      "batch loss: 0.7197964191436768 | avg loss: 0.6750490134745314\n",
      "batch loss: 0.7447859644889832 | avg loss: 0.6753535416449001\n",
      "batch loss: 0.7227036356925964 | avg loss: 0.6755594116190206\n",
      "batch loss: 0.5439451336860657 | avg loss: 0.67498965284009\n",
      "batch loss: 0.511002779006958 | avg loss: 0.6742828128666714\n",
      "batch loss: 0.33743688464164734 | avg loss: 0.672837122187594\n",
      "batch loss: 0.5956306457519531 | avg loss: 0.672507179980604\n",
      "batch loss: 0.5333114862442017 | avg loss: 0.671914857879598\n",
      "batch loss: 0.4890398383140564 | avg loss: 0.6711399637288966\n",
      "batch loss: 0.4811456799507141 | avg loss: 0.670338300084263\n",
      "batch loss: 0.6972606182098389 | avg loss: 0.6704514190679839\n",
      "batch loss: 0.4753802418708801 | avg loss: 0.6696352216738537\n",
      "batch loss: 0.826239824295044 | avg loss: 0.670287740851442\n",
      "batch loss: 0.7120991349220276 | avg loss: 0.6704612321131457\n",
      "batch loss: 0.6311086416244507 | avg loss: 0.6702986181028618\n",
      "batch loss: 0.5806309580802917 | avg loss: 0.6699296153867196\n",
      "batch loss: 0.536483645439148 | avg loss: 0.6693827056738196\n",
      "batch loss: 0.7490324378013611 | avg loss: 0.669707806621279\n",
      "batch loss: 0.639644980430603 | avg loss: 0.6695856000107479\n",
      "batch loss: 0.8398098945617676 | avg loss: 0.6702747671951649\n",
      "batch loss: 0.3929811120033264 | avg loss: 0.6691566476177785\n",
      "batch loss: 0.3045591711997986 | avg loss: 0.6676924007245336\n",
      "batch loss: 0.3476005792617798 | avg loss: 0.6664120334386826\n",
      "batch loss: 0.5384953618049622 | avg loss: 0.665902405264843\n",
      "batch loss: 0.46160662174224854 | avg loss: 0.665091707711182\n",
      "batch loss: 0.6661084294319153 | avg loss: 0.6650957263741097\n",
      "batch loss: 1.1138029098510742 | avg loss: 0.666862290088586\n",
      "batch loss: 0.5192461609840393 | avg loss: 0.6662834033077838\n",
      "batch loss: 0.4200271666049957 | avg loss: 0.6653214648831636\n",
      "batch loss: 0.6787427663803101 | avg loss: 0.6653736878461874\n",
      "batch loss: 0.18388119339942932 | avg loss: 0.6635074378677117\n",
      "batch loss: 0.38888898491859436 | avg loss: 0.6624471349605723\n",
      "batch loss: 0.6200101971626282 | avg loss: 0.6622839159690417\n",
      "batch loss: 1.0073153972625732 | avg loss: 0.6636058756674843\n",
      "batch loss: 0.6176508069038391 | avg loss: 0.6634304746416689\n",
      "batch loss: 0.41730719804763794 | avg loss: 0.6624946446926422\n",
      "batch loss: 0.922082781791687 | avg loss: 0.6634779330907445\n",
      "batch loss: 0.36377421021461487 | avg loss: 0.662346975645929\n",
      "batch loss: 0.45072242617607117 | avg loss: 0.6615513946328845\n",
      "batch loss: 0.6269229650497437 | avg loss: 0.6614217001400636\n",
      "batch loss: 0.2731812596321106 | avg loss: 0.6599730417799594\n",
      "batch loss: 0.36799994111061096 | avg loss: 0.658887639918735\n",
      "batch loss: 0.2844381630420685 | avg loss: 0.657500790004377\n",
      "batch loss: 0.7510904669761658 | avg loss: 0.6578461393658965\n",
      "batch loss: 0.5683610439300537 | avg loss: 0.6575171500444412\n",
      "batch loss: 0.541654646396637 | avg loss: 0.6570927452691745\n",
      "batch loss: 0.5128502249717712 | avg loss: 0.6565663127133446\n",
      "batch loss: 0.6408146619796753 | avg loss: 0.656509033983404\n",
      "batch loss: 0.6574955582618713 | avg loss: 0.6565126083467318\n",
      "batch loss: 0.5768950581550598 | avg loss: 0.6562251803677005\n",
      "batch loss: 0.823584258556366 | avg loss: 0.6568271914403215\n",
      "batch loss: 1.4675328731536865 | avg loss: 0.6597329465719107\n",
      "batch loss: 0.2168802171945572 | avg loss: 0.6581513296812773\n",
      "batch loss: 0.858628511428833 | avg loss: 0.6588647716092045\n",
      "batch loss: 0.549386739730835 | avg loss: 0.6584765516380047\n",
      "batch loss: 0.577915370464325 | avg loss: 0.658191883153292\n",
      "batch loss: 0.7130225896835327 | avg loss: 0.6583849490213562\n",
      "batch loss: 0.732513427734375 | avg loss: 0.6586450489466651\n",
      "batch loss: 0.5481268167495728 | avg loss: 0.6582586215613605\n",
      "batch loss: 0.5234608054161072 | avg loss: 0.6577889427594606\n",
      "batch loss: 0.4648888111114502 | avg loss: 0.6571191506356828\n",
      "batch loss: 0.5984093546867371 | avg loss: 0.6569160025528146\n",
      "batch loss: 0.8615881204605103 | avg loss: 0.6576217684766342\n",
      "batch loss: 0.41532978415489197 | avg loss: 0.6567891499738103\n",
      "batch loss: 0.9105340838432312 | avg loss: 0.6576581394733632\n",
      "batch loss: 0.4627211093902588 | avg loss: 0.6569928253775164\n",
      "batch loss: 0.684727668762207 | avg loss: 0.6570871615795051\n",
      "batch loss: 0.40917304158210754 | avg loss: 0.6562467747320563\n",
      "batch loss: 0.8016608953475952 | avg loss: 0.6567380386530548\n",
      "batch loss: 0.47061407566070557 | avg loss: 0.6561113586429795\n",
      "batch loss: 0.5935975313186646 | avg loss: 0.6559015806989382\n",
      "batch loss: 0.776279091835022 | avg loss: 0.6563041810706308\n",
      "batch loss: 0.6028928160667419 | avg loss: 0.6561261431872845\n",
      "batch loss: 0.7918696999549866 | avg loss: 0.656577118458938\n",
      "batch loss: 0.5306243896484375 | avg loss: 0.6561600564430091\n",
      "batch loss: 0.5272820591926575 | avg loss: 0.6557347165180905\n",
      "batch loss: 0.7542198896408081 | avg loss: 0.6560586809033626\n",
      "batch loss: 0.7770677804946899 | avg loss: 0.6564554320495637\n",
      "batch loss: 0.7176627516746521 | avg loss: 0.6566554559699072\n",
      "batch loss: 0.9191843867301941 | avg loss: 0.6575105990668462\n",
      "batch loss: 0.4523335099220276 | avg loss: 0.6568444396865059\n",
      "batch loss: 0.6426855325698853 | avg loss: 0.6567986179806268\n",
      "batch loss: 0.540549099445343 | avg loss: 0.6564236195337387\n",
      "batch loss: 0.5254783034324646 | avg loss: 0.6560025735012588\n",
      "batch loss: 0.7498278617858887 | avg loss: 0.6563032955790942\n",
      "batch loss: 1.1188076734542847 | avg loss: 0.6577809453486635\n",
      "batch loss: 0.4490721821784973 | avg loss: 0.6571162677589496\n",
      "batch loss: 0.500420331954956 | avg loss: 0.6566188203436988\n",
      "batch loss: 1.295724630355835 | avg loss: 0.6586413070842435\n",
      "batch loss: 0.35123565793037415 | avg loss: 0.6576715731752407\n",
      "batch loss: 0.5868064761161804 | avg loss: 0.6574487269580739\n",
      "batch loss: 0.8665570020675659 | avg loss: 0.658104238792273\n",
      "batch loss: 0.4928639233112335 | avg loss: 0.6575878628063947\n",
      "batch loss: 0.8352453112602234 | avg loss: 0.658141312801578\n",
      "batch loss: 0.47579410672187805 | avg loss: 0.657575017130523\n",
      "batch loss: 0.5783811807632446 | avg loss: 0.6573298349745872\n",
      "batch loss: 0.5070330500602722 | avg loss: 0.6568659560088023\n",
      "batch loss: 0.938472330570221 | avg loss: 0.6577324371612989\n",
      "batch loss: 0.47456565499305725 | avg loss: 0.6571705758663043\n",
      "batch loss: 0.5126650333404541 | avg loss: 0.6567286628922191\n",
      "batch loss: 0.4556087851524353 | avg loss: 0.6561154925332564\n",
      "batch loss: 0.9739277362823486 | avg loss: 0.6570814871951077\n",
      "batch loss: 0.6558827757835388 | avg loss: 0.6570778547362848\n",
      "batch loss: 0.6757209897041321 | avg loss: 0.6571341784068825\n",
      "batch loss: 0.862322211265564 | avg loss: 0.6577522146504328\n",
      "batch loss: 0.5625603199005127 | avg loss: 0.6574663531046372\n",
      "batch loss: 0.3295783996582031 | avg loss: 0.6564846526452167\n",
      "batch loss: 0.2910701036453247 | avg loss: 0.6553938629467095\n",
      "batch loss: 0.42238837480545044 | avg loss: 0.6547003942320034\n",
      "batch loss: 0.6630510091781616 | avg loss: 0.6547251735048407\n",
      "batch loss: 0.33424368500709534 | avg loss: 0.6537770034205279\n",
      "batch loss: 0.38732248544692993 | avg loss: 0.6529910018925822\n",
      "batch loss: 0.809013843536377 | avg loss: 0.6534498926032992\n",
      "batch loss: 0.5678573846817017 | avg loss: 0.6531988881812417\n",
      "batch loss: 0.4655713438987732 | avg loss: 0.6526502696307082\n",
      "batch loss: 0.4958721101284027 | avg loss: 0.6521931904484858\n",
      "batch loss: 0.5670386552810669 | avg loss: 0.651945648195092\n",
      "batch loss: 0.5697556734085083 | avg loss: 0.6517074163841164\n",
      "batch loss: 0.36825546622276306 | avg loss: 0.6508881910946328\n",
      "batch loss: 0.7066753506660461 | avg loss: 0.6510489610069423\n",
      "batch loss: 0.556597888469696 | avg loss: 0.6507775498789617\n",
      "batch loss: 0.8361678123474121 | avg loss: 0.6513087540694158\n",
      "batch loss: 0.29514622688293457 | avg loss: 0.6502911468488829\n",
      "batch loss: 1.4280829429626465 | avg loss: 0.6525070778919421\n",
      "batch loss: 0.8322797417640686 | avg loss: 0.6530177956870333\n",
      "batch loss: 0.27262336015701294 | avg loss: 0.6519401910538039\n",
      "batch loss: 0.6485121846199036 | avg loss: 0.6519305074198097\n",
      "batch loss: 0.5287425518035889 | avg loss: 0.6515834990941303\n",
      "batch loss: 0.6125830411911011 | avg loss: 0.6514739472460881\n",
      "batch loss: 0.4722648859024048 | avg loss: 0.6509719610798592\n",
      "batch loss: 0.8980873823165894 | avg loss: 0.6516622276196267\n",
      "batch loss: 1.3930683135986328 | avg loss: 0.6537274256307103\n",
      "batch loss: 0.9800187349319458 | avg loss: 0.6546337903787692\n",
      "batch loss: 0.9682227969169617 | avg loss: 0.6555024579869083\n",
      "batch loss: 1.0098683834075928 | avg loss: 0.656481369383098\n",
      "\n",
      "  Average training loss: 0.66\n",
      "  Training epoch took: 0:26:48\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.49\n",
      "  Validation took: 0:01:51\n",
      "\n",
      "======= Epoch 5 / 5 =======\n",
      "batch loss: 0.2921904921531677 | avg loss: 0.2921904921531677\n",
      "batch loss: 0.2598766088485718 | avg loss: 0.27603355050086975\n",
      "batch loss: 0.4867493510246277 | avg loss: 0.34627215067545575\n",
      "batch loss: 0.6094843149185181 | avg loss: 0.4120751917362213\n",
      "batch loss: 0.37888628244400024 | avg loss: 0.4054374098777771\n",
      "batch loss: 0.4403092563152313 | avg loss: 0.41124938428401947\n",
      "batch loss: 0.6639391779899597 | avg loss: 0.4473479262420109\n",
      "batch loss: 0.37692394852638245 | avg loss: 0.4385449290275574\n",
      "batch loss: 0.4305962920188904 | avg loss: 0.4376617471377055\n",
      "batch loss: 0.3780255615711212 | avg loss: 0.43169812858104706\n",
      "batch loss: 0.6456998586654663 | avg loss: 0.45115283131599426\n",
      "batch loss: 1.040558099746704 | avg loss: 0.5002699370185534\n",
      "batch loss: 0.7403784394264221 | avg loss: 0.5187398218191587\n",
      "batch loss: 0.2467413991689682 | avg loss: 0.4993113630584308\n",
      "batch loss: 0.6689634323120117 | avg loss: 0.5106215010086695\n",
      "batch loss: 0.6340636014938354 | avg loss: 0.5183366322889924\n",
      "batch loss: 0.38034960627555847 | avg loss: 0.5102197484058493\n",
      "batch loss: 0.4992896318435669 | avg loss: 0.5096125197079446\n",
      "batch loss: 0.3439798653125763 | avg loss: 0.5008950115818727\n",
      "batch loss: 0.5767494440078735 | avg loss: 0.5046877332031727\n",
      "batch loss: 0.42340436577796936 | avg loss: 0.5008170966591153\n",
      "batch loss: 0.4950698912143707 | avg loss: 0.5005558600479906\n",
      "batch loss: 0.5298205614089966 | avg loss: 0.5018282383680344\n",
      "batch loss: 0.684617817401886 | avg loss: 0.5094444708277782\n",
      "batch loss: 0.4822307527065277 | avg loss: 0.5083559221029281\n",
      "batch loss: 0.7724005579948425 | avg loss: 0.518511485021848\n",
      "batch loss: 0.6431465744972229 | avg loss: 0.5231275994468618\n",
      "batch loss: 0.5703484416007996 | avg loss: 0.5248140580952168\n",
      "batch loss: 0.5033643245697021 | avg loss: 0.5240744121115783\n",
      "batch loss: 0.42617279291152954 | avg loss: 0.52081102480491\n",
      "batch loss: 0.5019480586051941 | avg loss: 0.5202025420242741\n",
      "batch loss: 0.43817490339279175 | avg loss: 0.5176391783170402\n",
      "batch loss: 0.3209339380264282 | avg loss: 0.511678413459749\n",
      "batch loss: 0.4392522871494293 | avg loss: 0.5095482332741513\n",
      "batch loss: 0.6713996529579163 | avg loss: 0.5141725595508303\n",
      "batch loss: 0.8958921432495117 | avg loss: 0.5247758813202381\n",
      "batch loss: 0.8563168048858643 | avg loss: 0.5337364468220118\n",
      "batch loss: 0.5884227752685547 | avg loss: 0.5351755607284998\n",
      "batch loss: 0.18831031024456024 | avg loss: 0.5262815799468603\n",
      "batch loss: 0.8171457648277283 | avg loss: 0.533553184568882\n",
      "batch loss: 0.5191128253936768 | avg loss: 0.5332009806865599\n",
      "batch loss: 0.6489754915237427 | avg loss: 0.5359575166588738\n",
      "batch loss: 0.47377443313598633 | avg loss: 0.5345113984374112\n",
      "batch loss: 0.6594542264938354 | avg loss: 0.5373510081659664\n",
      "batch loss: 0.6063117384910583 | avg loss: 0.5388834688398573\n",
      "batch loss: 0.6669999361038208 | avg loss: 0.5416686094325521\n",
      "batch loss: 0.38960349559783936 | avg loss: 0.5384331814786221\n",
      "batch loss: 0.39710772037506104 | avg loss: 0.5354889010389646\n",
      "batch loss: 0.8128973841667175 | avg loss: 0.5411502986538167\n",
      "batch loss: 0.28297919034957886 | avg loss: 0.5359868764877319\n",
      "batch loss: 0.6054943203926086 | avg loss: 0.5373497675446903\n",
      "batch loss: 0.5252923965454102 | avg loss: 0.5371178950254734\n",
      "batch loss: 0.24342118203639984 | avg loss: 0.5315764476105852\n",
      "batch loss: 0.5467410087585449 | avg loss: 0.5318572728170289\n",
      "batch loss: 0.7637012004852295 | avg loss: 0.5360726169564507\n",
      "batch loss: 0.2708609104156494 | avg loss: 0.531336693625365\n",
      "batch loss: 0.8065691590309143 | avg loss: 0.536165333369322\n",
      "batch loss: 0.33108285069465637 | avg loss: 0.5326294284956209\n",
      "batch loss: 0.2231796830892563 | avg loss: 0.5273845175565299\n",
      "batch loss: 0.4898393750190735 | avg loss: 0.5267587651809057\n",
      "batch loss: 0.5047305226325989 | avg loss: 0.5263976464506055\n",
      "batch loss: 0.43406060338020325 | avg loss: 0.5249083393043087\n",
      "batch loss: 0.37281233072280884 | avg loss: 0.5224941169458722\n",
      "batch loss: 0.5561116337776184 | avg loss: 0.5230193906463683\n",
      "batch loss: 1.1694155931472778 | avg loss: 0.5329639476079208\n",
      "batch loss: 0.5487866401672363 | avg loss: 0.533203685373971\n",
      "batch loss: 1.29984450340271 | avg loss: 0.5446460856430566\n",
      "batch loss: 0.4574911594390869 | avg loss: 0.5433643955518218\n",
      "batch loss: 0.32328033447265625 | avg loss: 0.5401747714782107\n",
      "batch loss: 0.6916362047195435 | avg loss: 0.5423385062388011\n",
      "batch loss: 0.44993895292282104 | avg loss: 0.5410371040794212\n",
      "batch loss: 0.9543403387069702 | avg loss: 0.5467774267825816\n",
      "batch loss: 0.6124814748764038 | avg loss: 0.5476774822359216\n",
      "batch loss: 0.4085221588611603 | avg loss: 0.5457970048930194\n",
      "batch loss: 0.5832081437110901 | avg loss: 0.5462958200772603\n",
      "batch loss: 0.233488067984581 | avg loss: 0.5421799286023566\n",
      "batch loss: 0.2226356863975525 | avg loss: 0.5380300033789176\n",
      "batch loss: 0.5043280720710754 | avg loss: 0.5375979273365095\n",
      "batch loss: 0.5968253016471863 | avg loss: 0.5383476409353788\n",
      "batch loss: 0.2843252718448639 | avg loss: 0.5351723613217473\n",
      "batch loss: 0.5309638977050781 | avg loss: 0.5351204049808008\n",
      "batch loss: 0.5038477778434753 | avg loss: 0.534739031479126\n",
      "batch loss: 0.2661897540092468 | avg loss: 0.5315034980156336\n",
      "batch loss: 0.15954342484474182 | avg loss: 0.5270754019064563\n",
      "batch loss: 0.1984308660030365 | avg loss: 0.5232089956017102\n",
      "batch loss: 0.39653879404067993 | avg loss: 0.521736086281233\n",
      "batch loss: 0.35161128640174866 | avg loss: 0.5197806288113539\n",
      "batch loss: 0.7307515144348145 | avg loss: 0.5221780252388932\n",
      "batch loss: 0.2293260097503662 | avg loss: 0.5188875531547525\n",
      "batch loss: 0.6439785957336426 | avg loss: 0.5202774536278513\n",
      "batch loss: 0.9543411731719971 | avg loss: 0.5250473846118529\n",
      "batch loss: 0.2515372037887573 | avg loss: 0.5220744478637758\n",
      "batch loss: 0.6706158518791199 | avg loss: 0.5236716672617902\n",
      "batch loss: 0.5134581327438354 | avg loss: 0.5235630126392588\n",
      "batch loss: 1.1835060119628906 | avg loss: 0.5305097810531917\n",
      "batch loss: 0.2094268649816513 | avg loss: 0.5271651673441132\n",
      "batch loss: 0.5839513540267944 | avg loss: 0.5277505919490892\n",
      "batch loss: 1.0607339143753052 | avg loss: 0.533189197279969\n",
      "batch loss: 0.2869545817375183 | avg loss: 0.5307019789411564\n",
      "batch loss: 1.063420295715332 | avg loss: 0.5360291621088982\n",
      "batch loss: 0.18778012692928314 | avg loss: 0.5325811518595951\n",
      "batch loss: 0.5004675984382629 | avg loss: 0.5322663131005624\n",
      "batch loss: 0.6197768449783325 | avg loss: 0.5331159299149096\n",
      "batch loss: 0.3235391676425934 | avg loss: 0.5311007687392143\n",
      "batch loss: 0.30334141850471497 | avg loss: 0.5289316320703144\n",
      "batch loss: 0.21165448427200317 | avg loss: 0.5259384514307076\n",
      "batch loss: 0.4112808108329773 | avg loss: 0.5248668846961494\n",
      "batch loss: 0.7153797745704651 | avg loss: 0.5266308929357264\n",
      "batch loss: 0.521652102470398 | avg loss: 0.5265852159589802\n",
      "batch loss: 0.5441605448722839 | avg loss: 0.5267449916763739\n",
      "batch loss: 0.36003029346466064 | avg loss: 0.5252430574582504\n",
      "batch loss: 0.21633663773536682 | avg loss: 0.5224849644250104\n",
      "batch loss: 0.38299480080604553 | avg loss: 0.5212505381982938\n",
      "batch loss: 0.6032233238220215 | avg loss: 0.521969597721309\n",
      "batch loss: 0.3331727683544159 | avg loss: 0.5203278861615969\n",
      "batch loss: 0.6376535892486572 | avg loss: 0.5213393146364853\n",
      "batch loss: 0.42357301712036133 | avg loss: 0.5205037052560056\n",
      "batch loss: 0.30573570728302 | avg loss: 0.5186836374765735\n",
      "batch loss: 0.9415180087089539 | avg loss: 0.5222368674869297\n",
      "batch loss: 0.20968776941299438 | avg loss: 0.5196322916696469\n",
      "batch loss: 0.7344847321510315 | avg loss: 0.5214079316736253\n",
      "batch loss: 0.2527464032173157 | avg loss: 0.5192057879977539\n",
      "batch loss: 0.5870789289474487 | avg loss: 0.5197576021518164\n",
      "batch loss: 0.47327113151550293 | avg loss: 0.5193827112595881\n",
      "batch loss: 0.3341299593448639 | avg loss: 0.5179006892442704\n",
      "batch loss: 0.7153867483139038 | avg loss: 0.5194680389194262\n",
      "batch loss: 0.5566639304161072 | avg loss: 0.5197609199548331\n",
      "batch loss: 0.47752848267555237 | avg loss: 0.5194309790385887\n",
      "batch loss: 0.508452832698822 | avg loss: 0.5193458771289781\n",
      "batch loss: 0.30634695291519165 | avg loss: 0.5177074238657952\n",
      "batch loss: 0.4127740263938904 | avg loss: 0.5169064055644829\n",
      "batch loss: 0.3359443247318268 | avg loss: 0.51553548070969\n",
      "batch loss: 0.4048462212085724 | avg loss: 0.5147032306382531\n",
      "batch loss: 0.24304917454719543 | avg loss: 0.5126759615629467\n",
      "batch loss: 0.28894999623298645 | avg loss: 0.5110187321901322\n",
      "batch loss: 0.15528902411460876 | avg loss: 0.5084030725719297\n",
      "batch loss: 0.3873577117919922 | avg loss: 0.5075195297925141\n",
      "batch loss: 0.5284690260887146 | avg loss: 0.5076713377366895\n",
      "batch loss: 0.7316108345985413 | avg loss: 0.5092824132536813\n",
      "batch loss: 0.34963753819465637 | avg loss: 0.5081420927175454\n",
      "batch loss: 0.593619704246521 | avg loss: 0.5087483169127863\n",
      "batch loss: 0.27639269828796387 | avg loss: 0.5071120097393721\n",
      "batch loss: 0.3898817300796509 | avg loss: 0.5062922175739195\n",
      "batch loss: 0.5020262002944946 | avg loss: 0.5062625924539235\n",
      "batch loss: 0.8847078680992126 | avg loss: 0.5088725598721668\n",
      "batch loss: 0.4073238670825958 | avg loss: 0.5081770208804575\n",
      "batch loss: 0.43926742672920227 | avg loss: 0.5077082481311292\n",
      "batch loss: 0.49527081847190857 | avg loss: 0.5076242114442426\n",
      "batch loss: 0.4339132606983185 | avg loss: 0.5071295070768203\n",
      "batch loss: 0.54163658618927 | avg loss: 0.5073595542709033\n",
      "batch loss: 0.4254950284957886 | avg loss: 0.5068174050935846\n",
      "batch loss: 0.3643653392791748 | avg loss: 0.5058802204500688\n",
      "batch loss: 0.3952372968196869 | avg loss: 0.5051570640864715\n",
      "batch loss: 0.44012078642845154 | avg loss: 0.5047347505951857\n",
      "batch loss: 0.6822232604026794 | avg loss: 0.505879837755234\n",
      "batch loss: 0.5415536165237427 | avg loss: 0.5061085158242629\n",
      "batch loss: 0.4384816884994507 | avg loss: 0.5056777717011749\n",
      "batch loss: 0.2172449380159378 | avg loss: 0.5038522474373444\n",
      "batch loss: 0.669478178024292 | avg loss: 0.5048939199567591\n",
      "batch loss: 0.4191628694534302 | avg loss: 0.5043581008911133\n",
      "batch loss: 0.557409942150116 | avg loss: 0.5046876154330947\n",
      "batch loss: 0.7038726210594177 | avg loss: 0.5059171525048621\n",
      "batch loss: 0.31334125995635986 | avg loss: 0.504735705311313\n",
      "batch loss: 0.6578490138053894 | avg loss: 0.505669323046033\n",
      "batch loss: 0.6416602730751038 | avg loss: 0.5064935106219668\n",
      "batch loss: 0.7054433822631836 | avg loss: 0.5076920038246246\n",
      "batch loss: 1.0158882141113281 | avg loss: 0.5107350949041858\n",
      "batch loss: 1.069493293762207 | avg loss: 0.514061036564055\n",
      "batch loss: 0.3526608347892761 | avg loss: 0.5131060057843225\n",
      "batch loss: 0.217039093375206 | avg loss: 0.5113644357113277\n",
      "batch loss: 0.4956112205982208 | avg loss: 0.5112723116463388\n",
      "batch loss: 0.41845789551734924 | avg loss: 0.5107326929479145\n",
      "batch loss: 0.42706283926963806 | avg loss: 0.5102490521752077\n",
      "batch loss: 0.5633417367935181 | avg loss: 0.5105541825465773\n",
      "batch loss: 0.2914718985557556 | avg loss: 0.5093022837809154\n",
      "batch loss: 0.5495997667312622 | avg loss: 0.5095312467522242\n",
      "batch loss: 0.6352728009223938 | avg loss: 0.5102416510130726\n",
      "batch loss: 0.5724489688873291 | avg loss: 0.5105911303269729\n",
      "batch loss: 0.4518199563026428 | avg loss: 0.5102627997458314\n",
      "batch loss: 0.8992960453033447 | avg loss: 0.5124240955544843\n",
      "batch loss: 0.8136129379272461 | avg loss: 0.5140881223079249\n",
      "batch loss: 0.852755069732666 | avg loss: 0.5159489297113575\n",
      "batch loss: 0.991742730140686 | avg loss: 0.5185488958339222\n",
      "batch loss: 0.885585367679596 | avg loss: 0.520543659267866\n",
      "batch loss: 0.33809009194374084 | avg loss: 0.5195574237688168\n",
      "batch loss: 0.51725172996521 | avg loss: 0.5195450275655715\n",
      "batch loss: 0.4622271955013275 | avg loss: 0.5192385150946398\n",
      "batch loss: 0.42464831471443176 | avg loss: 0.5187353757309153\n",
      "batch loss: 0.42127883434295654 | avg loss: 0.5182197326547885\n",
      "batch loss: 0.8320112824440002 | avg loss: 0.5198712671273633\n",
      "batch loss: 0.4398317039012909 | avg loss: 0.5194522118225148\n",
      "batch loss: 0.4698191285133362 | avg loss: 0.5191937061802795\n",
      "batch loss: 0.2855711579322815 | avg loss: 0.5179832266556784\n",
      "batch loss: 0.25229236483573914 | avg loss: 0.5166136861308334\n",
      "batch loss: 0.5457109212875366 | avg loss: 0.5167629027213806\n",
      "batch loss: 1.2518337965011597 | avg loss: 0.5205132644243387\n",
      "batch loss: 0.3108995258808136 | avg loss: 0.519449235294676\n",
      "batch loss: 0.2689477503299713 | avg loss: 0.5181840762797029\n",
      "batch loss: 0.3340018391609192 | avg loss: 0.517258537399709\n",
      "batch loss: 0.3242335915565491 | avg loss: 0.5162934126704931\n",
      "batch loss: 0.39437615871429443 | avg loss: 0.5156868591682234\n",
      "batch loss: 0.878449022769928 | avg loss: 0.5174827114632814\n",
      "batch loss: 0.24738380312919617 | avg loss: 0.5161521749690249\n",
      "batch loss: 0.3971921503543854 | avg loss: 0.5155690375934628\n",
      "batch loss: 0.5775297284126282 | avg loss: 0.5158712848657515\n",
      "batch loss: 0.6873459219932556 | avg loss: 0.5167036860168559\n",
      "batch loss: 0.13114087283611298 | avg loss: 0.5148410637309586\n",
      "batch loss: 0.49124178290367126 | avg loss: 0.5147276056500582\n",
      "batch loss: 0.5389493107795715 | avg loss: 0.5148434989760367\n",
      "batch loss: 0.3833816647529602 | avg loss: 0.5142174902416411\n",
      "batch loss: 0.573279857635498 | avg loss: 0.5144974066747874\n",
      "batch loss: 0.4573085904121399 | avg loss: 0.5142276481075106\n",
      "batch loss: 0.5901572704315186 | avg loss: 0.5145841252076234\n",
      "batch loss: 0.8102539777755737 | avg loss: 0.5159657600327073\n",
      "batch loss: 0.32740941643714905 | avg loss: 0.5150887537834257\n",
      "batch loss: 0.5060829520225525 | avg loss: 0.515047060256755\n",
      "batch loss: 0.20084884762763977 | avg loss: 0.513599142226206\n",
      "batch loss: 0.7565606236457825 | avg loss: 0.5147136444345527\n",
      "batch loss: 0.3610950708389282 | avg loss: 0.5140121897606\n",
      "batch loss: 0.5235773324966431 | avg loss: 0.5140556676821275\n",
      "batch loss: 0.5272462368011475 | avg loss: 0.5141153535152453\n",
      "batch loss: 0.7435700297355652 | avg loss: 0.5151489331378593\n",
      "batch loss: 0.6276878118515015 | avg loss: 0.5156535917867994\n",
      "batch loss: 0.5113834142684937 | avg loss: 0.515634528494307\n",
      "batch loss: 0.18379634618759155 | avg loss: 0.5141596921284993\n",
      "batch loss: 0.7067107558250427 | avg loss: 0.5150116879855637\n",
      "batch loss: 0.49410319328308105 | avg loss: 0.5149195800793853\n",
      "batch loss: 0.5579354763031006 | avg loss: 0.5151082462908929\n",
      "batch loss: 0.6977475881576538 | avg loss: 0.5159057980021015\n",
      "batch loss: 0.5621475577354431 | avg loss: 0.5161068491313768\n",
      "batch loss: 0.4527744948863983 | avg loss: 0.5158326830956843\n",
      "batch loss: 0.4043354392051697 | avg loss: 0.5153520915271907\n",
      "batch loss: 0.5059744715690613 | avg loss: 0.5153118442312331\n",
      "batch loss: 0.4864824712276459 | avg loss: 0.5151886417824998\n",
      "batch loss: 0.44975417852401733 | avg loss: 0.5149101972579956\n",
      "batch loss: 0.35353949666023254 | avg loss: 0.5142264231029203\n",
      "batch loss: 0.17185744643211365 | avg loss: 0.5127818282646469\n",
      "batch loss: 0.37257397174835205 | avg loss: 0.5121927196238222\n",
      "batch loss: 0.4001818895339966 | avg loss: 0.5117240550627769\n",
      "batch loss: 0.5271360278129578 | avg loss: 0.5117882716159026\n",
      "batch loss: 0.6130058765411377 | avg loss: 0.5122082616778331\n",
      "batch loss: 0.4350515305995941 | avg loss: 0.5118894322105676\n",
      "batch loss: 0.5882138609886169 | avg loss: 0.5122035245100657\n",
      "batch loss: 0.5397919416427612 | avg loss: 0.5123165917933964\n",
      "batch loss: 0.5349757075309753 | avg loss: 0.5124090779800804\n",
      "batch loss: 0.5149047374725342 | avg loss: 0.5124192229373669\n",
      "batch loss: 0.4681738018989563 | avg loss: 0.512240091678102\n",
      "batch loss: 0.403597354888916 | avg loss: 0.5118020161265328\n",
      "batch loss: 0.24165235459804535 | avg loss: 0.51071707772682\n",
      "batch loss: 0.1532200276851654 | avg loss: 0.5092870895266532\n",
      "batch loss: 0.5312475562095642 | avg loss: 0.5093745814257884\n",
      "batch loss: 0.46485117077827454 | avg loss: 0.5091979012248062\n",
      "batch loss: 0.6045545339584351 | avg loss: 0.5095748049115004\n",
      "batch loss: 0.8936630487442017 | avg loss: 0.5110869633517866\n",
      "batch loss: 0.46023958921432495 | avg loss: 0.5108875618845808\n",
      "batch loss: 0.5893911123275757 | avg loss: 0.5111942163784988\n",
      "batch loss: 0.7888637185096741 | avg loss: 0.5122746424568303\n",
      "batch loss: 0.20205241441726685 | avg loss: 0.5110722307202428\n",
      "batch loss: 0.3130561411380768 | avg loss: 0.5103076898338251\n",
      "batch loss: 0.35782167315483093 | avg loss: 0.5097212051542905\n",
      "batch loss: 0.8669902682304382 | avg loss: 0.5110900521392566\n",
      "batch loss: 0.5606743097305298 | avg loss: 0.5112793050308264\n",
      "batch loss: 0.22538501024246216 | avg loss: 0.5101922544803003\n",
      "batch loss: 0.7911291122436523 | avg loss: 0.5112564092445554\n",
      "batch loss: 0.18423157930374146 | avg loss: 0.5100223532825147\n",
      "batch loss: 0.558045506477356 | avg loss: 0.51020289145242\n",
      "batch loss: 0.8724465370178223 | avg loss: 0.5115596092260732\n",
      "batch loss: 0.16712109744548798 | avg loss: 0.5102743908985338\n",
      "batch loss: 0.28459271788597107 | avg loss: 0.5094354255713495\n",
      "batch loss: 0.24452976882457733 | avg loss: 0.5084542935093244\n",
      "batch loss: 0.5212256908416748 | avg loss: 0.5085014204367501\n",
      "batch loss: 0.36602240800857544 | avg loss: 0.507977600538117\n",
      "batch loss: 0.3754758834838867 | avg loss: 0.5074922462631931\n",
      "batch loss: 0.2530561089515686 | avg loss: 0.5065636472219098\n",
      "batch loss: 0.5739302039146423 | avg loss: 0.5068086165189744\n",
      "batch loss: 0.7445052862167358 | avg loss: 0.5076698363367198\n",
      "batch loss: 0.30827975273132324 | avg loss: 0.5069500165403105\n",
      "batch loss: 0.668927013874054 | avg loss: 0.5075326676098563\n",
      "batch loss: 1.2780654430389404 | avg loss: 0.5102944338300323\n",
      "batch loss: 0.149028941988945 | avg loss: 0.5090041999305998\n",
      "batch loss: 0.6367480754852295 | avg loss: 0.5094588044699401\n",
      "batch loss: 0.4364999830722809 | avg loss: 0.5092000852451257\n",
      "batch loss: 0.3146398067474365 | avg loss: 0.5085125930949572\n",
      "batch loss: 0.6843478679656982 | avg loss: 0.5091317313867556\n",
      "batch loss: 0.540444016456604 | avg loss: 0.5092415990536673\n",
      "batch loss: 0.3489704430103302 | avg loss: 0.5086812103961731\n",
      "batch loss: 0.39670997858047485 | avg loss: 0.5082910667313101\n",
      "batch loss: 0.48964738845825195 | avg loss: 0.5082263317373064\n",
      "batch loss: 0.24456091225147247 | avg loss: 0.5073139946456598\n",
      "batch loss: 0.7064616084098816 | avg loss: 0.5080007105551917\n",
      "batch loss: 0.32972025871276855 | avg loss: 0.5073880629543587\n",
      "batch loss: 0.9140910506248474 | avg loss: 0.508780881405285\n",
      "batch loss: 0.3113182485103607 | avg loss: 0.5081069475046197\n",
      "batch loss: 0.6562671065330505 | avg loss: 0.508610893623764\n",
      "batch loss: 0.35876527428627014 | avg loss: 0.5081029423717726\n",
      "batch loss: 0.5130532383918762 | avg loss: 0.5081196663448134\n",
      "batch loss: 0.38883158564567566 | avg loss: 0.507718022975456\n",
      "batch loss: 0.3311556875705719 | avg loss: 0.5071255319170503\n",
      "batch loss: 0.5506041049957275 | avg loss: 0.507270945204939\n",
      "batch loss: 0.30220088362693787 | avg loss: 0.5065873783330123\n",
      "batch loss: 0.41065481305122375 | avg loss: 0.5062686654915445\n",
      "batch loss: 0.3541702330112457 | avg loss: 0.5057650282979012\n",
      "batch loss: 0.5962778329849243 | avg loss: 0.506063750425581\n",
      "batch loss: 0.6382690072059631 | avg loss: 0.5064986361386744\n",
      "batch loss: 0.5896479487419128 | avg loss: 0.5067712568357342\n",
      "batch loss: 0.5557141304016113 | avg loss: 0.5069312008669953\n",
      "batch loss: 0.5728533267974854 | avg loss: 0.5071459309188862\n",
      "batch loss: 0.4237915575504303 | avg loss: 0.506875299836521\n",
      "batch loss: 0.2826247811317444 | avg loss: 0.506149570002525\n",
      "batch loss: 0.4823853671550751 | avg loss: 0.5060729112836623\n",
      "batch loss: 0.47493842244148254 | avg loss: 0.5059728003870636\n",
      "batch loss: 0.4880859851837158 | avg loss: 0.5059154708511554\n",
      "batch loss: 1.1925276517868042 | avg loss: 0.5081091199915249\n",
      "batch loss: 0.39695119857788086 | avg loss: 0.5077551138723732\n",
      "batch loss: 0.42112576961517334 | avg loss: 0.5074801000810805\n",
      "batch loss: 0.949203610420227 | avg loss: 0.5088779592910145\n",
      "batch loss: 0.22106173634529114 | avg loss: 0.5079700216791984\n",
      "batch loss: 0.635424792766571 | avg loss: 0.5083708228461398\n",
      "batch loss: 0.6133763790130615 | avg loss: 0.5086999938686065\n",
      "batch loss: 0.19556103646755219 | avg loss: 0.5077214346267283\n",
      "batch loss: 0.7214602828025818 | avg loss: 0.5083872877363104\n",
      "batch loss: 0.32579943537712097 | avg loss: 0.5078202447165614\n",
      "batch loss: 0.3950698971748352 | avg loss: 0.5074711724331504\n",
      "batch loss: 0.2936500906944275 | avg loss: 0.5068112308228457\n",
      "batch loss: 0.6541374921798706 | avg loss: 0.507264542396252\n",
      "batch loss: 0.37550097703933716 | avg loss: 0.5068603596804332\n",
      "batch loss: 0.5994853377342224 | avg loss: 0.5071436164940534\n",
      "batch loss: 0.2606745958328247 | avg loss: 0.506392186553013\n",
      "batch loss: 0.8186397552490234 | avg loss: 0.5073412673089279\n",
      "batch loss: 0.5658648014068604 | avg loss: 0.507518611351649\n",
      "batch loss: 0.47064879536628723 | avg loss: 0.5074072221794877\n",
      "batch loss: 0.6369248032569885 | avg loss: 0.5077973353755043\n",
      "batch loss: 0.4371054768562317 | avg loss: 0.5075850475120831\n",
      "batch loss: 0.12604081630706787 | avg loss: 0.5064426995144633\n",
      "batch loss: 0.19032427668571472 | avg loss: 0.5054990624313924\n",
      "batch loss: 0.26462623476982117 | avg loss: 0.504782179015733\n",
      "batch loss: 0.3367326259613037 | avg loss: 0.5042835156535537\n",
      "batch loss: 0.24368837475776672 | avg loss: 0.5035125241124419\n",
      "batch loss: 0.34614816308021545 | avg loss: 0.5030483224574795\n",
      "batch loss: 0.367328405380249 | avg loss: 0.5026491462307818\n",
      "batch loss: 0.39303454756736755 | avg loss: 0.5023276957948187\n",
      "batch loss: 0.41597700119018555 | avg loss: 0.502075208383694\n",
      "batch loss: 0.38915881514549255 | avg loss: 0.5017460060710462\n",
      "batch loss: 0.42089349031448364 | avg loss: 0.501510969688033\n",
      "batch loss: 0.5293656587600708 | avg loss: 0.5015917079172273\n",
      "batch loss: 0.1773948073387146 | avg loss: 0.5006547226554396\n",
      "batch loss: 0.6187000274658203 | avg loss: 0.5009949108537405\n",
      "batch loss: 0.34129631519317627 | avg loss: 0.5005360068432216\n",
      "batch loss: 0.5570936799049377 | avg loss: 0.5006980632130259\n",
      "batch loss: 0.22659358382225037 | avg loss: 0.49991490755762374\n",
      "batch loss: 1.51675546169281 | avg loss: 0.5028118891933365\n",
      "batch loss: 0.4962020218372345 | avg loss: 0.5027931111610748\n",
      "batch loss: 0.3309732675552368 | avg loss: 0.502306369394486\n",
      "batch loss: 0.25961363315582275 | avg loss: 0.5016207966932469\n",
      "batch loss: 0.3276831805706024 | avg loss: 0.5011308315774085\n",
      "batch loss: 0.5872632265090942 | avg loss: 0.5013727765069919\n",
      "batch loss: 0.35095348954200745 | avg loss: 0.5009514339664737\n",
      "batch loss: 0.7161914110183716 | avg loss: 0.5015526629526522\n",
      "batch loss: 1.390130877494812 | avg loss: 0.5040278111825747\n",
      "batch loss: 0.6250353455543518 | avg loss: 0.5043639432224962\n",
      "batch loss: 0.8364837169647217 | avg loss: 0.5052839425957434\n",
      "batch loss: 0.773297905921936 | avg loss: 0.5060243126601804\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:27:27\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.49\n",
      "  Validation took: 0:02:14\n",
      "\n",
      "Training complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.49      0.53       566\n",
      "           2       0.39      0.45      0.42       463\n",
      "           3       0.52      0.53      0.53       418\n",
      "\n",
      "    accuracy                           0.49      1447\n",
      "   macro avg       0.50      0.49      0.49      1447\n",
      "weighted avg       0.50      0.49      0.49      1447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Definimos la funcion para entrenar el modelo y entregar los resultados en el set de validación\n",
    "#Train model\n",
    "def training(n_epochs, training_dataloader, validation_dataloader):\n",
    "    for epoch_i in range(0, n_epochs):\n",
    "        print('======= Epoch {:} / {:} ======='.format(epoch_i + 1, epochs))\n",
    "        # Mira cuanto tiempo le cuesta entrenar un EPOCH.\n",
    "        t0 = time.time()\n",
    "        # Resetea la perdida para este EPOCH.\n",
    "        total_loss = 0\n",
    "        # Pone el modelo en modo entrenamiento.\n",
    "        model.train()\n",
    "        # Para cada batch en el training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Limpia el gradiente calculado anteriormente\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Genera un paso adelante\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # Saca el loss value fuera del output\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Genera un paso atras\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipea el los gradientes a 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Actualiza los parametros\n",
    "            # ¿take a step using the computed gradient?\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calcula el average loss sobre el training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        #Validación\n",
    "        # Despues de completar un entrenamiento genera un paso de validacion\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Pone el modelo en modo evaluación\n",
    "        model.eval()\n",
    "\n",
    "        # Trackea las variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evalua el data para un epoch mas\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # El modelo no computa los gradientes\n",
    "            with torch.no_grad():\n",
    "                # Paso adelante \n",
    "                # Devolvemos los loggits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # Los \"logits\" son el valor de salida\n",
    "            # Prioriza aplicar la funcion de activación\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Mueve los logits y labels a la CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Guarda los logits y labels del batch\n",
    "            # Utilizamos esto en la matriz de confusión\n",
    "            predict_labels = np.argmax(logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calcula la precision para este batch\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "            # Accumula la precisión total\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #Print la matriz de confussión\"\n",
    "    conf = confusion_matrix(all_labels, all_logits, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    print(classification_report(all_labels, all_logits, target_names=target_names))\n",
    "\n",
    "\n",
    "\n",
    "#Llamamos a la funcion para entrenar el modelo\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
