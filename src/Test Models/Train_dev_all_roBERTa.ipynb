{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7234, 2)\n",
      "                                                 review  label\n",
      "0     @dianalaa32 Es una escena de uno de los docume...      2\n",
      "1     Qué feo es tener que terminar con alguien; y m...      0\n",
      "2     Oído en McDonalds \"el mejor mannequin challeng...      0\n",
      "3     Tengo que aceptar que me esta hundiendo el con...      1\n",
      "4     Mmm no quiero hacer spoiler pero hoy va a ver ...      1\n",
      "...                                                 ...    ...\n",
      "7229  @sebatramp Acá también, Seba ???? Para peor el...      0\n",
      "7230  @Phoyu_Agustina no soy hack pero es imposible ...      1\n",
      "7231  Nadie te vende un The Last of Us Remastered po...      0\n",
      "7232  Me propuse dejar las redes, las salidas &amp; ...      1\n",
      "7233  @irenichus siii! Voy como en media hora. Me va...      2\n",
      "\n",
      "[7234 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras \n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "MAX_LEN = 38\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "df = pd.read_csv('/Users/nfanlo/Desktop/Python/DataAnalysis/Datasets TFG/TFG/Task1-train-dev/train_dev/train_dev_all.csv')\n",
    "print(df.shape)\n",
    "df.isnull().sum()\n",
    "df.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df['review'] = df['text']\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "df['label'] = df['sentiment']\n",
    "df.drop('sentiment', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Environment stopwords\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['review']=df['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 5787\n",
      "Validation set length : 1447\n"
     ]
    }
   ],
   "source": [
    "review = df['review']\n",
    "labels = df['label']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(review, labels, stratify=labels, test_size=0.2)\n",
    "\n",
    "# Report datasets lenghts\n",
    "print('Training set length : {}'.format(len(X_train)))\n",
    "print('Validation set length : {}'.format(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 38\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\",\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=MAX_LEN,\n",
    "                   truncation=True ,pad_to_max_length=True,\n",
    "                   return_token_type_ids = False,\n",
    "                   return_attention_mask = True,)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_val_inputs, X_val_masks = preprocessing(X_val)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "batch_size = 16\n",
    "\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "y_val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks, y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el modelo + optimizador + definimos EPOCHS + Scheduler\n",
    "#Modelo\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3,\n",
    " output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 4e-5, eps = 1e-6)\n",
    "\n",
    "epochs=5\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion para formatear el tiempo y otra para calcular la exactitud\n",
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Training--------------------\n",
      "\n",
      "======= Epoch 1 / 5 =======\n",
      "batch loss: 1.0910661220550537 | avg loss: 1.0910661220550537\n",
      "batch loss: 1.0742703676223755 | avg loss: 1.0826682448387146\n",
      "batch loss: 1.0694100856781006 | avg loss: 1.0782488584518433\n",
      "batch loss: 1.1400320529937744 | avg loss: 1.093694657087326\n",
      "batch loss: 1.1626758575439453 | avg loss: 1.10749089717865\n",
      "batch loss: 1.1689671277999878 | avg loss: 1.1177369356155396\n",
      "batch loss: 1.0198320150375366 | avg loss: 1.1037505183901106\n",
      "batch loss: 1.1825615167617798 | avg loss: 1.1136018931865692\n",
      "batch loss: 1.1611988544464111 | avg loss: 1.1188904444376628\n",
      "batch loss: 1.1087017059326172 | avg loss: 1.1178715705871582\n",
      "batch loss: 1.0447044372558594 | avg loss: 1.1112200130115857\n",
      "batch loss: 1.1058145761489868 | avg loss: 1.1107695599397023\n",
      "batch loss: 1.081311821937561 | avg loss: 1.1085035800933838\n",
      "batch loss: 1.0940487384796143 | avg loss: 1.1074710914066859\n",
      "batch loss: 1.092970609664917 | avg loss: 1.1065043926239013\n",
      "batch loss: 1.166992425918579 | avg loss: 1.1102848947048187\n",
      "batch loss: 1.066324234008789 | avg loss: 1.1076989734874052\n",
      "batch loss: 1.0644571781158447 | avg loss: 1.1052966515223186\n",
      "batch loss: 1.0932027101516724 | avg loss: 1.1046601282922845\n",
      "batch loss: 1.0993995666503906 | avg loss: 1.1043971002101898\n",
      "batch loss: 1.1629310846328735 | avg loss: 1.1071844328017462\n",
      "batch loss: 1.0842965841293335 | avg loss: 1.1061440760439092\n",
      "batch loss: 1.0656434297561646 | avg loss: 1.1043831783792246\n",
      "batch loss: 1.1025762557983398 | avg loss: 1.1043078899383545\n",
      "batch loss: 1.1841859817504883 | avg loss: 1.10750301361084\n",
      "batch loss: 1.07766592502594 | avg loss: 1.1063554332806513\n",
      "batch loss: 1.104210376739502 | avg loss: 1.1062759867420904\n",
      "batch loss: 1.0797048807144165 | avg loss: 1.1053270186696733\n",
      "batch loss: 1.1068769693374634 | avg loss: 1.1053804652444248\n",
      "batch loss: 1.1098885536193848 | avg loss: 1.1055307348569234\n",
      "batch loss: 1.0669517517089844 | avg loss: 1.1042862515295706\n",
      "batch loss: 1.144601821899414 | avg loss: 1.1055461131036282\n",
      "batch loss: 1.1448183059692383 | avg loss: 1.106736179554101\n",
      "batch loss: 1.1230179071426392 | avg loss: 1.1072150538949406\n",
      "batch loss: 1.0976237058639526 | avg loss: 1.1069410153797694\n",
      "batch loss: 1.1190471649169922 | avg loss: 1.107277297311359\n",
      "batch loss: 1.0728007555007935 | avg loss: 1.1063454988840464\n",
      "batch loss: 1.090919017791748 | avg loss: 1.1059395388553017\n",
      "batch loss: 1.1362886428833008 | avg loss: 1.1067177210098658\n",
      "batch loss: 1.126958966255188 | avg loss: 1.1072237521409989\n",
      "batch loss: 1.1248263120651245 | avg loss: 1.1076530828708555\n",
      "batch loss: 1.1253859996795654 | avg loss: 1.1080752951758248\n",
      "batch loss: 1.102853536605835 | avg loss: 1.107953858930011\n",
      "batch loss: 1.0809884071350098 | avg loss: 1.107341007752852\n",
      "batch loss: 1.1033257246017456 | avg loss: 1.107251779238383\n",
      "batch loss: 1.107295036315918 | avg loss: 1.1072527196096338\n",
      "batch loss: 1.0872228145599365 | avg loss: 1.106826551417087\n",
      "batch loss: 1.1199800968170166 | avg loss: 1.1071005836129189\n",
      "batch loss: 1.0561825037002563 | avg loss: 1.1060614391249053\n",
      "batch loss: 1.0918101072311401 | avg loss: 1.10577641248703\n",
      "batch loss: 1.0994327068328857 | avg loss: 1.1056520261016547\n",
      "batch loss: 1.0228263139724731 | avg loss: 1.1040592239453242\n",
      "batch loss: 1.0792782306671143 | avg loss: 1.1035916580344147\n",
      "batch loss: 1.1132994890213013 | avg loss: 1.10377143268232\n",
      "batch loss: 1.1996426582336426 | avg loss: 1.1055145458741622\n",
      "batch loss: 1.1094683408737183 | avg loss: 1.1055851493562971\n",
      "batch loss: 1.1402274370193481 | avg loss: 1.1061929087889821\n",
      "batch loss: 1.124063491821289 | avg loss: 1.1065010222895393\n",
      "batch loss: 1.0193499326705933 | avg loss: 1.1050238851773537\n",
      "batch loss: 1.0791913270950317 | avg loss: 1.1045933425426484\n",
      "batch loss: 1.1312737464904785 | avg loss: 1.1050307262139243\n",
      "batch loss: 1.1311100721359253 | avg loss: 1.1054513608255694\n",
      "batch loss: 1.1210490465164185 | avg loss: 1.1056989431381226\n",
      "batch loss: 1.0470341444015503 | avg loss: 1.1047823056578636\n",
      "batch loss: 1.127105712890625 | avg loss: 1.1051257426922139\n",
      "batch loss: 1.1225942373275757 | avg loss: 1.1053904168533557\n",
      "batch loss: 1.0890734195709229 | avg loss: 1.1051468795804835\n",
      "batch loss: 1.0584276914596558 | avg loss: 1.1044598326963537\n",
      "batch loss: 1.1401128768920898 | avg loss: 1.104976543481799\n",
      "batch loss: 1.1123520135879517 | avg loss: 1.1050819073404585\n",
      "batch loss: 1.1062123775482178 | avg loss: 1.1050978294560607\n",
      "batch loss: 1.1062077283859253 | avg loss: 1.1051132447189755\n",
      "batch loss: 1.098027229309082 | avg loss: 1.1050161760147303\n",
      "batch loss: 1.0922454595565796 | avg loss: 1.1048435987652958\n",
      "batch loss: 1.1337133646011353 | avg loss: 1.1052285289764405\n",
      "batch loss: 1.089428424835205 | avg loss: 1.1050206328693188\n",
      "batch loss: 1.0953272581100464 | avg loss: 1.1048947448854323\n",
      "batch loss: 1.098527431488037 | avg loss: 1.1048131126623888\n",
      "batch loss: 1.0836163759231567 | avg loss: 1.1045447995391073\n",
      "batch loss: 1.1100990772247314 | avg loss: 1.1046142280101776\n",
      "batch loss: 1.104104995727539 | avg loss: 1.1046079411918734\n",
      "batch loss: 1.1672977209091187 | avg loss: 1.1053724507006204\n",
      "batch loss: 1.0723882913589478 | avg loss: 1.1049750511904797\n",
      "batch loss: 1.0714366436004639 | avg loss: 1.1045757844334556\n",
      "batch loss: 1.0835587978363037 | avg loss: 1.1043285257676068\n",
      "batch loss: 1.1012287139892578 | avg loss: 1.1042924814446027\n",
      "batch loss: 1.092362403869629 | avg loss: 1.1041553541161548\n",
      "batch loss: 1.159364104270935 | avg loss: 1.1047827262770047\n",
      "batch loss: 1.11319899559021 | avg loss: 1.1048772911007485\n",
      "batch loss: 1.0740700960159302 | avg loss: 1.1045349889331393\n",
      "batch loss: 1.1211903095245361 | avg loss: 1.1047180144341437\n",
      "batch loss: 1.0543057918548584 | avg loss: 1.1041700554930645\n",
      "batch loss: 1.0993874073028564 | avg loss: 1.1041186291684386\n",
      "batch loss: 1.0980627536773682 | avg loss: 1.1040542049610869\n",
      "batch loss: 1.0867605209350586 | avg loss: 1.1038721661818656\n",
      "batch loss: 1.0920125246047974 | avg loss: 1.103748628248771\n",
      "batch loss: 1.0774202346801758 | avg loss: 1.1034772015109504\n",
      "batch loss: 1.1491683721542358 | avg loss: 1.103943437946086\n",
      "batch loss: 1.0674443244934082 | avg loss: 1.1035747600324226\n",
      "batch loss: 1.106953740119934 | avg loss: 1.1036085498332977\n",
      "batch loss: 1.1608872413635254 | avg loss: 1.1041756655910226\n",
      "batch loss: 1.0462812185287476 | avg loss: 1.103608072972765\n",
      "batch loss: 1.0079220533370972 | avg loss: 1.1026790824908654\n",
      "batch loss: 1.1842169761657715 | avg loss: 1.103463100699278\n",
      "batch loss: 1.0874933004379272 | avg loss: 1.1033110073634556\n",
      "batch loss: 1.1486940383911133 | avg loss: 1.1037391491656035\n",
      "batch loss: 1.0961021184921265 | avg loss: 1.1036677750471597\n",
      "batch loss: 1.0784317255020142 | avg loss: 1.1034341079217416\n",
      "batch loss: 1.046701192855835 | avg loss: 1.1029136224624214\n",
      "batch loss: 1.0673013925552368 | avg loss: 1.1025898749178107\n",
      "batch loss: 1.07527494430542 | avg loss: 1.102343794461843\n",
      "batch loss: 1.1210212707519531 | avg loss: 1.1025105576430048\n",
      "batch loss: 1.1251306533813477 | avg loss: 1.1027107354813972\n",
      "batch loss: 1.1342542171478271 | avg loss: 1.1029874326889975\n",
      "batch loss: 1.1060833930969238 | avg loss: 1.103014354083849\n",
      "batch loss: 1.0133800506591797 | avg loss: 1.1022416445715675\n",
      "batch loss: 1.0802186727523804 | avg loss: 1.1020534140431983\n",
      "batch loss: 1.0977898836135864 | avg loss: 1.1020172824293881\n",
      "batch loss: 1.061089277267456 | avg loss: 1.1016733496129012\n",
      "batch loss: 1.057068943977356 | avg loss: 1.101301646232605\n",
      "batch loss: 1.1206623315811157 | avg loss: 1.1014616518966422\n",
      "batch loss: 1.0845999717712402 | avg loss: 1.101323441403811\n",
      "batch loss: 1.0700997114181519 | avg loss: 1.101069589940513\n",
      "batch loss: 1.131534218788147 | avg loss: 1.1013152724312198\n",
      "batch loss: 1.0303593873977661 | avg loss: 1.100747625350952\n",
      "batch loss: 1.1092673540115356 | avg loss: 1.1008152422450839\n",
      "batch loss: 1.1751070022583008 | avg loss: 1.1014002167333767\n",
      "batch loss: 1.0834757089614868 | avg loss: 1.101260181516409\n",
      "batch loss: 1.1116862297058105 | avg loss: 1.1013410035953966\n",
      "batch loss: 1.0432145595550537 | avg loss: 1.1008938771027785\n",
      "batch loss: 1.1049798727035522 | avg loss: 1.1009250679088913\n",
      "batch loss: 1.0529115200042725 | avg loss: 1.100561328909614\n",
      "batch loss: 1.1162714958190918 | avg loss: 1.1006794504653243\n",
      "batch loss: 1.0743697881698608 | avg loss: 1.1004831097019252\n",
      "batch loss: 1.1095436811447144 | avg loss: 1.100550225045946\n",
      "batch loss: 1.0715060234069824 | avg loss: 1.100336664739777\n",
      "batch loss: 1.0915888547897339 | avg loss: 1.1002728121124046\n",
      "batch loss: 1.072092890739441 | avg loss: 1.100068609783615\n",
      "batch loss: 1.0863702297210693 | avg loss: 1.0999700602867621\n",
      "batch loss: 1.0883066654205322 | avg loss: 1.0998867503234317\n",
      "batch loss: 1.1203476190567017 | avg loss: 1.1000318628676393\n",
      "batch loss: 1.1066585779190063 | avg loss: 1.1000785298750435\n",
      "batch loss: 1.1035053730010986 | avg loss: 1.1001024938129877\n",
      "batch loss: 1.0925805568695068 | avg loss: 1.1000502581397693\n",
      "batch loss: 1.0688308477401733 | avg loss: 1.0998349518611514\n",
      "batch loss: 1.1343331336975098 | avg loss: 1.1000712407778388\n",
      "batch loss: 1.0621366500854492 | avg loss: 1.0998131823377544\n",
      "batch loss: 1.1184978485107422 | avg loss: 1.0999394300821665\n",
      "batch loss: 1.0983086824417114 | avg loss: 1.0999284854671298\n",
      "batch loss: 1.0908185243606567 | avg loss: 1.0998677523930867\n",
      "batch loss: 1.0728118419647217 | avg loss: 1.0996885741783293\n",
      "batch loss: 1.102847933769226 | avg loss: 1.0997093594387959\n",
      "batch loss: 1.1471338272094727 | avg loss: 1.1000193232804343\n",
      "batch loss: 1.145020842552185 | avg loss: 1.100311540938043\n",
      "batch loss: 1.1033726930618286 | avg loss: 1.1003312903065836\n",
      "batch loss: 1.1088368892669678 | avg loss: 1.1003858133768425\n",
      "batch loss: 1.0873972177505493 | avg loss: 1.1003030834683947\n",
      "batch loss: 1.1177200078964233 | avg loss: 1.1004133171673063\n",
      "batch loss: 1.103717565536499 | avg loss: 1.1004340986035905\n",
      "batch loss: 1.114364743232727 | avg loss: 1.1005211651325226\n",
      "batch loss: 1.1038116216659546 | avg loss: 1.1005416027507426\n",
      "batch loss: 1.1051154136657715 | avg loss: 1.1005698361514527\n",
      "batch loss: 1.12407386302948 | avg loss: 1.100714032635367\n",
      "batch loss: 1.1204122304916382 | avg loss: 1.1008341435979052\n",
      "batch loss: 1.0487957000732422 | avg loss: 1.1005187590916952\n",
      "batch loss: 1.112290382385254 | avg loss: 1.10058967248503\n",
      "batch loss: 1.0806488990783691 | avg loss: 1.1004702666562474\n",
      "batch loss: 1.1243269443511963 | avg loss: 1.100612270690146\n",
      "batch loss: 1.106865644454956 | avg loss: 1.100649272901772\n",
      "batch loss: 1.0717869997024536 | avg loss: 1.100479494824129\n",
      "batch loss: 1.0521085262298584 | avg loss: 1.100196623662759\n",
      "batch loss: 1.0824811458587646 | avg loss: 1.1000936266987822\n",
      "batch loss: 1.1342250108718872 | avg loss: 1.100290917936777\n",
      "batch loss: 1.162243366241455 | avg loss: 1.1006469664902523\n",
      "batch loss: 1.178027868270874 | avg loss: 1.101089143071856\n",
      "batch loss: 1.1666462421417236 | avg loss: 1.1014616265892982\n",
      "batch loss: 1.0965352058410645 | avg loss: 1.101433793703715\n",
      "batch loss: 1.1197999715805054 | avg loss: 1.1015369744783037\n",
      "batch loss: 1.1084582805633545 | avg loss: 1.1015756409927453\n",
      "batch loss: 1.1140445470809937 | avg loss: 1.1016449126932355\n",
      "batch loss: 1.0987145900726318 | avg loss: 1.1016287230654975\n",
      "batch loss: 1.104305386543274 | avg loss: 1.101643430007683\n",
      "batch loss: 1.0783156156539917 | avg loss: 1.1015159556123078\n",
      "batch loss: 1.0783041715621948 | avg loss: 1.1013898046120354\n",
      "batch loss: 1.1234490871429443 | avg loss: 1.1015090439770672\n",
      "batch loss: 1.1263543367385864 | avg loss: 1.101642620819871\n",
      "batch loss: 1.1080533266067505 | avg loss: 1.1016769026689988\n",
      "batch loss: 1.1043766736984253 | avg loss: 1.1016912631531979\n",
      "batch loss: 1.1175283193588257 | avg loss: 1.1017750571013758\n",
      "batch loss: 1.07477867603302 | avg loss: 1.1016329708852266\n",
      "batch loss: 1.095810055732727 | avg loss: 1.1016024844184595\n",
      "batch loss: 1.1253069639205933 | avg loss: 1.1017259452491999\n",
      "batch loss: 1.067158579826355 | avg loss: 1.1015468397288743\n",
      "batch loss: 1.085180640220642 | avg loss: 1.101462477875739\n",
      "batch loss: 1.1093909740447998 | avg loss: 1.1015031368304522\n",
      "batch loss: 1.1090788841247559 | avg loss: 1.101541788602362\n",
      "batch loss: 1.1222481727600098 | avg loss: 1.1016468971513855\n",
      "batch loss: 1.1045230627059937 | avg loss: 1.101661423240045\n",
      "batch loss: 1.096213936805725 | avg loss: 1.101634048936355\n",
      "batch loss: 1.127206802368164 | avg loss: 1.101761912703514\n",
      "batch loss: 1.058622121810913 | avg loss: 1.1015472868781777\n",
      "batch loss: 1.0955026149749756 | avg loss: 1.1015173627598451\n",
      "batch loss: 1.0085867643356323 | avg loss: 1.101059576560711\n",
      "batch loss: 1.0441960096359253 | avg loss: 1.1007808335855895\n",
      "batch loss: 1.0518401861190796 | avg loss: 1.1005420987198993\n",
      "batch loss: 1.0576367378234863 | avg loss: 1.1003338202689459\n",
      "batch loss: 1.0927071571350098 | avg loss: 1.10029697648569\n",
      "batch loss: 1.0896776914596558 | avg loss: 1.1002459222307572\n",
      "batch loss: 1.1301666498184204 | avg loss: 1.1003890836067747\n",
      "batch loss: 1.107932686805725 | avg loss: 1.1004250055267697\n",
      "batch loss: 1.1270766258239746 | avg loss: 1.1005513165234388\n",
      "batch loss: 1.020240306854248 | avg loss: 1.1001724910061315\n",
      "batch loss: 1.012522578239441 | avg loss: 1.0997609890682596\n",
      "batch loss: 0.9744380712509155 | avg loss: 1.0991753679569636\n",
      "batch loss: 1.1461937427520752 | avg loss: 1.0993940580722898\n",
      "batch loss: 1.139979362487793 | avg loss: 1.0995819530001394\n",
      "batch loss: 1.008965015411377 | avg loss: 1.0991643634259975\n",
      "batch loss: 1.2308589220046997 | avg loss: 1.0997684669057164\n",
      "batch loss: 1.0270862579345703 | avg loss: 1.0994365846729715\n",
      "batch loss: 1.093010663986206 | avg loss: 1.099407375942577\n",
      "batch loss: 1.055723786354065 | avg loss: 1.0992097126412714\n",
      "batch loss: 1.077217936515808 | avg loss: 1.0991106505866524\n",
      "batch loss: 1.1140897274017334 | avg loss: 1.099177821334702\n",
      "batch loss: 1.0976357460021973 | avg loss: 1.0991709370698248\n",
      "batch loss: 1.1022100448608398 | avg loss: 1.0991844442155627\n",
      "batch loss: 1.0358165502548218 | avg loss: 1.098904055304232\n",
      "batch loss: 1.075605034828186 | avg loss: 1.0988014164475093\n",
      "batch loss: 1.0148745775222778 | avg loss: 1.0984333162767845\n",
      "batch loss: 1.076913833618164 | avg loss: 1.0983393447367906\n",
      "batch loss: 1.2118076086044312 | avg loss: 1.0988326850144758\n",
      "batch loss: 1.0205382108688354 | avg loss: 1.0984937478969623\n",
      "batch loss: 1.1438974142074585 | avg loss: 1.0986894533551972\n",
      "batch loss: 1.0622116327285767 | avg loss: 1.098532896185126\n",
      "batch loss: 1.133304238319397 | avg loss: 1.0986814916643322\n",
      "batch loss: 1.0915718078613281 | avg loss: 1.0986512376907025\n",
      "batch loss: 1.1111890077590942 | avg loss: 1.09870436383506\n",
      "batch loss: 1.0962811708450317 | avg loss: 1.0986941393920642\n",
      "batch loss: 1.1016734838485718 | avg loss: 1.098706657646083\n",
      "batch loss: 1.1057229042053223 | avg loss: 1.0987360143262472\n",
      "batch loss: 1.1987303495407104 | avg loss: 1.0991526573896409\n",
      "batch loss: 1.029815912246704 | avg loss: 1.0988649530529482\n",
      "batch loss: 1.022921085357666 | avg loss: 1.0985511354178437\n",
      "batch loss: 1.0580374002456665 | avg loss: 1.0983844122278348\n",
      "batch loss: 1.1167300939559937 | avg loss: 1.0984595994480322\n",
      "batch loss: 1.165353775024414 | avg loss: 1.0987326368993642\n",
      "batch loss: 1.0875232219696045 | avg loss: 1.0986870701720075\n",
      "batch loss: 1.0574151277542114 | avg loss: 1.098519977287725\n",
      "batch loss: 1.036647915840149 | avg loss: 1.0982704931689846\n",
      "batch loss: 1.1258398294448853 | avg loss: 1.0983812133949924\n",
      "batch loss: 1.131144642829895 | avg loss: 1.0985122671127319\n",
      "batch loss: 1.1254584789276123 | avg loss: 1.0986196225382892\n",
      "batch loss: 1.0669444799423218 | avg loss: 1.0984939275279877\n",
      "batch loss: 1.101337194442749 | avg loss: 1.098505165737137\n",
      "batch loss: 1.1217371225357056 | avg loss: 1.0985966301339818\n",
      "batch loss: 1.1369717121124268 | avg loss: 1.0987471206515442\n",
      "batch loss: 1.1078075170516968 | avg loss: 1.0987825128249824\n",
      "batch loss: 1.1143099069595337 | avg loss: 1.0988429307009924\n",
      "batch loss: 1.1321312189102173 | avg loss: 1.0989719550738963\n",
      "batch loss: 1.1646748781204224 | avg loss: 1.099225634313458\n",
      "batch loss: 1.1367112398147583 | avg loss: 1.0993698097192324\n",
      "batch loss: 1.0849922895431519 | avg loss: 1.099314723435033\n",
      "batch loss: 1.0755549669265747 | avg loss: 1.0992240373414892\n",
      "batch loss: 1.1000057458877563 | avg loss: 1.0992270096173304\n",
      "batch loss: 1.0919675827026367 | avg loss: 1.0991995117881082\n",
      "batch loss: 1.125126600265503 | avg loss: 1.0992973498578342\n",
      "batch loss: 1.127423644065857 | avg loss: 1.0994030878059846\n",
      "batch loss: 1.0905524492263794 | avg loss: 1.099369939346885\n",
      "batch loss: 1.0989041328430176 | avg loss: 1.0993682012629153\n",
      "batch loss: 1.1091675758361816 | avg loss: 1.0994046301646747\n",
      "batch loss: 1.0852854251861572 | avg loss: 1.0993523368129023\n",
      "batch loss: 1.0981603860855103 | avg loss: 1.099347938470735\n",
      "batch loss: 1.0793758630752563 | avg loss: 1.0992745117229574\n",
      "batch loss: 1.0780390501022339 | avg loss: 1.099196726149255\n",
      "batch loss: 1.0987873077392578 | avg loss: 1.0991952319214815\n",
      "batch loss: 1.127916932106018 | avg loss: 1.099299674467607\n",
      "batch loss: 1.1091601848602295 | avg loss: 1.0993354009545369\n",
      "batch loss: 1.120740532875061 | avg loss: 1.0994126757990152\n",
      "batch loss: 1.1206862926483154 | avg loss: 1.0994891996006313\n",
      "batch loss: 1.1279507875442505 | avg loss: 1.0995912124606444\n",
      "batch loss: 1.0522760152816772 | avg loss: 1.0994222296135767\n",
      "batch loss: 1.1424261331558228 | avg loss: 1.099575268416218\n",
      "batch loss: 1.0986216068267822 | avg loss: 1.0995718866375321\n",
      "batch loss: 1.1069459915161133 | avg loss: 1.0995979435452303\n",
      "batch loss: 1.0971599817276 | avg loss: 1.099589359172633\n",
      "batch loss: 1.0925618410110474 | avg loss: 1.0995647012141714\n",
      "batch loss: 1.095842957496643 | avg loss: 1.09955168812425\n",
      "batch loss: 1.1020557880401611 | avg loss: 1.0995604132110648\n",
      "batch loss: 1.0975306034088135 | avg loss: 1.0995533652603626\n",
      "batch loss: 1.0771238803863525 | avg loss: 1.099475754586058\n",
      "batch loss: 1.1054569482803345 | avg loss: 1.0994963793919004\n",
      "batch loss: 1.0920716524124146 | avg loss: 1.0994708648661977\n",
      "batch loss: 1.1115741729736328 | avg loss: 1.0995123145514971\n",
      "batch loss: 1.1156316995620728 | avg loss: 1.0995673295174035\n",
      "batch loss: 1.1090260744094849 | avg loss: 1.0995995021190772\n",
      "batch loss: 1.0703352689743042 | avg loss: 1.099500301328756\n",
      "batch loss: 1.1177610158920288 | avg loss: 1.0995619929320104\n",
      "batch loss: 1.0712517499923706 | avg loss: 1.0994666722487119\n",
      "batch loss: 1.1151432991027832 | avg loss: 1.0995192783790946\n",
      "batch loss: 1.1271870136260986 | avg loss: 1.09961181261069\n",
      "batch loss: 1.0998860597610474 | avg loss: 1.099612726767858\n",
      "batch loss: 1.0683422088623047 | avg loss: 1.099508838004052\n",
      "batch loss: 1.0930217504501343 | avg loss: 1.0994873575816881\n",
      "batch loss: 1.0866429805755615 | avg loss: 1.0994449668984996\n",
      "batch loss: 1.0818371772766113 | avg loss: 1.0993870465379012\n",
      "batch loss: 1.0992810726165771 | avg loss: 1.0993866990824215\n",
      "batch loss: 1.0698219537734985 | avg loss: 1.0992900822676865\n",
      "batch loss: 1.0712953805923462 | avg loss: 1.0991988943143465\n",
      "batch loss: 1.0968624353408813 | avg loss: 1.0991913084085885\n",
      "batch loss: 1.1142709255218506 | avg loss: 1.0992401097584696\n",
      "batch loss: 1.125293493270874 | avg loss: 1.0993241529310904\n",
      "batch loss: 1.0543224811553955 | avg loss: 1.0991794530218437\n",
      "batch loss: 1.1159334182739258 | avg loss: 1.0992331516284208\n",
      "batch loss: 1.0205096006393433 | avg loss: 1.0989816386859639\n",
      "batch loss: 1.051386833190918 | avg loss: 1.0988300628722854\n",
      "batch loss: 1.0045948028564453 | avg loss: 1.0985309033166795\n",
      "batch loss: 1.1122666597366333 | avg loss: 1.0985743709002869\n",
      "batch loss: 1.0715248584747314 | avg loss: 1.0984890412080928\n",
      "batch loss: 1.1447855234146118 | avg loss: 1.0986346276301258\n",
      "batch loss: 0.9810448884963989 | avg loss: 1.0982660077582331\n",
      "batch loss: 1.059455156326294 | avg loss: 1.0981447238475084\n",
      "batch loss: 1.1066718101501465 | avg loss: 1.0981712879792924\n",
      "batch loss: 1.0742273330688477 | avg loss: 1.0980969278708748\n",
      "batch loss: 1.2126529216766357 | avg loss: 1.0984515903903973\n",
      "batch loss: 1.0675925016403198 | avg loss: 1.0983563462893169\n",
      "batch loss: 1.0643545389175415 | avg loss: 1.0982517253435575\n",
      "batch loss: 1.093435525894165 | avg loss: 1.0982369517256145\n",
      "batch loss: 1.1147403717041016 | avg loss: 1.0982874208998608\n",
      "batch loss: 1.1234397888183594 | avg loss: 1.0983641049483928\n",
      "batch loss: 1.1062541007995605 | avg loss: 1.0983880866987001\n",
      "batch loss: 1.1300430297851562 | avg loss: 1.098484010768659\n",
      "batch loss: 1.0904279947280884 | avg loss: 1.0984596723516182\n",
      "batch loss: 1.0407326221466064 | avg loss: 1.0982857956943741\n",
      "batch loss: 1.0813992023468018 | avg loss: 1.098235085203841\n",
      "batch loss: 1.0102730989456177 | avg loss: 1.0979717259635469\n",
      "batch loss: 1.1360998153686523 | avg loss: 1.0980855411558008\n",
      "batch loss: 1.0949653387069702 | avg loss: 1.0980762548389889\n",
      "batch loss: 1.0264538526535034 | avg loss: 1.0978637254556491\n",
      "batch loss: 1.0241392850875854 | avg loss: 1.0976456058095898\n",
      "batch loss: 1.1051666736602783 | avg loss: 1.0976677918504474\n",
      "batch loss: 1.0828475952148438 | avg loss: 1.097624203036813\n",
      "batch loss: 1.0492688417434692 | avg loss: 1.0974823984582402\n",
      "batch loss: 1.1725412607192993 | avg loss: 1.0977018688157287\n",
      "batch loss: 1.1295814514160156 | avg loss: 1.097794812205234\n",
      "batch loss: 1.117504358291626 | avg loss: 1.0978521073973455\n",
      "batch loss: 1.1439869403839111 | avg loss: 1.0979858315509299\n",
      "batch loss: 1.115828514099121 | avg loss: 1.098037399997601\n",
      "batch loss: 1.127913236618042 | avg loss: 1.0981234975094754\n",
      "batch loss: 1.0853034257888794 | avg loss: 1.098086658222922\n",
      "batch loss: 1.1148124933242798 | avg loss: 1.0981345832518656\n",
      "batch loss: 1.046533465385437 | avg loss: 1.097987151486533\n",
      "batch loss: 1.070644736289978 | avg loss: 1.0979092528677394\n",
      "batch loss: 1.058295488357544 | avg loss: 1.0977967137640172\n",
      "batch loss: 1.1144975423812866 | avg loss: 1.0978440248932446\n",
      "batch loss: 1.105000376701355 | avg loss: 1.0978642405763184\n",
      "batch loss: 1.098970890045166 | avg loss: 1.0978673578987659\n",
      "batch loss: 1.1036200523376465 | avg loss: 1.0978835171528076\n",
      "batch loss: 1.0877504348754883 | avg loss: 1.0978551331688375\n",
      "batch loss: 1.1074800491333008 | avg loss: 1.0978820184089617\n",
      "batch loss: 1.0893754959106445 | avg loss: 1.09785832336022\n",
      "batch loss: 1.1276394128799438 | avg loss: 1.0979410486088859\n",
      "batch loss: 1.1335208415985107 | avg loss: 1.0980396075922367\n",
      "batch loss: 1.175737738609314 | avg loss: 1.0982542433132783\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.39\n",
      "  Validation took: 0:01:58\n",
      "\n",
      "======= Epoch 2 / 5 =======\n",
      "batch loss: 1.07815682888031 | avg loss: 1.07815682888031\n",
      "batch loss: 1.0629150867462158 | avg loss: 1.070535957813263\n",
      "batch loss: 1.0304601192474365 | avg loss: 1.0571773449579875\n",
      "batch loss: 1.080143928527832 | avg loss: 1.0629189908504486\n",
      "batch loss: 1.0900710821151733 | avg loss: 1.0683494091033936\n",
      "batch loss: 1.11100172996521 | avg loss: 1.0754581292470295\n",
      "batch loss: 1.0269361734390259 | avg loss: 1.0685264212744576\n",
      "batch loss: 1.1276476383209229 | avg loss: 1.0759165734052658\n",
      "batch loss: 1.0651956796646118 | avg loss: 1.0747253629896376\n",
      "batch loss: 1.076022982597351 | avg loss: 1.074855124950409\n",
      "batch loss: 1.090229868888855 | avg loss: 1.0762528289448132\n",
      "batch loss: 1.0841494798660278 | avg loss: 1.0769108831882477\n",
      "batch loss: 1.069718837738037 | avg loss: 1.0763576489228468\n",
      "batch loss: 1.124955654144287 | avg loss: 1.0798289350100927\n",
      "batch loss: 1.0908581018447876 | avg loss: 1.0805642127990722\n",
      "batch loss: 1.1906065940856934 | avg loss: 1.087441861629486\n",
      "batch loss: 1.1241329908370972 | avg loss: 1.089600163347581\n",
      "batch loss: 1.0503662824630737 | avg loss: 1.0874205032984416\n",
      "batch loss: 1.03732430934906 | avg loss: 1.084783861511632\n",
      "batch loss: 1.1075019836425781 | avg loss: 1.0859197676181793\n",
      "batch loss: 1.1164027452468872 | avg loss: 1.0873713379814511\n",
      "batch loss: 1.1147106885910034 | avg loss: 1.0886140357364307\n",
      "batch loss: 1.0603901147842407 | avg loss: 1.0873869087385095\n",
      "batch loss: 1.08165442943573 | avg loss: 1.087148055434227\n",
      "batch loss: 1.154728889465332 | avg loss: 1.0898512887954712\n",
      "batch loss: 1.0568912029266357 | avg loss: 1.0885835931851313\n",
      "batch loss: 1.1170343160629272 | avg loss: 1.0896373236620869\n",
      "batch loss: 1.0949203968048096 | avg loss: 1.0898260048457555\n",
      "batch loss: 1.1037359237670898 | avg loss: 1.0903056572223533\n",
      "batch loss: 1.0861057043075562 | avg loss: 1.09016565879186\n",
      "batch loss: 1.054000973701477 | avg loss: 1.0889990560470089\n",
      "batch loss: 1.1128978729248047 | avg loss: 1.08974589407444\n",
      "batch loss: 1.151178002357483 | avg loss: 1.09160747311332\n",
      "batch loss: 1.132852554321289 | avg loss: 1.0928205637370838\n",
      "batch loss: 1.0630122423171997 | avg loss: 1.0919688974108015\n",
      "batch loss: 1.0862903594970703 | avg loss: 1.0918111602465312\n",
      "batch loss: 1.0911169052124023 | avg loss: 1.09179239659696\n",
      "batch loss: 1.0793702602386475 | avg loss: 1.0914654982717413\n",
      "batch loss: 1.0916637182235718 | avg loss: 1.0914705808346088\n",
      "batch loss: 1.0666817426681519 | avg loss: 1.0908508598804474\n",
      "batch loss: 1.0798652172088623 | avg loss: 1.0905829173762625\n",
      "batch loss: 1.0659292936325073 | avg loss: 1.0899959263347445\n",
      "batch loss: 1.0735646486282349 | avg loss: 1.0896138035973837\n",
      "batch loss: 1.0661861896514893 | avg loss: 1.0890813578258862\n",
      "batch loss: 1.0981030464172363 | avg loss: 1.0892818397945827\n",
      "batch loss: 1.1180999279022217 | avg loss: 1.0899083199708357\n",
      "batch loss: 1.0070571899414062 | avg loss: 1.0881455299702096\n",
      "batch loss: 1.1584656238555908 | avg loss: 1.089610531926155\n",
      "batch loss: 0.9814828038215637 | avg loss: 1.08740384359749\n",
      "batch loss: 0.9650864005088806 | avg loss: 1.0849574947357177\n",
      "batch loss: 1.1396527290344238 | avg loss: 1.0860299503102022\n",
      "batch loss: 0.9171473383903503 | avg loss: 1.082782207773282\n",
      "batch loss: 1.0174636840820312 | avg loss: 1.0815497827979754\n",
      "batch loss: 1.1000596284866333 | avg loss: 1.0818925577181358\n",
      "batch loss: 1.2319035530090332 | avg loss: 1.0846200303597884\n",
      "batch loss: 1.111419677734375 | avg loss: 1.0850985954914774\n",
      "batch loss: 0.9438309073448181 | avg loss: 1.0826202149976765\n",
      "batch loss: 1.099477767944336 | avg loss: 1.0829108624622739\n",
      "batch loss: 1.081819772720337 | avg loss: 1.0828923694158004\n",
      "batch loss: 0.9532824754714966 | avg loss: 1.0807322045167287\n",
      "batch loss: 1.0485597848892212 | avg loss: 1.0802047878015237\n",
      "batch loss: 1.142168641090393 | avg loss: 1.0812042047900539\n",
      "batch loss: 1.09133780002594 | avg loss: 1.0813650555080838\n",
      "batch loss: 0.9379900693893433 | avg loss: 1.0791248213499784\n",
      "batch loss: 1.0512049198150635 | avg loss: 1.0786952844032875\n",
      "batch loss: 1.126387357711792 | avg loss: 1.0794178915746284\n",
      "batch loss: 1.0065739154815674 | avg loss: 1.0783306680508513\n",
      "batch loss: 0.9056963920593262 | avg loss: 1.0757919286980349\n",
      "batch loss: 1.12936532497406 | avg loss: 1.0765683547310207\n",
      "batch loss: 0.9332035183906555 | avg loss: 1.074520285640444\n",
      "batch loss: 0.9996626377105713 | avg loss: 1.0734659525710093\n",
      "batch loss: 1.1233752965927124 | avg loss: 1.074159137904644\n",
      "batch loss: 1.164146900177002 | avg loss: 1.0753918469768682\n",
      "batch loss: 1.0988084077835083 | avg loss: 1.0757082869877685\n",
      "batch loss: 1.0074706077575684 | avg loss: 1.0747984512646993\n",
      "batch loss: 1.132091999053955 | avg loss: 1.0755523137356107\n",
      "batch loss: 1.068408727645874 | avg loss: 1.0754595398902893\n",
      "batch loss: 1.1286625862121582 | avg loss: 1.0761416302277491\n",
      "batch loss: 1.1732910871505737 | avg loss: 1.0773713701887975\n",
      "batch loss: 1.1477121114730835 | avg loss: 1.078250629454851\n",
      "batch loss: 1.105828046798706 | avg loss: 1.078591091397368\n",
      "batch loss: 1.1142208576202393 | avg loss: 1.0790256007415493\n",
      "batch loss: 1.0673998594284058 | avg loss: 1.0788855315691017\n",
      "batch loss: 1.0586168766021729 | avg loss: 1.0786442380575907\n",
      "batch loss: 1.0575182437896729 | avg loss: 1.0783956969485564\n",
      "batch loss: 1.059645414352417 | avg loss: 1.0781776704067407\n",
      "batch loss: 1.0907824039459229 | avg loss: 1.078322552401444\n",
      "batch loss: 1.1806375980377197 | avg loss: 1.0794852233745835\n",
      "batch loss: 1.1257482767105103 | avg loss: 1.0800050329626276\n",
      "batch loss: 1.0707522630691528 | avg loss: 1.0799022244082557\n",
      "batch loss: 1.1688868999481201 | avg loss: 1.080880077985617\n",
      "batch loss: 0.9923262000083923 | avg loss: 1.0799175358336905\n",
      "batch loss: 1.0917264223098755 | avg loss: 1.080044513107628\n",
      "batch loss: 1.0265675783157349 | avg loss: 1.079475609546012\n",
      "batch loss: 0.985414445400238 | avg loss: 1.0784854920286882\n",
      "batch loss: 1.0580428838729858 | avg loss: 1.078272548193733\n",
      "batch loss: 1.0612660646438599 | avg loss: 1.0780972236210538\n",
      "batch loss: 1.220864176750183 | avg loss: 1.0795540292652286\n",
      "batch loss: 1.0431095361709595 | avg loss: 1.0791859030723572\n",
      "batch loss: 1.12040376663208 | avg loss: 1.0795980817079545\n",
      "batch loss: 1.1928308010101318 | avg loss: 1.0807191977406492\n",
      "batch loss: 1.047264814376831 | avg loss: 1.0803912135900235\n",
      "batch loss: 0.9211394786834717 | avg loss: 1.0788450802414162\n",
      "batch loss: 1.138214111328125 | avg loss: 1.0794159363095577\n",
      "batch loss: 1.1800130605697632 | avg loss: 1.0803740041596548\n",
      "batch loss: 1.0630251169204712 | avg loss: 1.0802103354121155\n",
      "batch loss: 1.0497088432312012 | avg loss: 1.0799252747375274\n",
      "batch loss: 1.053539514541626 | avg loss: 1.079680962143121\n",
      "batch loss: 1.1324647665023804 | avg loss: 1.0801652172289857\n",
      "batch loss: 1.108405351638794 | avg loss: 1.0804219457236204\n",
      "batch loss: 1.0065827369689941 | avg loss: 1.0797567276267317\n",
      "batch loss: 1.1030040979385376 | avg loss: 1.0799642934330873\n",
      "batch loss: 1.0601857900619507 | avg loss: 1.0797892624298029\n",
      "batch loss: 1.0758728981018066 | avg loss: 1.0797549083567501\n",
      "batch loss: 1.1177289485931396 | avg loss: 1.080085117402284\n",
      "batch loss: 0.9804089069366455 | avg loss: 1.079225839725856\n",
      "batch loss: 1.0330355167388916 | avg loss: 1.0788310506404974\n",
      "batch loss: 0.9941028952598572 | avg loss: 1.0781130154254073\n",
      "batch loss: 1.0198211669921875 | avg loss: 1.0776231679595818\n",
      "batch loss: 1.102301836013794 | avg loss: 1.0778288235267004\n",
      "batch loss: 1.0941108465194702 | avg loss: 1.0779633857001942\n",
      "batch loss: 0.9829344153404236 | avg loss: 1.0771844597136389\n",
      "batch loss: 1.0367820262908936 | avg loss: 1.0768559846451613\n",
      "batch loss: 1.1860371828079224 | avg loss: 1.0777364781787317\n",
      "batch loss: 1.0291557312011719 | avg loss: 1.0773478322029113\n",
      "batch loss: 1.141815185546875 | avg loss: 1.0778594778643713\n",
      "batch loss: 1.2004339694976807 | avg loss: 1.078824631341799\n",
      "batch loss: 1.052289366722107 | avg loss: 1.0786173245869577\n",
      "batch loss: 1.0360711812973022 | avg loss: 1.0782875095226967\n",
      "batch loss: 0.9837982058525085 | avg loss: 1.0775606687252337\n",
      "batch loss: 0.899587869644165 | avg loss: 1.076202097739882\n",
      "batch loss: 1.0131598711013794 | avg loss: 1.0757245051138329\n",
      "batch loss: 1.00749933719635 | avg loss: 1.0752115339264834\n",
      "batch loss: 1.1688542366027832 | avg loss: 1.0759103600658588\n",
      "batch loss: 1.0805716514587402 | avg loss: 1.0759448881502505\n",
      "batch loss: 1.0617464780807495 | avg loss: 1.07584048807621\n",
      "batch loss: 0.9231136441230774 | avg loss: 1.0747256935948002\n",
      "batch loss: 1.067204236984253 | avg loss: 1.0746711902860282\n",
      "batch loss: 1.0402089357376099 | avg loss: 1.0744232603971906\n",
      "batch loss: 1.1796561479568481 | avg loss: 1.0751749238797597\n",
      "batch loss: 1.0859832763671875 | avg loss: 1.07525157886194\n",
      "batch loss: 1.0629712343215942 | avg loss: 1.0751650975623601\n",
      "batch loss: 1.0460562705993652 | avg loss: 1.0749615393318497\n",
      "batch loss: 0.9799689650535583 | avg loss: 1.0743018686771393\n",
      "batch loss: 1.134951114654541 | avg loss: 1.0747201393390524\n",
      "batch loss: 1.0833812952041626 | avg loss: 1.0747794623244298\n",
      "batch loss: 0.9634741544723511 | avg loss: 1.0740222833594497\n",
      "batch loss: 1.1875592470169067 | avg loss: 1.074789425005784\n",
      "batch loss: 1.1232880353927612 | avg loss: 1.0751149190352267\n",
      "batch loss: 1.1513278484344482 | avg loss: 1.0756230052312215\n",
      "batch loss: 1.087604284286499 | avg loss: 1.0757023514501307\n",
      "batch loss: 1.0036286115646362 | avg loss: 1.0752281821087788\n",
      "batch loss: 1.125566840171814 | avg loss: 1.075557192945792\n",
      "batch loss: 1.0999151468276978 | avg loss: 1.0757153614774926\n",
      "batch loss: 1.0463382005691528 | avg loss: 1.0755258314071163\n",
      "batch loss: 1.1167449951171875 | avg loss: 1.075790056815514\n",
      "batch loss: 1.0655851364135742 | avg loss: 1.0757250573225081\n",
      "batch loss: 1.0744643211364746 | avg loss: 1.0757170779795586\n",
      "batch loss: 1.1454176902770996 | avg loss: 1.0761554466103607\n",
      "batch loss: 1.09263014793396 | avg loss: 1.0762584134936333\n",
      "batch loss: 1.097834825515747 | avg loss: 1.0763924284751372\n",
      "batch loss: 1.1017075777053833 | avg loss: 1.0765486948284102\n",
      "batch loss: 1.1367197036743164 | avg loss: 1.0769178421219434\n",
      "batch loss: 1.0531513690948486 | avg loss: 1.0767729246034854\n",
      "batch loss: 1.0804469585418701 | avg loss: 1.0767951914758394\n",
      "batch loss: 1.0983632802963257 | avg loss: 1.0769251197217458\n",
      "batch loss: 1.0994669198989868 | avg loss: 1.0770601005611304\n",
      "batch loss: 1.0663132667541504 | avg loss: 1.0769961313122796\n",
      "batch loss: 1.0322400331497192 | avg loss: 1.076731302329069\n",
      "batch loss: 0.9967882633209229 | avg loss: 1.076261049158433\n",
      "batch loss: 0.9866438508033752 | avg loss: 1.07573697197507\n",
      "batch loss: 1.0495758056640625 | avg loss: 1.0755848721709362\n",
      "batch loss: 1.062492847442627 | avg loss: 1.07550919572742\n",
      "batch loss: 1.1057183742523193 | avg loss: 1.0756828116959538\n",
      "batch loss: 1.2898836135864258 | avg loss: 1.0769068162781852\n",
      "batch loss: 1.1971014738082886 | avg loss: 1.0775897404686972\n",
      "batch loss: 1.0020229816436768 | avg loss: 1.0771628096278778\n",
      "batch loss: 1.0027111768722534 | avg loss: 1.0767445420281272\n",
      "batch loss: 1.1653499603271484 | avg loss: 1.0772395443649931\n",
      "batch loss: 1.0553953647613525 | avg loss: 1.0771181878116396\n",
      "batch loss: 1.0806734561920166 | avg loss: 1.077137830178382\n",
      "batch loss: 1.0498454570770264 | avg loss: 1.0769878720844186\n",
      "batch loss: 1.0577298402786255 | avg loss: 1.0768826369379387\n",
      "batch loss: 1.137795090675354 | avg loss: 1.077213682882164\n",
      "batch loss: 1.0082497596740723 | avg loss: 1.076840904918877\n",
      "batch loss: 1.0662122964859009 | avg loss: 1.0767837618627856\n",
      "batch loss: 1.147327184677124 | avg loss: 1.0771609994179425\n",
      "batch loss: 1.122698187828064 | avg loss: 1.0774032185052305\n",
      "batch loss: 1.027426838874817 | avg loss: 1.0771387932161804\n",
      "batch loss: 0.9961746335029602 | avg loss: 1.0767126660597952\n",
      "batch loss: 1.0061757564544678 | avg loss: 1.0763433628681442\n",
      "batch loss: 1.135452151298523 | avg loss: 1.0766512211412191\n",
      "batch loss: 0.9502906799316406 | avg loss: 1.0759965033111176\n",
      "batch loss: 1.023667335510254 | avg loss: 1.0757267653327627\n",
      "batch loss: 0.9581267833709717 | avg loss: 1.0751236885021895\n",
      "batch loss: 0.9413648247718811 | avg loss: 1.074441245319892\n",
      "batch loss: 1.0758705139160156 | avg loss: 1.0744485004904307\n",
      "batch loss: 1.4167319536209106 | avg loss: 1.0761772047991705\n",
      "batch loss: 1.0356594324111938 | avg loss: 1.0759735979027485\n",
      "batch loss: 0.9641730785369873 | avg loss: 1.0754145953059195\n",
      "batch loss: 0.9584813117980957 | avg loss: 1.0748328376765275\n",
      "batch loss: 1.0371936559677124 | avg loss: 1.0746465050938105\n",
      "batch loss: 0.9454537630081177 | avg loss: 1.0740100876451126\n",
      "batch loss: 0.8274722695350647 | avg loss: 1.0728015689288868\n",
      "batch loss: 0.9449199438095093 | avg loss: 1.0721777561234265\n",
      "batch loss: 1.008588433265686 | avg loss: 1.0718690700901365\n",
      "batch loss: 1.1029707193374634 | avg loss: 1.0720193196034087\n",
      "batch loss: 1.2183470726013184 | avg loss: 1.0727228184158986\n",
      "batch loss: 0.98724365234375 | avg loss: 1.072313827190673\n",
      "batch loss: 1.0637081861495972 | avg loss: 1.0722728479476202\n",
      "batch loss: 1.163257122039795 | avg loss: 1.0727040530381045\n",
      "batch loss: 1.0009069442749023 | avg loss: 1.0723653874307308\n",
      "batch loss: 0.9186538457870483 | avg loss: 1.0716437370004788\n",
      "batch loss: 0.9423267841339111 | avg loss: 1.0710394521739994\n",
      "batch loss: 1.0584548711776733 | avg loss: 1.0709809192391329\n",
      "batch loss: 1.037219762802124 | avg loss: 1.0708246175889615\n",
      "batch loss: 1.0090150833129883 | avg loss: 1.0705397810254778\n",
      "batch loss: 1.1151649951934814 | avg loss: 1.0707444838427622\n",
      "batch loss: 1.0263553857803345 | avg loss: 1.0705417938972717\n",
      "batch loss: 1.0145316123962402 | avg loss: 1.0702872021631762\n",
      "batch loss: 0.9477125406265259 | avg loss: 1.0697325656856347\n",
      "batch loss: 0.9507379531860352 | avg loss: 1.0691965539176185\n",
      "batch loss: 1.1652032136917114 | avg loss: 1.0696270770556189\n",
      "batch loss: 1.0266419649124146 | avg loss: 1.069435179233551\n",
      "batch loss: 1.098738193511963 | avg loss: 1.0695654148525662\n",
      "batch loss: 1.0849788188934326 | avg loss: 1.0696336157554018\n",
      "batch loss: 0.9750906229019165 | avg loss: 1.0692171268001003\n",
      "batch loss: 0.9359503984451294 | avg loss: 1.0686326236055608\n",
      "batch loss: 1.0729576349258423 | avg loss: 1.0686515101178764\n",
      "batch loss: 1.0985647439956665 | avg loss: 1.0687815676564756\n",
      "batch loss: 1.0089627504348755 | avg loss: 1.0685226117377673\n",
      "batch loss: 0.9187184572219849 | avg loss: 1.0678769041751992\n",
      "batch loss: 0.949133574962616 | avg loss: 1.0673672761528277\n",
      "batch loss: 1.1778851747512817 | avg loss: 1.0678395748647869\n",
      "batch loss: 0.9752286672592163 | avg loss: 1.0674454858962525\n",
      "batch loss: 1.1376296281814575 | avg loss: 1.0677428763296644\n",
      "batch loss: 0.9165787696838379 | avg loss: 1.067105053094872\n",
      "batch loss: 1.013123869895935 | avg loss: 1.0668782414007587\n",
      "batch loss: 1.0125303268432617 | avg loss: 1.066650844268719\n",
      "batch loss: 0.9654778242111206 | avg loss: 1.066229290018479\n",
      "batch loss: 1.1234086751937866 | avg loss: 1.0664665488781275\n",
      "batch loss: 1.3743304014205933 | avg loss: 1.0677387135580552\n",
      "batch loss: 1.1529111862182617 | avg loss: 1.0680892175607721\n",
      "batch loss: 1.3079172372817993 | avg loss: 1.0690721192809403\n",
      "batch loss: 0.982960045337677 | avg loss: 1.0687206414281105\n",
      "batch loss: 1.1734473705291748 | avg loss: 1.0691463598390905\n",
      "batch loss: 0.8921504020690918 | avg loss: 1.0684297770141107\n",
      "batch loss: 1.1416245698928833 | avg loss: 1.0687249173079767\n",
      "batch loss: 0.9199584722518921 | avg loss: 1.0681274617053418\n",
      "batch loss: 1.141264796257019 | avg loss: 1.0684200110435487\n",
      "batch loss: 0.9704836010932922 | avg loss: 1.0680298261433483\n",
      "batch loss: 0.9581542611122131 | avg loss: 1.0675938119963995\n",
      "batch loss: 0.9958996772766113 | avg loss: 1.0673104359698389\n",
      "batch loss: 0.9554222822189331 | avg loss: 1.0668699314275125\n",
      "batch loss: 1.1417897939682007 | avg loss: 1.0671637348100251\n",
      "batch loss: 0.9459884166717529 | avg loss: 1.0666903937235475\n",
      "batch loss: 1.0827553272247314 | avg loss: 1.0667529031924237\n",
      "batch loss: 1.2772384881973267 | avg loss: 1.0675687387932178\n",
      "batch loss: 1.202635407447815 | avg loss: 1.0680902317223862\n",
      "batch loss: 1.3309811353683472 | avg loss: 1.069101350582563\n",
      "batch loss: 1.0067534446716309 | avg loss: 1.068862469717004\n",
      "batch loss: 1.1094443798065186 | avg loss: 1.069017362503605\n",
      "batch loss: 1.1032373905181885 | avg loss: 1.0691474766785654\n",
      "batch loss: 0.9310795068740845 | avg loss: 1.0686244919444576\n",
      "batch loss: 1.0390948057174683 | avg loss: 1.0685130591662424\n",
      "batch loss: 0.9896368384361267 | avg loss: 1.0682165320206405\n",
      "batch loss: 1.0599955320358276 | avg loss: 1.0681857417585252\n",
      "batch loss: 0.980272114276886 | avg loss: 1.0678577058350862\n",
      "batch loss: 1.0288695096969604 | avg loss: 1.0677127683029741\n",
      "batch loss: 1.032886266708374 | avg loss: 1.0675837812600313\n",
      "batch loss: 1.1844114065170288 | avg loss: 1.068014879508212\n",
      "batch loss: 1.0963202714920044 | avg loss: 1.068118943449329\n",
      "batch loss: 1.0185904502868652 | avg loss: 1.0679375203974517\n",
      "batch loss: 1.0388703346252441 | avg loss: 1.0678314357778451\n",
      "batch loss: 1.0360195636749268 | avg loss: 1.0677157562429254\n",
      "batch loss: 1.1224539279937744 | avg loss: 1.0679140829521676\n",
      "batch loss: 1.0408366918563843 | avg loss: 1.0678163306377424\n",
      "batch loss: 1.0622434616088867 | avg loss: 1.0677962843462718\n",
      "batch loss: 0.9133363962173462 | avg loss: 1.0672426646755588\n",
      "batch loss: 0.9813899993896484 | avg loss: 1.0669360480138235\n",
      "batch loss: 1.060323715209961 | avg loss: 1.0669125165803577\n",
      "batch loss: 1.0900321006774902 | avg loss: 1.0669945009211277\n",
      "batch loss: 1.0356504917144775 | avg loss: 1.0668837447048498\n",
      "batch loss: 0.8981437683105469 | avg loss: 1.0662895898583908\n",
      "batch loss: 1.0321452617645264 | avg loss: 1.0661697851984124\n",
      "batch loss: 0.9952974915504456 | avg loss: 1.065921979975867\n",
      "batch loss: 1.2087152004241943 | avg loss: 1.0664195173293456\n",
      "batch loss: 0.9340285062789917 | avg loss: 1.0659598263187542\n",
      "batch loss: 1.0571026802062988 | avg loss: 1.0659291787543512\n",
      "batch loss: 0.9801842570304871 | avg loss: 1.0656335066104758\n",
      "batch loss: 0.9520149230957031 | avg loss: 1.0652430647427273\n",
      "batch loss: 0.9504863023757935 | avg loss: 1.0648500621318817\n",
      "batch loss: 1.0598630905151367 | avg loss: 1.0648330417509373\n",
      "batch loss: 0.7476223707199097 | avg loss: 1.0637540938902874\n",
      "batch loss: 0.9658674001693726 | avg loss: 1.0634222745895385\n",
      "batch loss: 1.0128600597381592 | avg loss: 1.0632514562961217\n",
      "batch loss: 0.8458978533744812 | avg loss: 1.0625196259832543\n",
      "batch loss: 0.9598501920700073 | avg loss: 1.062175097681532\n",
      "batch loss: 1.0407865047454834 | avg loss: 1.0621035639258931\n",
      "batch loss: 0.9992028474807739 | avg loss: 1.0618938948710759\n",
      "batch loss: 1.0809718370437622 | avg loss: 1.0619572767387593\n",
      "batch loss: 1.052480697631836 | avg loss: 1.061925897337743\n",
      "batch loss: 1.0539758205413818 | avg loss: 1.0618996594605272\n",
      "batch loss: 1.0968291759490967 | avg loss: 1.0620145591858186\n",
      "batch loss: 1.1224749088287354 | avg loss: 1.0622127898403855\n",
      "batch loss: 0.9790136218070984 | avg loss: 1.0619408971343944\n",
      "batch loss: 1.154896855354309 | avg loss: 1.0622436852719186\n",
      "batch loss: 1.1469582319259644 | avg loss: 1.0625187325013148\n",
      "batch loss: 1.0092791318893433 | avg loss: 1.062346436059205\n",
      "batch loss: 1.098629117012024 | avg loss: 1.0624634769655044\n",
      "batch loss: 0.9588140845298767 | avg loss: 1.0621301991763221\n",
      "batch loss: 1.009824275970459 | avg loss: 1.0619625519865599\n",
      "batch loss: 0.9240888953208923 | avg loss: 1.061522061070695\n",
      "batch loss: 0.9646437168121338 | avg loss: 1.0612135313119098\n",
      "batch loss: 0.9616261720657349 | avg loss: 1.0608973809650966\n",
      "batch loss: 1.0071818828582764 | avg loss: 1.060727395211594\n",
      "batch loss: 0.8833131194114685 | avg loss: 1.0601677287264202\n",
      "batch loss: 1.066640019416809 | avg loss: 1.0601880818417988\n",
      "batch loss: 0.7432175874710083 | avg loss: 1.0591944439284107\n",
      "batch loss: 1.0302705764770508 | avg loss: 1.059104056842625\n",
      "batch loss: 1.0268347263336182 | avg loss: 1.0590035293332514\n",
      "batch loss: 1.0394105911254883 | avg loss: 1.0589426816990657\n",
      "batch loss: 0.9066404700279236 | avg loss: 1.0584711578239228\n",
      "batch loss: 0.9401428699493408 | avg loss: 1.0581059470588778\n",
      "batch loss: 0.9666796326637268 | avg loss: 1.0578246353222773\n",
      "batch loss: 1.0250461101531982 | avg loss: 1.0577240876990592\n",
      "batch loss: 1.0941084623336792 | avg loss: 1.0578353548997768\n",
      "batch loss: 1.1038718223571777 | avg loss: 1.0579757099834883\n",
      "batch loss: 1.1430816650390625 | avg loss: 1.058234390697943\n",
      "batch loss: 0.8902906179428101 | avg loss: 1.0577254701744427\n",
      "batch loss: 1.149050235748291 | avg loss: 1.0580013758106175\n",
      "batch loss: 1.0621113777160645 | avg loss: 1.058013755334429\n",
      "batch loss: 1.0629644393920898 | avg loss: 1.0580286222535211\n",
      "batch loss: 1.0830023288726807 | avg loss: 1.0581033938302251\n",
      "batch loss: 1.024364709854126 | avg loss: 1.0580026813407442\n",
      "batch loss: 1.1055309772491455 | avg loss: 1.0581441346023763\n",
      "batch loss: 1.020681381225586 | avg loss: 1.058032969162089\n",
      "batch loss: 1.091871738433838 | avg loss: 1.0581330838640766\n",
      "batch loss: 1.0021419525146484 | avg loss: 1.0579679182848747\n",
      "batch loss: 1.1118780374526978 | avg loss: 1.0581264774588977\n",
      "batch loss: 0.9756476879119873 | avg loss: 1.057884604175769\n",
      "batch loss: 1.0007094144821167 | avg loss: 1.0577174252585362\n",
      "batch loss: 1.2660536766052246 | avg loss: 1.0583248195773312\n",
      "batch loss: 1.2745150327682495 | avg loss: 1.0589532794993977\n",
      "batch loss: 1.0668466091156006 | avg loss: 1.0589761587156765\n",
      "batch loss: 1.066292643547058 | avg loss: 1.0589973046255938\n",
      "batch loss: 1.1889656782150269 | avg loss: 1.0593718532526528\n",
      "batch loss: 1.0038902759552002 | avg loss: 1.0592124234328324\n",
      "batch loss: 1.077560305595398 | avg loss: 1.059264996161092\n",
      "batch loss: 1.0136736631393433 | avg loss: 1.0591347352096012\n",
      "batch loss: 0.9591389894485474 | avg loss: 1.0588498470450398\n",
      "batch loss: 1.0619040727615356 | avg loss: 1.0588585238226436\n",
      "batch loss: 1.068588137626648 | avg loss: 1.0588860864679808\n",
      "batch loss: 1.033345341682434 | avg loss: 1.0588139374714114\n",
      "batch loss: 1.1306456327438354 | avg loss: 1.0590162802749956\n",
      "batch loss: 1.1320399045944214 | avg loss: 1.0592214028152187\n",
      "batch loss: 1.081107258796692 | avg loss: 1.0592827077339344\n",
      "batch loss: 1.1050909757614136 | avg loss: 1.059410663789877\n",
      "batch loss: 0.9903226494789124 | avg loss: 1.0592182180675624\n",
      "batch loss: 1.0024240016937256 | avg loss: 1.059060456355413\n",
      "batch loss: 1.0477559566497803 | avg loss: 1.059029141951796\n",
      "batch loss: 1.1857283115386963 | avg loss: 1.0593791396578371\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.49\n",
      "  Validation took: 0:01:52\n",
      "\n",
      "======= Epoch 3 / 5 =======\n",
      "batch loss: 0.9806013107299805 | avg loss: 0.9806013107299805\n",
      "batch loss: 0.9961437582969666 | avg loss: 0.9883725345134735\n",
      "batch loss: 0.9995286464691162 | avg loss: 0.9920912384986877\n",
      "batch loss: 1.1348031759262085 | avg loss: 1.027769222855568\n",
      "batch loss: 1.1000608205795288 | avg loss: 1.0422275424003602\n",
      "batch loss: 0.9865095615386963 | avg loss: 1.0329412122567494\n",
      "batch loss: 0.9453080892562866 | avg loss: 1.0204221946852547\n",
      "batch loss: 1.0154916048049927 | avg loss: 1.019805870950222\n",
      "batch loss: 0.9883759617805481 | avg loss: 1.0163136588202581\n",
      "batch loss: 0.7837005257606506 | avg loss: 0.9930523455142974\n",
      "batch loss: 0.7878444790840149 | avg loss: 0.9743970849297263\n",
      "batch loss: 1.0734353065490723 | avg loss: 0.9826502700646719\n",
      "batch loss: 0.9616729021072388 | avg loss: 0.9810366263756385\n",
      "batch loss: 1.245357871055603 | avg loss: 0.9999167152813503\n",
      "batch loss: 1.0437549352645874 | avg loss: 1.0028392632802328\n",
      "batch loss: 1.2419582605361938 | avg loss: 1.0177842006087303\n",
      "batch loss: 1.0439510345458984 | avg loss: 1.0193234261344462\n",
      "batch loss: 0.8053116202354431 | avg loss: 1.0074338813622792\n",
      "batch loss: 0.8389269709587097 | avg loss: 0.9985650966041967\n",
      "batch loss: 1.0566554069519043 | avg loss: 1.001469612121582\n",
      "batch loss: 0.9735278487205505 | avg loss: 1.000139051959628\n",
      "batch loss: 0.7489758729934692 | avg loss: 0.9887225438248027\n",
      "batch loss: 0.855908989906311 | avg loss: 0.9829480414805205\n",
      "batch loss: 0.9132019281387329 | avg loss: 0.9800419534246126\n",
      "batch loss: 1.0724037885665894 | avg loss: 0.9837364268302917\n",
      "batch loss: 0.9045096635818481 | avg loss: 0.9806892436284286\n",
      "batch loss: 1.0013411045074463 | avg loss: 0.9814541273646884\n",
      "batch loss: 0.9939187169075012 | avg loss: 0.9818992912769318\n",
      "batch loss: 1.0010640621185303 | avg loss: 0.9825601454438835\n",
      "batch loss: 0.773541271686554 | avg loss: 0.9755928496519725\n",
      "batch loss: 0.7297126054763794 | avg loss: 0.9676612288721146\n",
      "batch loss: 1.0450960397720337 | avg loss: 0.9700810667127371\n",
      "batch loss: 0.9573160409927368 | avg loss: 0.9696942477515249\n",
      "batch loss: 1.0710920095443726 | avg loss: 0.9726765348630793\n",
      "batch loss: 0.7523380517959595 | avg loss: 0.9663811496325901\n",
      "batch loss: 0.9673662781715393 | avg loss: 0.9664085143142276\n",
      "batch loss: 0.8748987317085266 | avg loss: 0.9639352769465059\n",
      "batch loss: 1.174569845199585 | avg loss: 0.9694782919005344\n",
      "batch loss: 0.8900848031044006 | avg loss: 0.9674425614185822\n",
      "batch loss: 0.9091539978981018 | avg loss: 0.9659853473305702\n",
      "batch loss: 0.8679137229919434 | avg loss: 0.9635933564930428\n",
      "batch loss: 0.9876292943954468 | avg loss: 0.9641656407288143\n",
      "batch loss: 0.9714421629905701 | avg loss: 0.9643348621767621\n",
      "batch loss: 0.979799747467041 | avg loss: 0.9646863368424502\n",
      "batch loss: 1.0110831260681152 | avg loss: 0.9657173766030206\n",
      "batch loss: 1.1187357902526855 | avg loss: 0.9690438638562742\n",
      "batch loss: 0.9621335864067078 | avg loss: 0.9688968366764962\n",
      "batch loss: 1.1489410400390625 | avg loss: 0.972647757579883\n",
      "batch loss: 0.7994982004165649 | avg loss: 0.9691140931479785\n",
      "batch loss: 0.8884411454200745 | avg loss: 0.9675006341934204\n",
      "batch loss: 0.9779787063598633 | avg loss: 0.9677060865888408\n",
      "batch loss: 0.7937383055686951 | avg loss: 0.9643605523384534\n",
      "batch loss: 0.7682705521583557 | avg loss: 0.9606607410143007\n",
      "batch loss: 0.7482479214668274 | avg loss: 0.95672717028194\n",
      "batch loss: 1.246795892715454 | avg loss: 0.9620011470534585\n",
      "batch loss: 0.9696433544158936 | avg loss: 0.9621376150420734\n",
      "batch loss: 0.8474501371383667 | avg loss: 0.9601255540262189\n",
      "batch loss: 0.7862871885299683 | avg loss: 0.9571283408280077\n",
      "batch loss: 0.7540212273597717 | avg loss: 0.9536858473793935\n",
      "batch loss: 0.8012860417366028 | avg loss: 0.9511458506186803\n",
      "batch loss: 0.8815429210662842 | avg loss: 0.9500048189866738\n",
      "batch loss: 1.0610246658325195 | avg loss: 0.9517954616777359\n",
      "batch loss: 0.9798350930213928 | avg loss: 0.9522405351911273\n",
      "batch loss: 1.0105594396591187 | avg loss: 0.9531517680734396\n",
      "batch loss: 0.9319036602973938 | avg loss: 0.9528248741076543\n",
      "batch loss: 1.1567137241363525 | avg loss: 0.9559140991080891\n",
      "batch loss: 0.9084711670875549 | avg loss: 0.955205995645096\n",
      "batch loss: 0.8796219825744629 | avg loss: 0.9540944660411161\n",
      "batch loss: 0.8995663523674011 | avg loss: 0.9533042035241058\n",
      "batch loss: 0.8241912722587585 | avg loss: 0.951459733077458\n",
      "batch loss: 0.8675150871276855 | avg loss: 0.9502774141204189\n",
      "batch loss: 1.1707016229629517 | avg loss: 0.9533388614654541\n",
      "batch loss: 1.0875449180603027 | avg loss: 0.9551773005968904\n",
      "batch loss: 0.9539055228233337 | avg loss: 0.9551601144107612\n",
      "batch loss: 1.036218523979187 | avg loss: 0.9562408932050069\n",
      "batch loss: 1.0322126150131226 | avg loss: 0.9572405211235347\n",
      "batch loss: 0.874944806098938 | avg loss: 0.9561717456037347\n",
      "batch loss: 1.0125161409378052 | avg loss: 0.9568941096464793\n",
      "batch loss: 0.975122332572937 | avg loss: 0.9571248466455484\n",
      "batch loss: 0.9606537818908691 | avg loss: 0.9571689583361149\n",
      "batch loss: 0.9975278377532959 | avg loss: 0.9576672161066974\n",
      "batch loss: 1.1133040189743042 | avg loss: 0.9595652258977657\n",
      "batch loss: 0.9689048528671265 | avg loss: 0.9596777515239027\n",
      "batch loss: 0.8544519543647766 | avg loss: 0.9584250634624845\n",
      "batch loss: 0.9971380233764648 | avg loss: 0.9588805100497078\n",
      "batch loss: 0.9914520382881165 | avg loss: 0.9592592487501543\n",
      "batch loss: 0.9724403023719788 | avg loss: 0.9594107551136236\n",
      "batch loss: 0.9668800234794617 | avg loss: 0.9594956331632354\n",
      "batch loss: 0.9700028300285339 | avg loss: 0.9596136915549803\n",
      "batch loss: 0.9218853116035461 | avg loss: 0.9591944873332977\n",
      "batch loss: 0.8906282782554626 | avg loss: 0.9584410125082665\n",
      "batch loss: 0.9664655923843384 | avg loss: 0.9585282362025717\n",
      "batch loss: 0.9608483910560608 | avg loss: 0.9585531841042221\n",
      "batch loss: 0.988380491733551 | avg loss: 0.9588704958875128\n",
      "batch loss: 0.9382292032241821 | avg loss: 0.9586532191226357\n",
      "batch loss: 1.1034363508224487 | avg loss: 0.9601613767445087\n",
      "batch loss: 0.849895715713501 | avg loss: 0.9590246173524365\n",
      "batch loss: 1.232118844985962 | avg loss: 0.9618112931446153\n",
      "batch loss: 1.2130146026611328 | avg loss: 0.9643487003114488\n",
      "batch loss: 1.0161840915679932 | avg loss: 0.9648670542240143\n",
      "batch loss: 1.313639521598816 | avg loss: 0.9683202469702994\n",
      "batch loss: 1.0701992511749268 | avg loss: 0.9693190607370115\n",
      "batch loss: 0.6937206983566284 | avg loss: 0.9666433484808913\n",
      "batch loss: 1.1255638599395752 | avg loss: 0.9681714303218402\n",
      "batch loss: 1.0252693891525269 | avg loss: 0.9687152204059419\n",
      "batch loss: 0.9149585962295532 | avg loss: 0.9682080824420137\n",
      "batch loss: 0.9877604842185974 | avg loss: 0.9683908151688977\n",
      "batch loss: 0.8655332922935486 | avg loss: 0.9674384306978296\n",
      "batch loss: 1.1213083267211914 | avg loss: 0.9688500811200623\n",
      "batch loss: 1.090723991394043 | avg loss: 0.9699580257589167\n",
      "batch loss: 0.9053615927696228 | avg loss: 0.9693760759121662\n",
      "batch loss: 1.0353549718856812 | avg loss: 0.9699651731976441\n",
      "batch loss: 1.0919979810714722 | avg loss: 0.9710451095505098\n",
      "batch loss: 0.9911819696426392 | avg loss: 0.971221748674125\n",
      "batch loss: 1.0210108757019043 | avg loss: 0.9716546976048014\n",
      "batch loss: 0.907895028591156 | avg loss: 0.9711050452857182\n",
      "batch loss: 0.903114914894104 | avg loss: 0.9705239330601488\n",
      "batch loss: 0.8267850279808044 | avg loss: 0.9693058067459172\n",
      "batch loss: 0.9608181715011597 | avg loss: 0.9692344820799947\n",
      "batch loss: 0.9950679540634155 | avg loss: 0.9694497610131899\n",
      "batch loss: 0.972111701965332 | avg loss: 0.9694717605251911\n",
      "batch loss: 0.8227559328079224 | avg loss: 0.9682691717734102\n",
      "batch loss: 0.9528443813323975 | avg loss: 0.9681437669730768\n",
      "batch loss: 1.098008394241333 | avg loss: 0.9691910623542724\n",
      "batch loss: 0.8346954584121704 | avg loss: 0.9681150975227356\n",
      "batch loss: 1.1367666721343994 | avg loss: 0.9694536020831456\n",
      "batch loss: 1.2408246994018555 | avg loss: 0.9715903823769937\n",
      "batch loss: 0.9882236123085022 | avg loss: 0.9717203294858336\n",
      "batch loss: 0.9718959331512451 | avg loss: 0.9717216907545577\n",
      "batch loss: 0.8181016445159912 | avg loss: 0.9705399980911842\n",
      "batch loss: 0.6854305267333984 | avg loss: 0.9683635899128805\n",
      "batch loss: 1.1318104267120361 | avg loss: 0.9696018235249952\n",
      "batch loss: 0.8128334283828735 | avg loss: 0.9684231137870846\n",
      "batch loss: 1.285463809967041 | avg loss: 0.9707890891317111\n",
      "batch loss: 1.0249578952789307 | avg loss: 0.9711903395476165\n",
      "batch loss: 1.101681113243103 | avg loss: 0.9721498305306715\n",
      "batch loss: 0.6270671486854553 | avg loss: 0.9696309788383707\n",
      "batch loss: 1.0918149948120117 | avg loss: 0.9705163702584695\n",
      "batch loss: 0.8644664883613586 | avg loss: 0.9697534214678428\n",
      "batch loss: 1.3347798585891724 | avg loss: 0.9723607531615666\n",
      "batch loss: 1.187422752380371 | avg loss: 0.9738860155673738\n",
      "batch loss: 0.7533511519432068 | avg loss: 0.9723329531474852\n",
      "batch loss: 1.0114253759384155 | avg loss: 0.9726063267334358\n",
      "batch loss: 0.9786781072616577 | avg loss: 0.9726484918759929\n",
      "batch loss: 1.0014231204986572 | avg loss: 0.9728469375906319\n",
      "batch loss: 1.006921648979187 | avg loss: 0.9730803260248001\n",
      "batch loss: 0.7669264078140259 | avg loss: 0.9716779184179242\n",
      "batch loss: 1.2935941219329834 | avg loss: 0.9738530279011339\n",
      "batch loss: 0.8915248513221741 | avg loss: 0.9733004898032886\n",
      "batch loss: 1.0639177560806274 | avg loss: 0.9739046049118042\n",
      "batch loss: 1.0037493705749512 | avg loss: 0.974102252366527\n",
      "batch loss: 0.9301391839981079 | avg loss: 0.9738130216535769\n",
      "batch loss: 1.1969916820526123 | avg loss: 0.9752717057084725\n",
      "batch loss: 0.8964720964431763 | avg loss: 0.9747600199340226\n",
      "batch loss: 0.9760081171989441 | avg loss: 0.9747680721744414\n",
      "batch loss: 0.8390196561813354 | avg loss: 0.9738978900206394\n",
      "batch loss: 1.0267508029937744 | avg loss: 0.9742345327784301\n",
      "batch loss: 0.9384413957595825 | avg loss: 0.9740079939365387\n",
      "batch loss: 1.1596870422363281 | avg loss: 0.9751757866931412\n",
      "batch loss: 0.8216431736946106 | avg loss: 0.9742162078619003\n",
      "batch loss: 1.0265790224075317 | avg loss: 0.9745414427348545\n",
      "batch loss: 1.0395985841751099 | avg loss: 0.9749430300276957\n",
      "batch loss: 1.2453033924102783 | avg loss: 0.9766016825576501\n",
      "batch loss: 0.7765622735023499 | avg loss: 0.9753819300634105\n",
      "batch loss: 1.1006892919540405 | avg loss: 0.9761413686203234\n",
      "batch loss: 0.9018935561180115 | avg loss: 0.9756940926413938\n",
      "batch loss: 1.0849334001541138 | avg loss: 0.9763482202312903\n",
      "batch loss: 0.853252649307251 | avg loss: 0.9756155084995997\n",
      "batch loss: 1.0007565021514893 | avg loss: 0.9757642717756463\n",
      "batch loss: 0.8528220653533936 | avg loss: 0.9750410823261036\n",
      "batch loss: 0.9819451570510864 | avg loss: 0.9750814570320977\n",
      "batch loss: 1.0217260122299194 | avg loss: 0.9753526463065036\n",
      "batch loss: 1.0836336612701416 | avg loss: 0.9759785481271027\n",
      "batch loss: 0.9880539774894714 | avg loss: 0.9760479471464266\n",
      "batch loss: 1.12371826171875 | avg loss: 0.9768917775154113\n",
      "batch loss: 1.0906556844711304 | avg loss: 0.977538163350387\n",
      "batch loss: 0.9471443295478821 | avg loss: 0.9773664467752317\n",
      "batch loss: 1.019353985786438 | avg loss: 0.9776023318258564\n",
      "batch loss: 1.1405116319656372 | avg loss: 0.9785124396478664\n",
      "batch loss: 1.046121597290039 | avg loss: 0.9788880460792118\n",
      "batch loss: 0.9644187092781067 | avg loss: 0.9788081049919128\n",
      "batch loss: 1.0916101932525635 | avg loss: 0.9794278966856527\n",
      "batch loss: 0.8796979784965515 | avg loss: 0.9788829244551112\n",
      "batch loss: 1.081641435623169 | avg loss: 0.9794413946245027\n",
      "batch loss: 1.0666223764419556 | avg loss: 0.9799126431748674\n",
      "batch loss: 0.9478550553321838 | avg loss: 0.9797402905520572\n",
      "batch loss: 1.0553770065307617 | avg loss: 0.9801447649690557\n",
      "batch loss: 1.105099081993103 | avg loss: 0.980809415591524\n",
      "batch loss: 0.7962744235992432 | avg loss: 0.9798330399725173\n",
      "batch loss: 0.8664078712463379 | avg loss: 0.9792360654002742\n",
      "batch loss: 0.962117075920105 | avg loss: 0.9791464371831006\n",
      "batch loss: 1.0478588342666626 | avg loss: 0.9795043142512441\n",
      "batch loss: 0.8573579788208008 | avg loss: 0.978871431684247\n",
      "batch loss: 1.0161733627319336 | avg loss: 0.9790637096793381\n",
      "batch loss: 0.9117770195007324 | avg loss: 0.9787186497297042\n",
      "batch loss: 0.9901747107505798 | avg loss: 0.978777099020627\n",
      "batch loss: 0.9131515622138977 | avg loss: 0.9784439744683087\n",
      "batch loss: 1.1919209957122803 | avg loss: 0.9795221412422681\n",
      "batch loss: 0.906557559967041 | avg loss: 0.9791554850549554\n",
      "batch loss: 0.8323061466217041 | avg loss: 0.9784212383627892\n",
      "batch loss: 0.8372914791107178 | avg loss: 0.9777191002570574\n",
      "batch loss: 0.8625054359436035 | avg loss: 0.9771487355822384\n",
      "batch loss: 0.9887086153030396 | avg loss: 0.9772056808025379\n",
      "batch loss: 0.8505344390869141 | avg loss: 0.9765847433431476\n",
      "batch loss: 0.8749471306800842 | avg loss: 0.9760889501106448\n",
      "batch loss: 0.7857304811477661 | avg loss: 0.975164879872961\n",
      "batch loss: 1.1110090017318726 | avg loss: 0.9758211316693808\n",
      "batch loss: 1.24948251247406 | avg loss: 0.9771368113847879\n",
      "batch loss: 0.9315510988235474 | avg loss: 0.9769186979275571\n",
      "batch loss: 0.8289423584938049 | avg loss: 0.9762140486921583\n",
      "batch loss: 1.1761256456375122 | avg loss: 0.9771614970189135\n",
      "batch loss: 0.8368160128593445 | avg loss: 0.9764994900181608\n",
      "batch loss: 0.7729405164718628 | avg loss: 0.9755438140860186\n",
      "batch loss: 0.7312964200973511 | avg loss: 0.9744024711234547\n",
      "batch loss: 1.048241376876831 | avg loss: 0.9747459078944006\n",
      "batch loss: 1.11771821975708 | avg loss: 0.9754078167456167\n",
      "batch loss: 0.8834561109542847 | avg loss: 0.9749840761659332\n",
      "batch loss: 1.0271581411361694 | avg loss: 0.9752234067391912\n",
      "batch loss: 0.9136961698532104 | avg loss: 0.9749424604520406\n",
      "batch loss: 1.150862455368042 | avg loss: 0.9757420967925678\n",
      "batch loss: 0.8475507497787476 | avg loss: 0.975162045448614\n",
      "batch loss: 0.8896005153656006 | avg loss: 0.9747766331509427\n",
      "batch loss: 1.1231807470321655 | avg loss: 0.9754421224508585\n",
      "batch loss: 0.9116829633712769 | avg loss: 0.9751574833478246\n",
      "batch loss: 0.9396663904190063 | avg loss: 0.9749997451570299\n",
      "batch loss: 1.0581554174423218 | avg loss: 0.9753676906096197\n",
      "batch loss: 0.8801189064979553 | avg loss: 0.9749480924417269\n",
      "batch loss: 0.8594488501548767 | avg loss: 0.9744415168176618\n",
      "batch loss: 1.023408055305481 | avg loss: 0.9746553444966478\n",
      "batch loss: 0.9018596410751343 | avg loss: 0.9743388414382934\n",
      "batch loss: 0.945410966873169 | avg loss: 0.9742136125440721\n",
      "batch loss: 0.6809593439102173 | avg loss: 0.9729495855240986\n",
      "batch loss: 0.8776587843894958 | avg loss: 0.9725406121286712\n",
      "batch loss: 1.2338167428970337 | avg loss: 0.9736571767900744\n",
      "batch loss: 0.9434625506401062 | avg loss: 0.9735286890192235\n",
      "batch loss: 1.0928034782409668 | avg loss: 0.9740340906684681\n",
      "batch loss: 0.8182976245880127 | avg loss: 0.9733769747778334\n",
      "batch loss: 0.8465774655342102 | avg loss: 0.9728442037305912\n",
      "batch loss: 1.0839362144470215 | avg loss: 0.9733090238591119\n",
      "batch loss: 0.8898025751113892 | avg loss: 0.972961080322663\n",
      "batch loss: 0.8962011933326721 | avg loss: 0.9726425745675178\n",
      "batch loss: 1.1736524105072021 | avg loss: 0.9734731937242933\n",
      "batch loss: 0.9325276017189026 | avg loss: 0.9733046933456704\n",
      "batch loss: 1.1308772563934326 | avg loss: 0.973950482538489\n",
      "batch loss: 0.9647337794303894 | avg loss: 0.9739128633421295\n",
      "batch loss: 0.9086832404136658 | avg loss: 0.9736477022733145\n",
      "batch loss: 0.9884169697761536 | avg loss: 0.9737074968785893\n",
      "batch loss: 0.922252893447876 | avg loss: 0.9735000186389492\n",
      "batch loss: 0.7894027233123779 | avg loss: 0.9727606720713726\n",
      "batch loss: 0.8792554140090942 | avg loss: 0.9723866510391236\n",
      "batch loss: 0.8717606067657471 | avg loss: 0.9719857504643292\n",
      "batch loss: 1.0313923358917236 | avg loss: 0.9722214908826918\n",
      "batch loss: 0.7839798331260681 | avg loss: 0.9714774527097408\n",
      "batch loss: 0.8270137906074524 | avg loss: 0.970908698134535\n",
      "batch loss: 1.0879273414611816 | avg loss: 0.9713675947750315\n",
      "batch loss: 0.954291582107544 | avg loss: 0.9713008916005492\n",
      "batch loss: 0.9892149567604065 | avg loss: 0.9713705961342451\n",
      "batch loss: 1.0774815082550049 | avg loss: 0.9717818787393644\n",
      "batch loss: 0.8769434690475464 | avg loss: 0.9714157072733728\n",
      "batch loss: 1.289357304573059 | avg loss: 0.9726385595706794\n",
      "batch loss: 1.0120435953140259 | avg loss: 0.9727895367191213\n",
      "batch loss: 1.089585542678833 | avg loss: 0.9732353230014102\n",
      "batch loss: 1.0399624109268188 | avg loss: 0.9734890381646247\n",
      "batch loss: 0.7474567890167236 | avg loss: 0.9726328554027008\n",
      "batch loss: 1.1690629720687866 | avg loss: 0.9733741011259691\n",
      "batch loss: 0.8063681125640869 | avg loss: 0.9727462590637064\n",
      "batch loss: 1.0409560203552246 | avg loss: 0.9730017263344611\n",
      "batch loss: 0.7564219236373901 | avg loss: 0.9721935927423079\n",
      "batch loss: 0.8976504802703857 | avg loss: 0.971916480800033\n",
      "batch loss: 0.9533487558364868 | avg loss: 0.9718477114483163\n",
      "batch loss: 1.083901286125183 | avg loss: 0.9722611932736921\n",
      "batch loss: 0.9878010749816895 | avg loss: 0.9723183251917362\n",
      "batch loss: 0.8433526158332825 | avg loss: 0.9718459233259543\n",
      "batch loss: 1.000555396080017 | avg loss: 0.9719507024235969\n",
      "batch loss: 0.9525054097175598 | avg loss: 0.9718799922683022\n",
      "batch loss: 1.0293852090835571 | avg loss: 0.9720883445031401\n",
      "batch loss: 0.8344631195068359 | avg loss: 0.9715915025356444\n",
      "batch loss: 0.9276545643806458 | avg loss: 0.9714334559955186\n",
      "batch loss: 0.7693972587585449 | avg loss: 0.9707093119194004\n",
      "batch loss: 0.9221404790878296 | avg loss: 0.9705358518021447\n",
      "batch loss: 1.019091010093689 | avg loss: 0.9707086459597659\n",
      "batch loss: 0.9372511506080627 | avg loss: 0.9705900023592279\n",
      "batch loss: 0.952775776386261 | avg loss: 0.9705270545642705\n",
      "batch loss: 0.7276743650436401 | avg loss: 0.9696719394603246\n",
      "batch loss: 1.0129503011703491 | avg loss: 0.9698237933610615\n",
      "batch loss: 0.9435306191444397 | avg loss: 0.9697318591854789\n",
      "batch loss: 1.1066443920135498 | avg loss: 0.9702089063381899\n",
      "batch loss: 0.9454061985015869 | avg loss: 0.9701227858248684\n",
      "batch loss: 0.9566684365272522 | avg loss: 0.9700762309830081\n",
      "batch loss: 1.0111100673675537 | avg loss: 0.9702177269705411\n",
      "batch loss: 0.8138601779937744 | avg loss: 0.9696804158056724\n",
      "batch loss: 0.8030170798301697 | avg loss: 0.9691096509564413\n",
      "batch loss: 0.858417272567749 | avg loss: 0.9687318612691078\n",
      "batch loss: 0.8011422157287598 | avg loss: 0.9681618284611475\n",
      "batch loss: 0.853367805480957 | avg loss: 0.9677726961798587\n",
      "batch loss: 0.91286301612854 | avg loss: 0.9675871905040097\n",
      "batch loss: 0.8279409408569336 | avg loss: 0.9671170011112585\n",
      "batch loss: 0.7918663024902344 | avg loss: 0.9665289115185706\n",
      "batch loss: 0.9114438891410828 | avg loss: 0.9663446806744987\n",
      "batch loss: 0.8451606035232544 | avg loss: 0.9659407337506613\n",
      "batch loss: 1.1063668727874756 | avg loss: 0.9664072657740393\n",
      "batch loss: 0.9447998404502869 | avg loss: 0.9663357180080666\n",
      "batch loss: 1.0897160768508911 | avg loss: 0.9667429139118383\n",
      "batch loss: 1.1114816665649414 | avg loss: 0.9672190282297762\n",
      "batch loss: 0.9776335954666138 | avg loss: 0.9672531743518642\n",
      "batch loss: 0.9965993762016296 | avg loss: 0.9673490769722882\n",
      "batch loss: 1.0394548177719116 | avg loss: 0.9675839490921567\n",
      "batch loss: 0.9866123199462891 | avg loss: 0.9676457295170078\n",
      "batch loss: 0.8317567110061646 | avg loss: 0.967205959230565\n",
      "batch loss: 1.0334937572479248 | avg loss: 0.9674197908370725\n",
      "batch loss: 0.8267465829849243 | avg loss: 0.9669674654098952\n",
      "batch loss: 1.0490238666534424 | avg loss: 0.9672304666959323\n",
      "batch loss: 0.7886506915092468 | avg loss: 0.9666599242831952\n",
      "batch loss: 0.8462022542953491 | avg loss: 0.9662763011303677\n",
      "batch loss: 0.8363780975341797 | avg loss: 0.965863925880856\n",
      "batch loss: 0.9046968221664429 | avg loss: 0.9656703590969496\n",
      "batch loss: 0.7927945256233215 | avg loss: 0.9651250094645407\n",
      "batch loss: 0.9386476278305054 | avg loss: 0.9650417472581444\n",
      "batch loss: 0.6394560933113098 | avg loss: 0.9640211025749255\n",
      "batch loss: 1.0213645696640015 | avg loss: 0.9642003009095788\n",
      "batch loss: 0.8366272449493408 | avg loss: 0.9638028770592354\n",
      "batch loss: 0.9483970999717712 | avg loss: 0.9637550330310134\n",
      "batch loss: 0.8225792646408081 | avg loss: 0.9633179563486908\n",
      "batch loss: 0.9255091547966003 | avg loss: 0.9632012625167399\n",
      "batch loss: 0.8780814409255981 | avg loss: 0.9629393553733826\n",
      "batch loss: 0.8951073288917542 | avg loss: 0.9627312816725186\n",
      "batch loss: 1.1478986740112305 | avg loss: 0.9632975428111692\n",
      "batch loss: 1.0986087322235107 | avg loss: 0.9637100769252311\n",
      "batch loss: 1.1467918157577515 | avg loss: 0.9642665563745701\n",
      "batch loss: 0.8184102773666382 | avg loss: 0.9638245676503037\n",
      "batch loss: 1.0727272033691406 | avg loss: 0.9641535786343485\n",
      "batch loss: 1.1542181968688965 | avg loss: 0.9647260624242117\n",
      "batch loss: 1.0025688409805298 | avg loss: 0.9648397044018582\n",
      "batch loss: 0.9513846635818481 | avg loss: 0.9647994198485049\n",
      "batch loss: 0.985282838344574 | avg loss: 0.964860564381329\n",
      "batch loss: 0.960715651512146 | avg loss: 0.964848228331123\n",
      "batch loss: 0.8272229433059692 | avg loss: 0.9644398446960336\n",
      "batch loss: 0.9935944676399231 | avg loss: 0.9645261009769327\n",
      "batch loss: 0.8030193448066711 | avg loss: 0.9640496798672858\n",
      "batch loss: 1.0620323419570923 | avg loss: 0.9643378641675501\n",
      "batch loss: 0.8246551752090454 | avg loss: 0.9639282375137128\n",
      "batch loss: 0.9280818104743958 | avg loss: 0.9638234233995627\n",
      "batch loss: 1.3089163303375244 | avg loss: 0.9648295251690612\n",
      "batch loss: 1.133878231048584 | avg loss: 0.9653209458256877\n",
      "batch loss: 1.0422464609146118 | avg loss: 0.9655439183331918\n",
      "batch loss: 1.0755552053451538 | avg loss: 0.9658618700297582\n",
      "batch loss: 1.1430468559265137 | avg loss: 0.9663724895856566\n",
      "batch loss: 0.9801515340805054 | avg loss: 0.9664120845411016\n",
      "batch loss: 0.9547564387321472 | avg loss: 0.9663786872751733\n",
      "batch loss: 1.0590275526046753 | avg loss: 0.966643398318972\n",
      "batch loss: 0.9281406998634338 | avg loss: 0.9665337040213777\n",
      "batch loss: 1.0563113689422607 | avg loss: 0.9667887542058121\n",
      "batch loss: 0.969426691532135 | avg loss: 0.9667962271160849\n",
      "batch loss: 0.956464946269989 | avg loss: 0.9667670427069153\n",
      "batch loss: 1.0250394344329834 | avg loss: 0.9669311902892421\n",
      "batch loss: 1.0471099615097046 | avg loss: 0.9671564115567154\n",
      "batch loss: 0.9190537333488464 | avg loss: 0.9670216701611751\n",
      "batch loss: 0.9341354966163635 | avg loss: 0.9669298093412175\n",
      "batch loss: 0.949885368347168 | avg loss: 0.9668823317897021\n",
      "batch loss: 0.9897445440292358 | avg loss: 0.9669458379348119\n",
      "batch loss: 0.9402984380722046 | avg loss: 0.9668720224227271\n",
      "batch loss: 1.1857106685638428 | avg loss: 0.9674765490695257\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.52\n",
      "  Validation took: 0:01:33\n",
      "\n",
      "======= Epoch 4 / 5 =======\n",
      "batch loss: 0.8774302005767822 | avg loss: 0.8774302005767822\n",
      "batch loss: 0.9280283451080322 | avg loss: 0.9027292728424072\n",
      "batch loss: 0.9530504941940308 | avg loss: 0.9195030132929484\n",
      "batch loss: 0.9759251475334167 | avg loss: 0.9336085468530655\n",
      "batch loss: 1.0083775520324707 | avg loss: 0.9485623478889466\n",
      "batch loss: 0.8524342179298401 | avg loss: 0.9325409928957621\n",
      "batch loss: 0.8478062748908997 | avg loss: 0.9204360331807818\n",
      "batch loss: 0.9545231461524963 | avg loss: 0.9246969223022461\n",
      "batch loss: 0.9186649322509766 | avg loss: 0.9240267011854384\n",
      "batch loss: 0.6035650968551636 | avg loss: 0.8919805407524108\n",
      "batch loss: 0.7355383038520813 | avg loss: 0.8777585192160173\n",
      "batch loss: 0.8891435861587524 | avg loss: 0.8787072747945786\n",
      "batch loss: 0.9452177286148071 | avg loss: 0.8838234635499808\n",
      "batch loss: 0.92323899269104 | avg loss: 0.8866388584886279\n",
      "batch loss: 0.8978188037872314 | avg loss: 0.8873841881752014\n",
      "batch loss: 0.9986498355865479 | avg loss: 0.8943382911384106\n",
      "batch loss: 0.9420756101608276 | avg loss: 0.8971463687279645\n",
      "batch loss: 0.612392008304596 | avg loss: 0.8813266820377774\n",
      "batch loss: 0.655876874923706 | avg loss: 0.8694609027159842\n",
      "batch loss: 0.8761959671974182 | avg loss: 0.8697976559400559\n",
      "batch loss: 0.840589702129364 | avg loss: 0.8684068009966895\n",
      "batch loss: 0.7004305720329285 | avg loss: 0.8607715178619731\n",
      "batch loss: 0.8895310163497925 | avg loss: 0.8620219308397045\n",
      "batch loss: 0.9185812473297119 | avg loss: 0.8643785690267881\n",
      "batch loss: 1.167281985282898 | avg loss: 0.8764947056770325\n",
      "batch loss: 0.7120305299758911 | avg loss: 0.8701691604577578\n",
      "batch loss: 0.8264074325561523 | avg loss: 0.8685483557206614\n",
      "batch loss: 0.9346580505371094 | avg loss: 0.8709094162498202\n",
      "batch loss: 0.8370603919029236 | avg loss: 0.8697422085137203\n",
      "batch loss: 0.6768791079521179 | avg loss: 0.8633134384950002\n",
      "batch loss: 0.6165492534637451 | avg loss: 0.855353303493992\n",
      "batch loss: 0.9314714670181274 | avg loss: 0.8577319961041212\n",
      "batch loss: 0.9260727763175964 | avg loss: 0.8598029288378629\n",
      "batch loss: 1.028147578239441 | avg loss: 0.8647542420555564\n",
      "batch loss: 0.6212793588638306 | avg loss: 0.857797816821507\n",
      "batch loss: 0.919699490070343 | avg loss: 0.8595173077450858\n",
      "batch loss: 0.7017706632614136 | avg loss: 0.8552538849212028\n",
      "batch loss: 0.878902018070221 | avg loss: 0.855876204214598\n",
      "batch loss: 0.9158481359481812 | avg loss: 0.8574139460539206\n",
      "batch loss: 0.9512139558792114 | avg loss: 0.859758946299553\n",
      "batch loss: 0.8496610522270203 | avg loss: 0.8595126562002229\n",
      "batch loss: 0.7937031388282776 | avg loss: 0.8579457629294622\n",
      "batch loss: 0.7747958898544312 | avg loss: 0.8560120449509732\n",
      "batch loss: 1.0865544080734253 | avg loss: 0.861251644112847\n",
      "batch loss: 0.98126220703125 | avg loss: 0.8639185455110338\n",
      "batch loss: 0.8716639280319214 | avg loss: 0.8640869233919226\n",
      "batch loss: 0.7391030788421631 | avg loss: 0.8614276926568214\n",
      "batch loss: 0.9186233282089233 | avg loss: 0.8626192683974901\n",
      "batch loss: 0.721847414970398 | avg loss: 0.8597463734295904\n",
      "batch loss: 0.887596607208252 | avg loss: 0.8603033781051636\n",
      "batch loss: 0.8889952301979065 | avg loss: 0.8608659634403154\n",
      "batch loss: 0.6380296945571899 | avg loss: 0.8565806505771784\n",
      "batch loss: 0.4766496419906616 | avg loss: 0.8494121409812063\n",
      "batch loss: 0.5382919311523438 | avg loss: 0.8436506556140052\n",
      "batch loss: 1.0791047811508179 | avg loss: 0.8479316397146746\n",
      "batch loss: 0.7313740849494934 | avg loss: 0.8458502548081535\n",
      "batch loss: 0.718626081943512 | avg loss: 0.8436182517754404\n",
      "batch loss: 0.6224809885025024 | avg loss: 0.8398055403397001\n",
      "batch loss: 0.6928694248199463 | avg loss: 0.8373150977037721\n",
      "batch loss: 0.6749019026756287 | avg loss: 0.8346082111199696\n",
      "batch loss: 0.7586534023284912 | avg loss: 0.8333630503201094\n",
      "batch loss: 0.9293162226676941 | avg loss: 0.8349106821321672\n",
      "batch loss: 0.9323859214782715 | avg loss: 0.8364579081535339\n",
      "batch loss: 0.9159131050109863 | avg loss: 0.8376993956044316\n",
      "batch loss: 0.8216667771339417 | avg loss: 0.8374527399356548\n",
      "batch loss: 0.8633512854576111 | avg loss: 0.837845142140533\n",
      "batch loss: 0.699921727180481 | avg loss: 0.8357865837082934\n",
      "batch loss: 0.907203197479248 | avg loss: 0.8368368280284545\n",
      "batch loss: 0.7963878512382507 | avg loss: 0.836250610973524\n",
      "batch loss: 0.6661887764930725 | avg loss: 0.8338211561952319\n",
      "batch loss: 0.7712727189064026 | avg loss: 0.8329401922897554\n",
      "batch loss: 0.893520176410675 | avg loss: 0.8337815809581015\n",
      "batch loss: 0.9496562480926514 | avg loss: 0.8353689051654241\n",
      "batch loss: 0.7687739729881287 | avg loss: 0.8344689736495147\n",
      "batch loss: 1.2871891260147095 | avg loss: 0.8405052423477173\n",
      "batch loss: 0.7223382592201233 | avg loss: 0.8389504136223542\n",
      "batch loss: 0.7134602069854736 | avg loss: 0.837320670679018\n",
      "batch loss: 1.0003291368484497 | avg loss: 0.8394105228093954\n",
      "batch loss: 0.9642991423606873 | avg loss: 0.8409913914113105\n",
      "batch loss: 0.9437198638916016 | avg loss: 0.8422754973173141\n",
      "batch loss: 0.9502596855163574 | avg loss: 0.8436086354432283\n",
      "batch loss: 1.032805323600769 | avg loss: 0.8459159121280764\n",
      "batch loss: 0.7642942667007446 | avg loss: 0.8449325188096747\n",
      "batch loss: 0.5853151082992554 | avg loss: 0.8418418353512174\n",
      "batch loss: 0.9276912212371826 | avg loss: 0.8428518281263464\n",
      "batch loss: 0.8009271621704102 | avg loss: 0.8423643320105797\n",
      "batch loss: 0.7334502935409546 | avg loss: 0.8411124465109288\n",
      "batch loss: 0.7563223242759705 | avg loss: 0.8401489223946225\n",
      "batch loss: 0.8381103277206421 | avg loss: 0.8401260168364878\n",
      "batch loss: 0.7542027235031128 | avg loss: 0.8391713135772281\n",
      "batch loss: 0.676085889339447 | avg loss: 0.8373791660581317\n",
      "batch loss: 0.7639682292938232 | avg loss: 0.8365812210933022\n",
      "batch loss: 0.8908672332763672 | avg loss: 0.8371649416544105\n",
      "batch loss: 0.9073619246482849 | avg loss: 0.8379117180692389\n",
      "batch loss: 0.8736097812652588 | avg loss: 0.8382874871555127\n",
      "batch loss: 0.9294477105140686 | avg loss: 0.8392370728154978\n",
      "batch loss: 0.7245896458625793 | avg loss: 0.8380551405788697\n",
      "batch loss: 1.1718629598617554 | avg loss: 0.8414613428164501\n",
      "batch loss: 0.9727760553359985 | avg loss: 0.8427877540540214\n",
      "batch loss: 0.936939537525177 | avg loss: 0.8437292718887329\n",
      "batch loss: 1.3851051330566406 | avg loss: 0.8490894289299993\n",
      "batch loss: 1.0325038433074951 | avg loss: 0.850887609463112\n",
      "batch loss: 0.5120118856430054 | avg loss: 0.847597553892043\n",
      "batch loss: 1.1633915901184082 | avg loss: 0.8506340350096042\n",
      "batch loss: 0.9716452360153198 | avg loss: 0.8517865226382301\n",
      "batch loss: 0.8539684414863586 | avg loss: 0.8518071067783067\n",
      "batch loss: 0.9187902808189392 | avg loss: 0.8524331177506491\n",
      "batch loss: 0.8232372999191284 | avg loss: 0.8521627861040609\n",
      "batch loss: 1.1811316013336182 | avg loss: 0.8551808486291028\n",
      "batch loss: 1.0265452861785889 | avg loss: 0.85673870715228\n",
      "batch loss: 0.7672017812728882 | avg loss: 0.8559320681803936\n",
      "batch loss: 1.1348097324371338 | avg loss: 0.858422047325543\n",
      "batch loss: 1.0316364765167236 | avg loss: 0.8599549183803322\n",
      "batch loss: 1.0769600868225098 | avg loss: 0.8618584724894741\n",
      "batch loss: 0.9566506743431091 | avg loss: 0.8626827525055927\n",
      "batch loss: 0.9456194639205933 | avg loss: 0.863397724155722\n",
      "batch loss: 0.8124274611473083 | avg loss: 0.8629620808821458\n",
      "batch loss: 0.7255290746688843 | avg loss: 0.8617973943888131\n",
      "batch loss: 0.8595051169395447 | avg loss: 0.8617781315531049\n",
      "batch loss: 0.8304165005683899 | avg loss: 0.8615167846282323\n",
      "batch loss: 0.9068994522094727 | avg loss: 0.8618918479966723\n",
      "batch loss: 0.7432048320770264 | avg loss: 0.8609190036038883\n",
      "batch loss: 0.8051480650901794 | avg loss: 0.8604655813395492\n",
      "batch loss: 0.9708691239356995 | avg loss: 0.8613559324895183\n",
      "batch loss: 0.599077045917511 | avg loss: 0.8592577013969421\n",
      "batch loss: 1.0580271482467651 | avg loss: 0.8608352366894011\n",
      "batch loss: 1.0910552740097046 | avg loss: 0.8626479928887735\n",
      "batch loss: 0.8134030699729919 | avg loss: 0.862263266928494\n",
      "batch loss: 0.8926690816879272 | avg loss: 0.8624989709188772\n",
      "batch loss: 0.6946383714675903 | avg loss: 0.8612077355384826\n",
      "batch loss: 0.690597653388977 | avg loss: 0.8599053684991734\n",
      "batch loss: 0.8782575726509094 | avg loss: 0.8600444003488078\n",
      "batch loss: 0.7539063692092896 | avg loss: 0.8592463700394881\n",
      "batch loss: 1.1691408157348633 | avg loss: 0.8615590151566178\n",
      "batch loss: 0.9794002771377563 | avg loss: 0.8624319133935151\n",
      "batch loss: 1.0223053693771362 | avg loss: 0.8636074535110417\n",
      "batch loss: 0.5819495916366577 | avg loss: 0.8615515567090389\n",
      "batch loss: 1.051277995109558 | avg loss: 0.8629263859728108\n",
      "batch loss: 0.7952219247817993 | avg loss: 0.8624393035181992\n",
      "batch loss: 1.0810824632644653 | avg loss: 0.8640010403735298\n",
      "batch loss: 0.9144130349159241 | avg loss: 0.8643585722497169\n",
      "batch loss: 0.7177084684371948 | avg loss: 0.8633258250397695\n",
      "batch loss: 0.9526640176773071 | avg loss: 0.8639505676456265\n",
      "batch loss: 1.0060620307922363 | avg loss: 0.8649374528063668\n",
      "batch loss: 0.856345534324646 | avg loss: 0.8648781981961481\n",
      "batch loss: 0.8029300570487976 | avg loss: 0.8644538958595224\n",
      "batch loss: 0.6030499339103699 | avg loss: 0.8626756376149703\n",
      "batch loss: 1.2443945407867432 | avg loss: 0.865254819393158\n",
      "batch loss: 0.6545917987823486 | avg loss: 0.8638409736172464\n",
      "batch loss: 0.9169787168502808 | avg loss: 0.8641952252388001\n",
      "batch loss: 0.9286297559738159 | avg loss: 0.8646219439853896\n",
      "batch loss: 0.7253726720809937 | avg loss: 0.8637058303544396\n",
      "batch loss: 1.108017086982727 | avg loss: 0.8653026359533172\n",
      "batch loss: 0.8077865839004517 | avg loss: 0.8649291550958311\n",
      "batch loss: 0.8630473613739014 | avg loss: 0.8649170144911735\n",
      "batch loss: 0.7880783677101135 | avg loss: 0.8644244590630898\n",
      "batch loss: 1.0172346830368042 | avg loss: 0.8653977725915848\n",
      "batch loss: 0.7821152210235596 | avg loss: 0.8648706678348251\n",
      "batch loss: 1.0224021673202515 | avg loss: 0.8658614319825323\n",
      "batch loss: 0.6283831000328064 | avg loss: 0.8643771924078465\n",
      "batch loss: 0.9352128505706787 | avg loss: 0.8648171654399137\n",
      "batch loss: 1.1040287017822266 | avg loss: 0.8662937798617799\n",
      "batch loss: 1.2618293762207031 | avg loss: 0.8687203786124481\n",
      "batch loss: 0.7919954061508179 | avg loss: 0.8682525434145113\n",
      "batch loss: 0.9432131052017212 | avg loss: 0.8687068498495853\n",
      "batch loss: 0.7325071692466736 | avg loss: 0.8678863698459534\n",
      "batch loss: 1.193047285079956 | avg loss: 0.8698334411946599\n",
      "batch loss: 0.8922799229621887 | avg loss: 0.8699670512051809\n",
      "batch loss: 0.9780552983283997 | avg loss: 0.8706066266319455\n",
      "batch loss: 0.8191091418266296 | avg loss: 0.8703037002507378\n",
      "batch loss: 1.029528260231018 | avg loss: 0.8712348380284003\n",
      "batch loss: 0.8557896614074707 | avg loss: 0.8711450404899065\n",
      "batch loss: 1.034488558769226 | avg loss: 0.8720892226764921\n",
      "batch loss: 0.8549879193305969 | avg loss: 0.8719909393239296\n",
      "batch loss: 0.9327790141105652 | avg loss: 0.8723382997512817\n",
      "batch loss: 0.9939457774162292 | avg loss: 0.8730292513289235\n",
      "batch loss: 0.8162517547607422 | avg loss: 0.872708474512154\n",
      "batch loss: 0.8843210935592651 | avg loss: 0.872773713945003\n",
      "batch loss: 0.9519465565681458 | avg loss: 0.8732160203283725\n",
      "batch loss: 1.1075985431671143 | avg loss: 0.8745181454552544\n",
      "batch loss: 1.015267252922058 | avg loss: 0.8752957648335241\n",
      "batch loss: 0.8354913592338562 | avg loss: 0.8750770593082512\n",
      "batch loss: 0.8501771688461304 | avg loss: 0.8749409943330483\n",
      "batch loss: 0.8791503310203552 | avg loss: 0.8749638711628707\n",
      "batch loss: 0.7551549673080444 | avg loss: 0.874316255466358\n",
      "batch loss: 0.8811085820198059 | avg loss: 0.874352773351054\n",
      "batch loss: 0.9922720193862915 | avg loss: 0.874983357554451\n",
      "batch loss: 0.9902506470680237 | avg loss: 0.8755964814348424\n",
      "batch loss: 0.6316348910331726 | avg loss: 0.8743056793692251\n",
      "batch loss: 0.7694686055183411 | avg loss: 0.8737539052963257\n",
      "batch loss: 0.6948260068893433 | avg loss: 0.8728171100167079\n",
      "batch loss: 0.9245386719703674 | avg loss: 0.8730864931518832\n",
      "batch loss: 0.7876471281051636 | avg loss: 0.8726438021412786\n",
      "batch loss: 1.0308245420455933 | avg loss: 0.8734591667799606\n",
      "batch loss: 0.7633987665176392 | avg loss: 0.872894754470923\n",
      "batch loss: 0.8971474170684814 | avg loss: 0.8730184925454003\n",
      "batch loss: 0.7234463691711426 | avg loss: 0.8722592431881706\n",
      "batch loss: 1.04541015625 | avg loss: 0.873133742749089\n",
      "batch loss: 0.6124210357666016 | avg loss: 0.8718236286436493\n",
      "batch loss: 0.6654013991355896 | avg loss: 0.8707915174961091\n",
      "batch loss: 0.865749716758728 | avg loss: 0.8707664339103509\n",
      "batch loss: 0.6255625486373901 | avg loss: 0.8695525532901878\n",
      "batch loss: 0.8886246085166931 | avg loss: 0.8696465043011558\n",
      "batch loss: 0.7471436858177185 | avg loss: 0.869046000288982\n",
      "batch loss: 0.7750586867332458 | avg loss: 0.8685875255887101\n",
      "batch loss: 0.6429473757743835 | avg loss: 0.8674921850556309\n",
      "batch loss: 1.093808889389038 | avg loss: 0.8685855024678696\n",
      "batch loss: 1.2278640270233154 | avg loss: 0.8703128030666938\n",
      "batch loss: 0.8468325138092041 | avg loss: 0.870200457185079\n",
      "batch loss: 0.6864710450172424 | avg loss: 0.8693255552223751\n",
      "batch loss: 1.0191386938095093 | avg loss: 0.8700355700971956\n",
      "batch loss: 0.6894787549972534 | avg loss: 0.8691838870071015\n",
      "batch loss: 0.7241395711898804 | avg loss: 0.8685029277779127\n",
      "batch loss: 0.5876948237419128 | avg loss: 0.8671907403758753\n",
      "batch loss: 1.139060616493225 | avg loss: 0.868455251427584\n",
      "batch loss: 0.9068275690078735 | avg loss: 0.8686329010460112\n",
      "batch loss: 0.7615875601768494 | avg loss: 0.8681396045443099\n",
      "batch loss: 0.9491304755210876 | avg loss: 0.8685111223010842\n",
      "batch loss: 0.9541089534759521 | avg loss: 0.8689019799776817\n",
      "batch loss: 1.1118993759155273 | avg loss: 0.870006513595581\n",
      "batch loss: 0.8790003061294556 | avg loss: 0.8700472094893995\n",
      "batch loss: 0.634527862071991 | avg loss: 0.8689863115280598\n",
      "batch loss: 0.9426265358924866 | avg loss: 0.869316536749425\n",
      "batch loss: 0.7585226893424988 | avg loss: 0.8688219213592154\n",
      "batch loss: 0.7512465715408325 | avg loss: 0.8682993642489115\n",
      "batch loss: 0.8592289686203003 | avg loss: 0.8682592297549796\n",
      "batch loss: 0.7217289805412292 | avg loss: 0.8676137220491922\n",
      "batch loss: 0.7696640491485596 | avg loss: 0.8671841182206806\n",
      "batch loss: 1.0555013418197632 | avg loss: 0.8680064641752618\n",
      "batch loss: 0.7393302321434021 | avg loss: 0.8674470022968624\n",
      "batch loss: 0.8317461609840393 | avg loss: 0.86729245320027\n",
      "batch loss: 0.5369716882705688 | avg loss: 0.865868656799711\n",
      "batch loss: 0.6464335918426514 | avg loss: 0.8649268754050455\n",
      "batch loss: 1.2486103773117065 | avg loss: 0.8665665484901167\n",
      "batch loss: 0.7617645263671875 | avg loss: 0.8661205824385299\n",
      "batch loss: 1.0668537616729736 | avg loss: 0.8669711467573198\n",
      "batch loss: 0.7904022336006165 | avg loss: 0.8666480711743801\n",
      "batch loss: 0.8624261021614075 | avg loss: 0.8666303318087795\n",
      "batch loss: 1.1596282720565796 | avg loss: 0.8678562646131636\n",
      "batch loss: 0.7704769372940063 | avg loss: 0.8674505174160003\n",
      "batch loss: 0.7930798530578613 | avg loss: 0.8671419254477093\n",
      "batch loss: 1.0609254837036133 | avg loss: 0.867942683952899\n",
      "batch loss: 0.9305592775344849 | avg loss: 0.8682003654079673\n",
      "batch loss: 0.983568549156189 | avg loss: 0.8686731858331649\n",
      "batch loss: 0.8705494403839111 | avg loss: 0.8686808440150047\n",
      "batch loss: 0.8474098443984985 | avg loss: 0.8685943765368888\n",
      "batch loss: 0.8071591258049011 | avg loss: 0.8683456508254233\n",
      "batch loss: 0.8723898530006409 | avg loss: 0.8683619580922588\n",
      "batch loss: 0.7804317474365234 | avg loss: 0.8680088247161314\n",
      "batch loss: 0.6963845491409302 | avg loss: 0.8673223276138305\n",
      "batch loss: 0.790463924407959 | avg loss: 0.867016118836118\n",
      "batch loss: 1.0407793521881104 | avg loss: 0.8677056554764037\n",
      "batch loss: 0.7111568450927734 | avg loss: 0.8670868854748873\n",
      "batch loss: 0.9637040495872498 | avg loss: 0.8674672680107627\n",
      "batch loss: 1.1789798736572266 | avg loss: 0.8686888860721215\n",
      "batch loss: 0.8628016114234924 | avg loss: 0.8686658889055252\n",
      "batch loss: 1.0351433753967285 | avg loss: 0.8693136612265027\n",
      "batch loss: 1.0503767728805542 | avg loss: 0.8700154562329137\n",
      "batch loss: 0.740271806716919 | avg loss: 0.8695145155011917\n",
      "batch loss: 0.9579091668128967 | avg loss: 0.8698544949293137\n",
      "batch loss: 0.9760276079177856 | avg loss: 0.870261288465668\n",
      "batch loss: 0.9460172653198242 | avg loss: 0.870550433415493\n",
      "batch loss: 1.0764071941375732 | avg loss: 0.8713331587414325\n",
      "batch loss: 0.6867403388023376 | avg loss: 0.8706339435143904\n",
      "batch loss: 1.039239764213562 | avg loss: 0.8712701918943873\n",
      "batch loss: 0.6772180795669556 | avg loss: 0.8705406726751113\n",
      "batch loss: 0.9828306436538696 | avg loss: 0.8709612343641703\n",
      "batch loss: 0.6905457973480225 | avg loss: 0.8702880424350056\n",
      "batch loss: 0.7111464738845825 | avg loss: 0.8696964380909519\n",
      "batch loss: 0.7893167734146118 | avg loss: 0.8693987356291877\n",
      "batch loss: 0.9791740775108337 | avg loss: 0.8698038106914816\n",
      "batch loss: 0.8627066016197205 | avg loss: 0.8697777180110707\n",
      "batch loss: 0.8046429753303528 | avg loss: 0.8695391292100424\n",
      "batch loss: 0.9331905841827393 | avg loss: 0.8697714337902348\n",
      "batch loss: 0.9455454349517822 | avg loss: 0.8700469756126403\n",
      "batch loss: 1.0281246900558472 | avg loss: 0.8706197209548259\n",
      "batch loss: 0.6544954776763916 | avg loss: 0.869839489029633\n",
      "batch loss: 0.7933629751205444 | avg loss: 0.8695643936558594\n",
      "batch loss: 0.6345129013061523 | avg loss: 0.868721915188656\n",
      "batch loss: 0.7210662364959717 | avg loss: 0.8681945734790393\n",
      "batch loss: 0.9708802103996277 | avg loss: 0.868560002791924\n",
      "batch loss: 0.9006102681159973 | avg loss: 0.868673656215059\n",
      "batch loss: 0.7846449613571167 | avg loss: 0.8683767350318153\n",
      "batch loss: 0.618282675743103 | avg loss: 0.867496122146996\n",
      "batch loss: 0.9841195940971375 | avg loss: 0.8679053273117333\n",
      "batch loss: 0.9663032293319702 | avg loss: 0.8682493759201957\n",
      "batch loss: 0.9394957423210144 | avg loss: 0.8684976210992926\n",
      "batch loss: 0.8474817276000977 | avg loss: 0.8684246492468648\n",
      "batch loss: 0.9395145773887634 | avg loss: 0.8686706351573904\n",
      "batch loss: 1.028418779373169 | avg loss: 0.8692214908271\n",
      "batch loss: 0.7231190800666809 | avg loss: 0.8687194206870299\n",
      "batch loss: 0.7218769192695618 | avg loss: 0.8682165354082029\n",
      "batch loss: 0.8473125100135803 | avg loss: 0.8681451906116342\n",
      "batch loss: 0.6608801484107971 | avg loss: 0.8674402074748967\n",
      "batch loss: 0.7361540198326111 | avg loss: 0.8669951695506856\n",
      "batch loss: 0.6930923461914062 | avg loss: 0.8664076600123096\n",
      "batch loss: 0.7724924683570862 | avg loss: 0.8660914472457937\n",
      "batch loss: 0.6961753964424133 | avg loss: 0.8655212591558494\n",
      "batch loss: 0.8408505320549011 | avg loss: 0.8654387483628697\n",
      "batch loss: 0.7548377513885498 | avg loss: 0.8650700783729554\n",
      "batch loss: 1.159483790397644 | avg loss: 0.8660481970175556\n",
      "batch loss: 0.7842478156089783 | avg loss: 0.865777334827461\n",
      "batch loss: 0.9786320328712463 | avg loss: 0.8661497925767804\n",
      "batch loss: 0.9094345569610596 | avg loss: 0.8662921766701498\n",
      "batch loss: 0.9944681525230408 | avg loss: 0.8667124257713068\n",
      "batch loss: 0.8800417184829712 | avg loss: 0.8667559855514102\n",
      "batch loss: 0.8888304233551025 | avg loss: 0.8668278892576112\n",
      "batch loss: 0.8978953957557678 | avg loss: 0.8669287577852026\n",
      "batch loss: 0.8184338212013245 | avg loss: 0.8667718162428599\n",
      "batch loss: 0.8835099935531616 | avg loss: 0.8668258103632158\n",
      "batch loss: 0.8383216857910156 | avg loss: 0.8667341572295432\n",
      "batch loss: 0.8172935843467712 | avg loss: 0.8665756938549188\n",
      "batch loss: 0.6279492974281311 | avg loss: 0.8658133092017981\n",
      "batch loss: 0.7206190824508667 | avg loss: 0.865350907205776\n",
      "batch loss: 0.6902930736541748 | avg loss: 0.864795168051644\n",
      "batch loss: 0.7876108288764954 | avg loss: 0.8645509138137479\n",
      "batch loss: 0.6624131798744202 | avg loss: 0.8639132553470623\n",
      "batch loss: 0.9010733962059021 | avg loss: 0.8640301111359266\n",
      "batch loss: 0.6630193591117859 | avg loss: 0.8633999833866347\n",
      "batch loss: 0.9643309116363525 | avg loss: 0.863715392537415\n",
      "batch loss: 0.7525094747543335 | avg loss: 0.8633689566564708\n",
      "batch loss: 0.8673590421676636 | avg loss: 0.8633813482263814\n",
      "batch loss: 0.7515233755111694 | avg loss: 0.8630350387133312\n",
      "batch loss: 0.9556164145469666 | avg loss: 0.863320783700472\n",
      "batch loss: 0.826350212097168 | avg loss: 0.8632070280955388\n",
      "batch loss: 0.7050003409385681 | avg loss: 0.8627217315091678\n",
      "batch loss: 1.0038105249404907 | avg loss: 0.8631531957092635\n",
      "batch loss: 1.1452064514160156 | avg loss: 0.8640131141717841\n",
      "batch loss: 1.2237759828567505 | avg loss: 0.8651066183319208\n",
      "batch loss: 0.7553673386573792 | avg loss: 0.8647740750601798\n",
      "batch loss: 1.0658074617385864 | avg loss: 0.8653814266815647\n",
      "batch loss: 0.9748037457466125 | avg loss: 0.8657110119799534\n",
      "batch loss: 0.950162947177887 | avg loss: 0.8659646213949621\n",
      "batch loss: 0.8143824338912964 | avg loss: 0.8658101837078255\n",
      "batch loss: 1.1148823499679565 | avg loss: 0.8665536827115871\n",
      "batch loss: 0.7802584767341614 | avg loss: 0.8662968517414161\n",
      "batch loss: 0.7302017211914062 | avg loss: 0.865893008624057\n",
      "batch loss: 1.0721778869628906 | avg loss: 0.8665033189150003\n",
      "batch loss: 0.6577709317207336 | avg loss: 0.865887589159265\n",
      "batch loss: 0.9733741283416748 | avg loss: 0.8662037260392133\n",
      "batch loss: 0.7320250868797302 | avg loss: 0.8658102402938775\n",
      "batch loss: 0.7956864237785339 | avg loss: 0.8656051998947099\n",
      "batch loss: 1.2914503812789917 | avg loss: 0.8668467310357719\n",
      "batch loss: 1.0142080783843994 | avg loss: 0.8672751070455064\n",
      "batch loss: 0.781586766242981 | avg loss: 0.8670267350431802\n",
      "batch loss: 0.8964586853981018 | avg loss: 0.8671117984835123\n",
      "batch loss: 0.9691576957702637 | avg loss: 0.8674058788791513\n",
      "batch loss: 0.8481127619743347 | avg loss: 0.8673504388880455\n",
      "batch loss: 0.8699510097503662 | avg loss: 0.8673578903804877\n",
      "batch loss: 0.9146202206611633 | avg loss: 0.8674929256098611\n",
      "batch loss: 0.8324416875839233 | avg loss: 0.8673930645328641\n",
      "batch loss: 1.0151989459991455 | avg loss: 0.8678129676052115\n",
      "batch loss: 0.8815710544586182 | avg loss: 0.8678519423555044\n",
      "batch loss: 0.8379204273223877 | avg loss: 0.8677673900531511\n",
      "batch loss: 0.931237518787384 | avg loss: 0.8679461791481771\n",
      "batch loss: 0.8057500720024109 | avg loss: 0.8677714709820372\n",
      "batch loss: 0.7614713311195374 | avg loss: 0.8674737114866241\n",
      "batch loss: 0.8718128800392151 | avg loss: 0.8674858320691732\n",
      "batch loss: 0.9373500943183899 | avg loss: 0.8676804400420123\n",
      "batch loss: 0.7554957270622253 | avg loss: 0.8673688158392906\n",
      "batch loss: 0.8467293381690979 | avg loss: 0.8673116427709522\n",
      "batch loss: 0.9846039414405823 | avg loss: 0.8676356546457301\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.51\n",
      "  Validation took: 0:01:39\n",
      "\n",
      "======= Epoch 5 / 5 =======\n",
      "batch loss: 0.7018527388572693 | avg loss: 0.7018527388572693\n",
      "batch loss: 0.8622618913650513 | avg loss: 0.7820573151111603\n",
      "batch loss: 1.0805989503860474 | avg loss: 0.8815711935361227\n",
      "batch loss: 0.8662140965461731 | avg loss: 0.8777319192886353\n",
      "batch loss: 1.1239114999771118 | avg loss: 0.9269678354263305\n",
      "batch loss: 0.8773632049560547 | avg loss: 0.9187003970146179\n",
      "batch loss: 0.7084581851959229 | avg loss: 0.8886657953262329\n",
      "batch loss: 0.854058027267456 | avg loss: 0.8843398243188858\n",
      "batch loss: 0.7774304151535034 | avg loss: 0.8724610010782877\n",
      "batch loss: 0.3975335955619812 | avg loss: 0.8249682605266571\n",
      "batch loss: 0.5184965133666992 | avg loss: 0.7971071926030245\n",
      "batch loss: 0.7632032036781311 | avg loss: 0.7942818601926168\n",
      "batch loss: 0.7345848679542542 | avg loss: 0.7896897838665888\n",
      "batch loss: 0.8392228484153748 | avg loss: 0.7932278599057879\n",
      "batch loss: 0.8963488340377808 | avg loss: 0.8001025915145874\n",
      "batch loss: 1.0110276937484741 | avg loss: 0.8132854104042053\n",
      "batch loss: 0.8546394109725952 | avg loss: 0.8157179986729342\n",
      "batch loss: 0.43850967288017273 | avg loss: 0.7947619805733362\n",
      "batch loss: 0.5902613997459412 | avg loss: 0.7839987921087366\n",
      "batch loss: 0.9311652183532715 | avg loss: 0.7913571134209633\n",
      "batch loss: 0.722278356552124 | avg loss: 0.7880676488081614\n",
      "batch loss: 0.5613982677459717 | avg loss: 0.7777644951235164\n",
      "batch loss: 0.7371264696121216 | avg loss: 0.7759976244491079\n",
      "batch loss: 0.8317135572433472 | avg loss: 0.778319121648868\n",
      "batch loss: 1.1821907758712769 | avg loss: 0.7944739878177642\n",
      "batch loss: 0.6129075884819031 | avg loss: 0.787490664766385\n",
      "batch loss: 0.6663281321525574 | avg loss: 0.7830031635584654\n",
      "batch loss: 0.6666397452354431 | avg loss: 0.7788473271897861\n",
      "batch loss: 0.7637557983398438 | avg loss: 0.7783269296432364\n",
      "batch loss: 0.7795178890228271 | avg loss: 0.7783666282892228\n",
      "batch loss: 0.6241326928138733 | avg loss: 0.7733913400480824\n",
      "batch loss: 0.821039617061615 | avg loss: 0.7748803487047553\n",
      "batch loss: 0.7497879266738892 | avg loss: 0.7741199722795775\n",
      "batch loss: 0.9580753445625305 | avg loss: 0.7795304244055468\n",
      "batch loss: 0.533149242401123 | avg loss: 0.7724909620625632\n",
      "batch loss: 0.6268158555030823 | avg loss: 0.7684444313247999\n",
      "batch loss: 0.6557411551475525 | avg loss: 0.765398396833523\n",
      "batch loss: 0.7534484267234802 | avg loss: 0.7650839239358902\n",
      "batch loss: 0.6423145532608032 | avg loss: 0.7619359913544778\n",
      "batch loss: 0.8552680015563965 | avg loss: 0.7642692916095257\n",
      "batch loss: 0.6431770324707031 | avg loss: 0.7613158218744325\n",
      "batch loss: 0.5182434916496277 | avg loss: 0.7555283854405085\n",
      "batch loss: 0.7124342918395996 | avg loss: 0.7545261972172316\n",
      "batch loss: 0.975759744644165 | avg loss: 0.7595542323860255\n",
      "batch loss: 0.795026421546936 | avg loss: 0.760342503256268\n",
      "batch loss: 0.8087798357009888 | avg loss: 0.7613954887441967\n",
      "batch loss: 0.5634617805480957 | avg loss: 0.7571841332506626\n",
      "batch loss: 0.7823984622955322 | avg loss: 0.7577094317724308\n",
      "batch loss: 0.7422329783439636 | avg loss: 0.7573935857840947\n",
      "batch loss: 0.856626570224762 | avg loss: 0.759378245472908\n",
      "batch loss: 0.7353383898735046 | avg loss: 0.7589068757552727\n",
      "batch loss: 0.5732133984565735 | avg loss: 0.7553358473456823\n",
      "batch loss: 0.4571821689605713 | avg loss: 0.7497103062440764\n",
      "batch loss: 0.36890384554862976 | avg loss: 0.7426583347497163\n",
      "batch loss: 0.783291220664978 | avg loss: 0.7433971144936301\n",
      "batch loss: 0.5765907168388367 | avg loss: 0.7404184288212231\n",
      "batch loss: 0.6246242523193359 | avg loss: 0.7383869520404882\n",
      "batch loss: 0.4727373421192169 | avg loss: 0.7338067863521904\n",
      "batch loss: 0.5477009415626526 | avg loss: 0.7306524499998255\n",
      "batch loss: 0.64214688539505 | avg loss: 0.7291773572564125\n",
      "batch loss: 0.8703378438949585 | avg loss: 0.7314914635947494\n",
      "batch loss: 0.9415724277496338 | avg loss: 0.7348798662424088\n",
      "batch loss: 0.9292008280754089 | avg loss: 0.7379643259540437\n",
      "batch loss: 0.7724702954292297 | avg loss: 0.7385034817270935\n",
      "batch loss: 0.8880225419998169 | avg loss: 0.7408037749620584\n",
      "batch loss: 0.7656869292259216 | avg loss: 0.7411807924509048\n",
      "batch loss: 0.5021221041679382 | avg loss: 0.7376127523272785\n",
      "batch loss: 0.8344895839691162 | avg loss: 0.739037411616129\n",
      "batch loss: 0.6896589398384094 | avg loss: 0.738321781590365\n",
      "batch loss: 0.6663784980773926 | avg loss: 0.7372940203973225\n",
      "batch loss: 0.707198977470398 | avg loss: 0.7368701465532813\n",
      "batch loss: 0.7312711477279663 | avg loss: 0.7367923826807075\n",
      "batch loss: 0.9275139570236206 | avg loss: 0.7394050069867748\n",
      "batch loss: 0.6412415504455566 | avg loss: 0.7380784737902719\n",
      "batch loss: 1.0789602994918823 | avg loss: 0.7426235647996267\n",
      "batch loss: 0.8293367624282837 | avg loss: 0.7437645279263195\n",
      "batch loss: 0.5360097885131836 | avg loss: 0.7410664144274476\n",
      "batch loss: 0.8989778757095337 | avg loss: 0.7430909203413205\n",
      "batch loss: 0.8035881519317627 | avg loss: 0.7438567080829717\n",
      "batch loss: 0.8323657512664795 | avg loss: 0.7449630711227655\n",
      "batch loss: 0.7169621586799622 | avg loss: 0.7446173808456938\n",
      "batch loss: 0.7523961067199707 | avg loss: 0.7447122433563558\n",
      "batch loss: 0.6775764226913452 | avg loss: 0.7439033780471388\n",
      "batch loss: 0.49767664074897766 | avg loss: 0.7409721073650178\n",
      "batch loss: 0.889579176902771 | avg loss: 0.7427204258301678\n",
      "batch loss: 0.8598190546035767 | avg loss: 0.7440820377926494\n",
      "batch loss: 0.6627218723297119 | avg loss: 0.7431468634769834\n",
      "batch loss: 0.7232853770256042 | avg loss: 0.7429211647673086\n",
      "batch loss: 0.610473096370697 | avg loss: 0.7414329842235265\n",
      "batch loss: 0.7645184993743896 | avg loss: 0.741689489947425\n",
      "batch loss: 0.650545597076416 | avg loss: 0.7406879087070842\n",
      "batch loss: 0.9410860538482666 | avg loss: 0.7428661494151406\n",
      "batch loss: 0.6663835644721985 | avg loss: 0.7420437560286574\n",
      "batch loss: 0.8598051071166992 | avg loss: 0.7432965363593812\n",
      "batch loss: 0.7704250812530518 | avg loss: 0.7435820999898408\n",
      "batch loss: 0.8020645976066589 | avg loss: 0.7441912926733494\n",
      "batch loss: 0.7497703433036804 | avg loss: 0.744248808659229\n",
      "batch loss: 1.0567768812179565 | avg loss: 0.747437870624114\n",
      "batch loss: 0.7998644113540649 | avg loss: 0.7479674316415883\n",
      "batch loss: 0.8427164554595947 | avg loss: 0.7489149218797684\n",
      "batch loss: 1.1225948333740234 | avg loss: 0.7526147229836719\n",
      "batch loss: 0.8973454833030701 | avg loss: 0.754033652006411\n",
      "batch loss: 0.6477153897285461 | avg loss: 0.753001435867791\n",
      "batch loss: 1.3299952745437622 | avg loss: 0.7585494535473677\n",
      "batch loss: 0.8937153816223145 | avg loss: 0.7598367481004624\n",
      "batch loss: 0.7949987053871155 | avg loss: 0.7601684646786384\n",
      "batch loss: 0.7542681694030762 | avg loss: 0.7601133217321379\n",
      "batch loss: 0.8320982456207275 | avg loss: 0.7607798488051803\n",
      "batch loss: 0.8385934233665466 | avg loss: 0.7614937348103304\n",
      "batch loss: 0.8992968797683716 | avg loss: 0.7627464906735854\n",
      "batch loss: 0.5569298267364502 | avg loss: 0.7608922864939716\n",
      "batch loss: 1.0501461029052734 | avg loss: 0.7634749098547867\n",
      "batch loss: 0.9636663794517517 | avg loss: 0.7652465157804236\n",
      "batch loss: 1.050988793373108 | avg loss: 0.767753026987377\n",
      "batch loss: 0.7910176515579224 | avg loss: 0.7679553280705991\n",
      "batch loss: 0.7832468748092651 | avg loss: 0.7680871517493807\n",
      "batch loss: 0.7239357233047485 | avg loss: 0.7677097891131018\n",
      "batch loss: 0.5320805311203003 | avg loss: 0.7657129309945188\n",
      "batch loss: 0.7048443555831909 | avg loss: 0.7652014303608101\n",
      "batch loss: 0.7064307928085327 | avg loss: 0.7647116750478744\n",
      "batch loss: 0.8441108465194702 | avg loss: 0.7653678665476397\n",
      "batch loss: 0.8171688914299011 | avg loss: 0.7657924651122484\n",
      "batch loss: 0.6536339521408081 | avg loss: 0.7648806072832123\n",
      "batch loss: 1.0449128150939941 | avg loss: 0.7671389315397509\n",
      "batch loss: 0.3874475359916687 | avg loss: 0.7641014003753662\n",
      "batch loss: 0.8504606485366821 | avg loss: 0.7647867912337893\n",
      "batch loss: 0.9426324963569641 | avg loss: 0.766187151116649\n",
      "batch loss: 0.7622869610786438 | avg loss: 0.7661566808819771\n",
      "batch loss: 0.6569939851760864 | avg loss: 0.7653104584346446\n",
      "batch loss: 0.6386756896972656 | avg loss: 0.7643363448289725\n",
      "batch loss: 0.6922593116760254 | avg loss: 0.7637861384690263\n",
      "batch loss: 0.7335001826286316 | avg loss: 0.7635566994096293\n",
      "batch loss: 0.6013867855072021 | avg loss: 0.7623373767487088\n",
      "batch loss: 1.2527172565460205 | avg loss: 0.7659969280904798\n",
      "batch loss: 0.908859133720398 | avg loss: 0.7670551666507015\n",
      "batch loss: 0.994231641292572 | avg loss: 0.7687255819054211\n",
      "batch loss: 0.5040424466133118 | avg loss: 0.7667935882171575\n",
      "batch loss: 0.9866296052932739 | avg loss: 0.7683866028336511\n",
      "batch loss: 0.6654466390609741 | avg loss: 0.767646027554711\n",
      "batch loss: 1.012036919593811 | avg loss: 0.7693916767835617\n",
      "batch loss: 0.8647583723068237 | avg loss: 0.7700680363262798\n",
      "batch loss: 0.5688484311103821 | avg loss: 0.7686509968529285\n",
      "batch loss: 0.8513917922973633 | avg loss: 0.7692296038140783\n",
      "batch loss: 1.053287148475647 | avg loss: 0.7712022256520059\n",
      "batch loss: 0.7871326208114624 | avg loss: 0.7713120904462091\n",
      "batch loss: 0.7109856605529785 | avg loss: 0.770898895720913\n",
      "batch loss: 0.6103678345680237 | avg loss: 0.7698068476858593\n",
      "batch loss: 1.0300707817077637 | avg loss: 0.7715653877806019\n",
      "batch loss: 0.4074532687664032 | avg loss: 0.7691216822838624\n",
      "batch loss: 0.9195401072502136 | avg loss: 0.770124471783638\n",
      "batch loss: 0.9038110971450806 | avg loss: 0.7710098136734489\n",
      "batch loss: 0.7200384736061096 | avg loss: 0.770674475909848\n",
      "batch loss: 1.08877694606781 | avg loss: 0.7727535770219915\n",
      "batch loss: 0.627583920955658 | avg loss: 0.7718109169176647\n",
      "batch loss: 0.8168599009513855 | avg loss: 0.7721015555243338\n",
      "batch loss: 0.6839482188224792 | avg loss: 0.7715364700326552\n",
      "batch loss: 1.1256763935089111 | avg loss: 0.7737921383350518\n",
      "batch loss: 0.6424978375434875 | avg loss: 0.7729611617477634\n",
      "batch loss: 0.948249340057373 | avg loss: 0.7740636031207798\n",
      "batch loss: 0.4969847500324249 | avg loss: 0.7723318602889776\n",
      "batch loss: 0.8875871300697327 | avg loss: 0.7730477315298518\n",
      "batch loss: 0.9510865807533264 | avg loss: 0.7741467367719721\n",
      "batch loss: 1.2144567966461182 | avg loss: 0.7768480254828565\n",
      "batch loss: 0.8538970947265625 | avg loss: 0.7773178368806839\n",
      "batch loss: 0.7241688966751099 | avg loss: 0.7769957220915592\n",
      "batch loss: 0.729680061340332 | avg loss: 0.7767106879906482\n",
      "batch loss: 1.0104395151138306 | avg loss: 0.778110261805757\n",
      "batch loss: 0.9093588590621948 | avg loss: 0.778891503456093\n",
      "batch loss: 0.7910158634185791 | avg loss: 0.778963245231019\n",
      "batch loss: 0.677103579044342 | avg loss: 0.7783640707240386\n",
      "batch loss: 1.0455055236816406 | avg loss: 0.7799263014430888\n",
      "batch loss: 0.8684263229370117 | avg loss: 0.7804408364517744\n",
      "batch loss: 1.0469588041305542 | avg loss: 0.781981402738935\n",
      "batch loss: 0.6951825022697449 | avg loss: 0.7814825584833649\n",
      "batch loss: 0.876250147819519 | avg loss: 0.7820240875652859\n",
      "batch loss: 0.9797438979148865 | avg loss: 0.7831474955786358\n",
      "batch loss: 0.6599711179733276 | avg loss: 0.7824515838407528\n",
      "batch loss: 0.9602988362312317 | avg loss: 0.7834507257081149\n",
      "batch loss: 0.8461024761199951 | avg loss: 0.7838007354869523\n",
      "batch loss: 0.9363994002342224 | avg loss: 0.7846485058466593\n",
      "batch loss: 0.7833759784698486 | avg loss: 0.7846414753086659\n",
      "batch loss: 0.7285084128379822 | avg loss: 0.7843330518884973\n",
      "batch loss: 0.8895795345306396 | avg loss: 0.7849081692799844\n",
      "batch loss: 0.7957757115364075 | avg loss: 0.7849672320096389\n",
      "batch loss: 0.7253289222717285 | avg loss: 0.7846448627678124\n",
      "batch loss: 0.8137531280517578 | avg loss: 0.7848013588177261\n",
      "batch loss: 0.8310519456863403 | avg loss: 0.7850486881592694\n",
      "batch loss: 0.963218629360199 | avg loss: 0.7859964006124659\n",
      "batch loss: 0.6137016415596008 | avg loss: 0.7850847881307047\n",
      "batch loss: 0.6482387185096741 | avg loss: 0.784364545659015\n",
      "batch loss: 0.7154159545898438 | avg loss: 0.7840035582712184\n",
      "batch loss: 0.889336884021759 | avg loss: 0.7845521693428358\n",
      "batch loss: 0.7052885293960571 | avg loss: 0.7841414769078784\n",
      "batch loss: 0.8209469318389893 | avg loss: 0.7843311957477295\n",
      "batch loss: 0.7285037040710449 | avg loss: 0.7840449009186182\n",
      "batch loss: 0.8314014673233032 | avg loss: 0.784286516053336\n",
      "batch loss: 0.560909628868103 | avg loss: 0.7831526232249846\n",
      "batch loss: 0.9908894300460815 | avg loss: 0.7842017990170103\n",
      "batch loss: 0.4738228917121887 | avg loss: 0.7826421060154786\n",
      "batch loss: 0.6138384938240051 | avg loss: 0.7817980879545212\n",
      "batch loss: 0.8720061779022217 | avg loss: 0.7822468844219227\n",
      "batch loss: 0.5277331471443176 | avg loss: 0.7809869154254989\n",
      "batch loss: 0.9437752962112427 | avg loss: 0.78178882863134\n",
      "batch loss: 0.7620069980621338 | avg loss: 0.7816918588736478\n",
      "batch loss: 0.6654256582260132 | avg loss: 0.7811247066753667\n",
      "batch loss: 0.4461081326007843 | avg loss: 0.7794984126264609\n",
      "batch loss: 0.7236984968185425 | avg loss: 0.7792288478157946\n",
      "batch loss: 1.084363579750061 | avg loss: 0.7806958417193248\n",
      "batch loss: 0.7831124663352966 | avg loss: 0.7807074045165304\n",
      "batch loss: 0.5189124941825867 | avg loss: 0.7794607620863687\n",
      "batch loss: 0.8441110849380493 | avg loss: 0.7797671617207369\n",
      "batch loss: 0.6038541793823242 | avg loss: 0.7789373835021595\n",
      "batch loss: 0.600182294845581 | avg loss: 0.7780981577338187\n",
      "batch loss: 0.4276103973388672 | avg loss: 0.7764603644609451\n",
      "batch loss: 0.9960280060768127 | avg loss: 0.7774816093056701\n",
      "batch loss: 0.9966782927513123 | avg loss: 0.7784964087660666\n",
      "batch loss: 0.6906386613845825 | avg loss: 0.7780915343541703\n",
      "batch loss: 1.0139514207839966 | avg loss: 0.7791734604387108\n",
      "batch loss: 0.9908764362335205 | avg loss: 0.7801401406934817\n",
      "batch loss: 1.1865845918655396 | avg loss: 0.7819876154715365\n",
      "batch loss: 0.6679039597511292 | avg loss: 0.7814713998347925\n",
      "batch loss: 0.6218981146812439 | avg loss: 0.7807526012530198\n",
      "batch loss: 0.9145777225494385 | avg loss: 0.7813527139045733\n",
      "batch loss: 0.6493300795555115 | avg loss: 0.7807633271440864\n",
      "batch loss: 0.5492064952850342 | avg loss: 0.7797341856691572\n",
      "batch loss: 0.8980137705802917 | avg loss: 0.7802575466643392\n",
      "batch loss: 0.7145346403121948 | avg loss: 0.7799680184425236\n",
      "batch loss: 0.8415547013282776 | avg loss: 0.7802381354727244\n",
      "batch loss: 0.9273995161056519 | avg loss: 0.7808807615890253\n",
      "batch loss: 0.6532660126686096 | avg loss: 0.7803259148545887\n",
      "batch loss: 0.9122245907783508 | avg loss: 0.7808969047936526\n",
      "batch loss: 0.5319280028343201 | avg loss: 0.7798237629748624\n",
      "batch loss: 0.6256881952285767 | avg loss: 0.7791622369330329\n",
      "batch loss: 0.9963785409927368 | avg loss: 0.7800905117367067\n",
      "batch loss: 0.5303695201873779 | avg loss: 0.7790278692194756\n",
      "batch loss: 0.8210213780403137 | avg loss: 0.7792058078161741\n",
      "batch loss: 0.6552179455757141 | avg loss: 0.7786826522792945\n",
      "batch loss: 0.8107472062110901 | avg loss: 0.7788173772958147\n",
      "batch loss: 1.1559127569198608 | avg loss: 0.7803951822314801\n",
      "batch loss: 0.7778071165084839 | avg loss: 0.780384398624301\n",
      "batch loss: 0.6868667602539062 | avg loss: 0.7799963586310628\n",
      "batch loss: 0.9061203002929688 | avg loss: 0.7805175319437153\n",
      "batch loss: 1.024487018585205 | avg loss: 0.7815215216006761\n",
      "batch loss: 0.7416849136352539 | avg loss: 0.7813582568139327\n",
      "batch loss: 0.7801322937011719 | avg loss: 0.7813532528828602\n",
      "batch loss: 0.9278249740600586 | avg loss: 0.7819486663835805\n",
      "batch loss: 0.5922598838806152 | avg loss: 0.7811806956042162\n",
      "batch loss: 1.0677423477172852 | avg loss: 0.7823361861369302\n",
      "batch loss: 0.6042138338088989 | avg loss: 0.7816208353243679\n",
      "batch loss: 0.6580451130867004 | avg loss: 0.7811265324354172\n",
      "batch loss: 0.7692957520484924 | avg loss: 0.7810793978522024\n",
      "batch loss: 0.8241757154464722 | avg loss: 0.781250414985513\n",
      "batch loss: 0.703933835029602 | avg loss: 0.7809448158552524\n",
      "batch loss: 0.8238701224327087 | avg loss: 0.7811138131252424\n",
      "batch loss: 0.9617547392845154 | avg loss: 0.7818222089141023\n",
      "batch loss: 0.7164282202720642 | avg loss: 0.7815667636459693\n",
      "batch loss: 0.8731153011322021 | avg loss: 0.7819229836361882\n",
      "batch loss: 1.0571415424346924 | avg loss: 0.7829897222361824\n",
      "batch loss: 0.7598526477813721 | avg loss: 0.78290038990238\n",
      "batch loss: 0.8010725975036621 | avg loss: 0.7829702830085388\n",
      "batch loss: 1.014400839805603 | avg loss: 0.7838569901227037\n",
      "batch loss: 0.8034062385559082 | avg loss: 0.7839316055747388\n",
      "batch loss: 1.1764990091323853 | avg loss: 0.7854242572992927\n",
      "batch loss: 0.5639724731445312 | avg loss: 0.784585424783555\n",
      "batch loss: 0.9382568597793579 | avg loss: 0.7851653169910863\n",
      "batch loss: 0.49268367886543274 | avg loss: 0.7840657619605387\n",
      "batch loss: 0.8555622100830078 | avg loss: 0.7843335389197988\n",
      "batch loss: 0.6684677600860596 | avg loss: 0.7839012039241506\n",
      "batch loss: 0.6139476895332336 | avg loss: 0.7832694064728832\n",
      "batch loss: 0.6400697231292725 | avg loss: 0.7827390372753144\n",
      "batch loss: 0.8191925883293152 | avg loss: 0.7828735522238531\n",
      "batch loss: 0.8713560104370117 | avg loss: 0.7831988553790485\n",
      "batch loss: 0.91166090965271 | avg loss: 0.7836694123544099\n",
      "batch loss: 0.946841299533844 | avg loss: 0.7842649301908312\n",
      "batch loss: 0.7056399583816528 | avg loss: 0.7839790212024342\n",
      "batch loss: 0.9898338317871094 | avg loss: 0.7847248719654222\n",
      "batch loss: 0.7131803035736084 | avg loss: 0.7844665883250185\n",
      "batch loss: 0.7541976571083069 | avg loss: 0.7843577072774763\n",
      "batch loss: 0.7143867611885071 | avg loss: 0.7841069153560105\n",
      "batch loss: 0.7193180918693542 | avg loss: 0.783875526700701\n",
      "batch loss: 0.9544574618339539 | avg loss: 0.7844825798506414\n",
      "batch loss: 0.9291290044784546 | avg loss: 0.7849955104344281\n",
      "batch loss: 0.8288219571113586 | avg loss: 0.7851503742036045\n",
      "batch loss: 0.6936011910438538 | avg loss: 0.7848280179248729\n",
      "batch loss: 0.9563555717468262 | avg loss: 0.7854298689909148\n",
      "batch loss: 0.8800230622291565 | avg loss: 0.7857606144218178\n",
      "batch loss: 1.085442066192627 | avg loss: 0.7868048006649216\n",
      "batch loss: 0.7430895566940308 | avg loss: 0.7866530116233561\n",
      "batch loss: 0.839471161365509 | avg loss: 0.7868357733871697\n",
      "batch loss: 0.898655116558075 | avg loss: 0.7872213573291384\n",
      "batch loss: 0.6437330842018127 | avg loss: 0.7867282704799036\n",
      "batch loss: 0.7036523222923279 | avg loss: 0.7864437638080284\n",
      "batch loss: 0.7141847610473633 | avg loss: 0.786197146051166\n",
      "batch loss: 0.6813004612922668 | avg loss: 0.7858403546064079\n",
      "batch loss: 0.7131759524345398 | avg loss: 0.7855940345990455\n",
      "batch loss: 0.6798337697982788 | avg loss: 0.7852367364071511\n",
      "batch loss: 0.6743992567062378 | avg loss: 0.7848635462398079\n",
      "batch loss: 0.544338583946228 | avg loss: 0.7840564154938563\n",
      "batch loss: 0.7369991540908813 | avg loss: 0.7838990333486959\n",
      "batch loss: 0.6303747892379761 | avg loss: 0.7833872858683268\n",
      "batch loss: 0.974990725517273 | avg loss: 0.7840238421462303\n",
      "batch loss: 0.6956783533096313 | avg loss: 0.7837313074149833\n",
      "batch loss: 0.8381312489509583 | avg loss: 0.7839108451758281\n",
      "batch loss: 0.841751754283905 | avg loss: 0.7841011113242099\n",
      "batch loss: 0.802131175994873 | avg loss: 0.7841602262903432\n",
      "batch loss: 1.0631582736968994 | avg loss: 0.7850719846152012\n",
      "batch loss: 0.8097664713859558 | avg loss: 0.7851524226828585\n",
      "batch loss: 0.7554587721824646 | avg loss: 0.7850560147266883\n",
      "batch loss: 0.834112286567688 | avg loss: 0.785214772887986\n",
      "batch loss: 0.7930800914764404 | avg loss: 0.7852401448834327\n",
      "batch loss: 0.7773860096931458 | avg loss: 0.7852148904294446\n",
      "batch loss: 0.7711082100868225 | avg loss: 0.7851696767103977\n",
      "batch loss: 0.5165401101112366 | avg loss: 0.7843114352835634\n",
      "batch loss: 0.7965914607048035 | avg loss: 0.7843505436447775\n",
      "batch loss: 0.7782983183860779 | avg loss: 0.7843313302312579\n",
      "batch loss: 0.6413097381591797 | avg loss: 0.7838787302563462\n",
      "batch loss: 0.7121752500534058 | avg loss: 0.7836525363124883\n",
      "batch loss: 0.7647523283958435 | avg loss: 0.7835931016963983\n",
      "batch loss: 0.6001452803611755 | avg loss: 0.7830180301561624\n",
      "batch loss: 0.7355943918228149 | avg loss: 0.7828698312863708\n",
      "batch loss: 0.7213382720947266 | avg loss: 0.7826781441860853\n",
      "batch loss: 0.7824195027351379 | avg loss: 0.7826773409517656\n",
      "batch loss: 0.6394588947296143 | avg loss: 0.7822339401894679\n",
      "batch loss: 0.8702724575996399 | avg loss: 0.7825056640086351\n",
      "batch loss: 0.6810227036476135 | avg loss: 0.7821934087459858\n",
      "batch loss: 0.5834579467773438 | avg loss: 0.7815837907644868\n",
      "batch loss: 0.7987350821495056 | avg loss: 0.7816362411968569\n",
      "batch loss: 1.1410698890686035 | avg loss: 0.7827320754891489\n",
      "batch loss: 1.0506093502044678 | avg loss: 0.7835462921296209\n",
      "batch loss: 0.6943912506103516 | avg loss: 0.7832761253371383\n",
      "batch loss: 0.920878529548645 | avg loss: 0.7836918425704057\n",
      "batch loss: 0.7550979256629944 | avg loss: 0.7836057163146605\n",
      "batch loss: 1.0223963260650635 | avg loss: 0.7843228052328298\n",
      "batch loss: 0.9546728730201721 | avg loss: 0.7848328353759058\n",
      "batch loss: 0.9849719405174255 | avg loss: 0.7854302655405073\n",
      "batch loss: 0.7231781482696533 | avg loss: 0.7852449913819631\n",
      "batch loss: 0.5935662388801575 | avg loss: 0.7846762117009488\n",
      "batch loss: 0.8512968420982361 | avg loss: 0.7848733141577455\n",
      "batch loss: 0.5116598010063171 | avg loss: 0.7840673745909271\n",
      "batch loss: 0.9517687559127808 | avg loss: 0.7845606139477561\n",
      "batch loss: 0.5566409826278687 | avg loss: 0.7838922279321553\n",
      "batch loss: 0.6743651032447815 | avg loss: 0.7835719731816074\n",
      "batch loss: 1.3916170597076416 | avg loss: 0.7853446993813918\n",
      "batch loss: 0.7815853357315063 | avg loss: 0.7853337709986886\n",
      "batch loss: 0.8370403051376343 | avg loss: 0.7854836450106856\n",
      "batch loss: 0.8832058906555176 | avg loss: 0.7857660792466533\n",
      "batch loss: 1.0887092351913452 | avg loss: 0.7866391142781942\n",
      "batch loss: 0.676971971988678 | avg loss: 0.786323978811845\n",
      "batch loss: 0.6976101398468018 | avg loss: 0.7860697844308564\n",
      "batch loss: 0.9949864745140076 | avg loss: 0.7866666892596653\n",
      "batch loss: 0.776741087436676 | avg loss: 0.7866384111917936\n",
      "batch loss: 1.0766355991363525 | avg loss: 0.7874622668393634\n",
      "batch loss: 0.8483478426933289 | avg loss: 0.7876347472242188\n",
      "batch loss: 0.5892791152000427 | avg loss: 0.7870744205800827\n",
      "batch loss: 0.9473942518234253 | avg loss: 0.7875260257385146\n",
      "batch loss: 0.6050718426704407 | avg loss: 0.7870135139883234\n",
      "batch loss: 0.6377885341644287 | avg loss: 0.7865955168459596\n",
      "batch loss: 0.7606943249702454 | avg loss: 0.7865231671479828\n",
      "batch loss: 0.9160125851631165 | avg loss: 0.786883861905685\n",
      "batch loss: 0.7505533695220947 | avg loss: 0.7867829438712862\n",
      "batch loss: 0.7969151139259338 | avg loss: 0.7868110108243461\n",
      "batch loss: 0.8663442134857178 | avg loss: 0.7870307158040737\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.51\n",
      "  Validation took: 0:01:36\n",
      "\n",
      "Training complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.56      0.56       566\n",
      "           2       0.40      0.36      0.38       463\n",
      "           3       0.55      0.61      0.58       418\n",
      "\n",
      "    accuracy                           0.51      1447\n",
      "   macro avg       0.50      0.51      0.51      1447\n",
      "weighted avg       0.51      0.51      0.51      1447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Definimos la funcion para entrenar el modelo y entregar los resultados en el set de validación\n",
    "#Train model\n",
    "def training(n_epochs, training_dataloader, validation_dataloader):\n",
    "    for epoch_i in range(0, n_epochs):\n",
    "        print('======= Epoch {:} / {:} ======='.format(epoch_i + 1, epochs))\n",
    "        # Mira cuanto tiempo le cuesta entrenar un EPOCH.\n",
    "        t0 = time.time()\n",
    "        # Resetea la perdida para este EPOCH.\n",
    "        total_loss = 0\n",
    "        # Pone el modelo en modo entrenamiento.\n",
    "        model.train()\n",
    "        # Para cada batch en el training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Limpia el gradiente calculado anteriormente\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Genera un paso adelante\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # Saca el loss value fuera del output\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Genera un paso atras\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipea el los gradientes a 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Actualiza los parametros\n",
    "            # ¿take a step using the computed gradient?\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calcula el average loss sobre el training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "        #Validación\n",
    "        # Despues de completar un entrenamiento genera un paso de validacion\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Pone el modelo en modo evaluación\n",
    "        model.eval()\n",
    "\n",
    "        # Trackea las variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evalua el data para un epoch mas\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # El modelo no computa los gradientes\n",
    "            with torch.no_grad():\n",
    "                # Paso adelante \n",
    "                # Devolvemos los loggits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # Los \"logits\" son el valor de salida\n",
    "            # Prioriza aplicar la funcion de activación\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Mueve los logits y labels a la CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Guarda los logits y labels del batch\n",
    "            # Utilizamos esto en la matriz de confusión\n",
    "            predict_labels = np.argmax(logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calcula la precision para este batch\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "            # Accumula la precisión total\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #Print la matriz de confussión\"\n",
    "    conf = confusion_matrix(all_labels, all_logits, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    print(classification_report(all_labels, all_logits, target_names=target_names))\n",
    "\n",
    "#Llamamos a la funcion para entrenar el modelo\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
