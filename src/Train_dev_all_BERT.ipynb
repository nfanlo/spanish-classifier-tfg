{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7234, 2)\n",
      "                                                 review  label\n",
      "0     @dianalaa32 Es una escena de uno de los docume...      2\n",
      "1     Qué feo es tener que terminar con alguien; y m...      0\n",
      "2     Oído en McDonalds \"el mejor mannequin challeng...      0\n",
      "3     Tengo que aceptar que me esta hundiendo el con...      1\n",
      "4     Mmm no quiero hacer spoiler pero hoy va a ver ...      1\n",
      "...                                                 ...    ...\n",
      "7229  @sebatramp Acá también, Seba ???? Para peor el...      0\n",
      "7230  @Phoyu_Agustina no soy hack pero es imposible ...      1\n",
      "7231  Nadie te vende un The Last of Us Remastered po...      0\n",
      "7232  Me propuse dejar las redes, las salidas &amp; ...      1\n",
      "7233  @irenichus siii! Voy como en media hora. Me va...      2\n",
      "\n",
      "[7234 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Modelo BETO\n",
    "#Libreria transformers (modelo BERT predefinido para la clasificación (BertForSequenceClassification))\n",
    "#Libreria sera BERT + Capa de clasificación por encima\n",
    "#Debemos tokenizar nuestro dataset (tokens + attention mask + max_length)\n",
    "\n",
    "import torch\n",
    "from transformers import  BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras \n",
    "from keras import backend as K\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
    "from keras.models import Model\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "MAX_LEN = 38\n",
    "\n",
    "# Select cpu or cuda\n",
    "run_on = 'cpu'\n",
    "device = torch.device(run_on)\n",
    "\n",
    "df = pd.read_csv('/Users/nfanlo/Desktop/Python/DataAnalysis/Datasets TFG/TFG/Task1-train-dev/train_dev/train_dev_all.csv')\n",
    "print(df.shape)\n",
    "df.isnull().sum()\n",
    "df.sentiment.replace(\"P\" , 2 , inplace = True)\n",
    "df.sentiment.replace(\"N\" , 0 , inplace = True)\n",
    "df.sentiment.replace(\"NEU\" , 1, inplace = True)\n",
    "df['review'] = df['text']\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "df['label'] = df['sentiment']\n",
    "df.drop('sentiment', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Environment stopwords\n",
    "stop = set(stopwords.words('spanish'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "#Data cleaning stopwords (ignored)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['review']=df['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 5787\n",
      "Validation set length : 1447\n"
     ]
    }
   ],
   "source": [
    "review = df['review']\n",
    "label = df['label']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(review, label, stratify=label, test_size=0.2)\n",
    "\n",
    "# Report datasets lenghts\n",
    "print('Training set length : {}'.format(len(X_train)))\n",
    "print('Validation set length : {}'.format(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max n°tokens in a sentence: 34\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',\n",
    "            do_lower_case=True)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for doc in dataset:\n",
    "        encoded_doc = tokenizer.encode_plus(doc,\n",
    "                   add_special_tokens=True, max_length=MAX_LEN,\n",
    "                   truncation=True ,pad_to_max_length=True,\n",
    "                   return_token_type_ids = False,\n",
    "                   return_attention_mask = True,)\n",
    "        input_ids.append(encoded_doc['input_ids'])\n",
    "        attention_mask.append(encoded_doc['attention_mask'])\n",
    "    return (torch.tensor(input_ids),\n",
    "           torch.tensor(attention_mask))\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "X_train_inputs, X_train_masks = preprocessing(X_train)\n",
    "X_val_inputs, X_val_masks = preprocessing(X_val)\n",
    "\n",
    "# Report max n° tokens in a sentence\n",
    "max_len = max([torch.sum(sen) for sen in X_train_masks])\n",
    "print('Max n°tokens in a sentence: {0}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "batch_size = 16\n",
    "\n",
    "y_train_labels = torch.tensor(y_train.values)\n",
    "y_val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "def dataloader(x_inputs, x_masks, y_labels):\n",
    "    data = TensorDataset(x_inputs, x_masks, y_labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = dataloader(X_train_inputs, X_train_masks, y_train_labels)\n",
    "val_dataloader = dataloader(X_val_inputs, X_val_masks, y_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nfanlo/Library/Python/3.8/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el modelo + optimizador + definimos EPOCHS + Scheduler\n",
    "#Modelo\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3,\n",
    " output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 4e-5, eps = 1e-6)\n",
    "\n",
    "epochs=5\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps= total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion para formatear el tiempo y otra para calcular la exactitud\n",
    "#fuction to format time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#function to compute accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Training--------------------\n",
      "\n",
      "======= Epoch 1 / 5 =======\n",
      "batch loss: 1.0707027912139893 | avg loss: 1.0707027912139893\n",
      "batch loss: 1.0490219593048096 | avg loss: 1.0598623752593994\n",
      "batch loss: 1.1506094932556152 | avg loss: 1.0901114145914714\n",
      "batch loss: 1.1034897565841675 | avg loss: 1.0934560000896454\n",
      "batch loss: 1.1289029121398926 | avg loss: 1.100545382499695\n",
      "batch loss: 1.0507280826568604 | avg loss: 1.0922424991925557\n",
      "batch loss: 1.1329689025878906 | avg loss: 1.0980605568204607\n",
      "batch loss: 1.1456120014190674 | avg loss: 1.1040044873952866\n",
      "batch loss: 1.0998774766921997 | avg loss: 1.103545930650499\n",
      "batch loss: 1.1421608924865723 | avg loss: 1.1074074268341065\n",
      "batch loss: 1.1078721284866333 | avg loss: 1.1074496724388816\n",
      "batch loss: 1.1055915355682373 | avg loss: 1.1072948276996613\n",
      "batch loss: 1.1357041597366333 | avg loss: 1.1094801609332745\n",
      "batch loss: 0.955872654914856 | avg loss: 1.0985081962176733\n",
      "batch loss: 1.1395277976989746 | avg loss: 1.1012428363164266\n",
      "batch loss: 1.0285471677780151 | avg loss: 1.0966993570327759\n",
      "batch loss: 1.0790120363235474 | avg loss: 1.0956589264028214\n",
      "batch loss: 1.0938318967819214 | avg loss: 1.0955574247572157\n",
      "batch loss: 1.1539630889892578 | avg loss: 1.098631407085218\n",
      "batch loss: 1.3117778301239014 | avg loss: 1.1092887282371522\n",
      "batch loss: 1.0498311519622803 | avg loss: 1.1064574150812059\n",
      "batch loss: 1.2830356359481812 | avg loss: 1.1144836978478865\n",
      "batch loss: 1.0858784914016724 | avg loss: 1.1132399932197903\n",
      "batch loss: 1.0263679027557373 | avg loss: 1.109620322783788\n",
      "batch loss: 1.0805613994598389 | avg loss: 1.10845796585083\n",
      "batch loss: 1.0397800207138062 | avg loss: 1.105816506422483\n",
      "batch loss: 0.9690132141113281 | avg loss: 1.100749717818366\n",
      "batch loss: 1.145432710647583 | avg loss: 1.102345538990838\n",
      "batch loss: 1.2020013332366943 | avg loss: 1.1057819456889713\n",
      "batch loss: 1.0680216550827026 | avg loss: 1.1045232693354288\n",
      "batch loss: 1.1259360313415527 | avg loss: 1.105214003593691\n",
      "batch loss: 1.0293351411819458 | avg loss: 1.102842789143324\n",
      "batch loss: 0.9918099641799927 | avg loss: 1.099478158083829\n",
      "batch loss: 1.1463369131088257 | avg loss: 1.1008563567610348\n",
      "batch loss: 0.9931130409240723 | avg loss: 1.0977779763085502\n",
      "batch loss: 1.1369363069534302 | avg loss: 1.0988657077153523\n",
      "batch loss: 1.0892667770385742 | avg loss: 1.0986062771565206\n",
      "batch loss: 1.097498893737793 | avg loss: 1.0985771354876066\n",
      "batch loss: 1.094847321510315 | avg loss: 1.0984814992317786\n",
      "batch loss: 1.1301871538162231 | avg loss: 1.0992741405963897\n",
      "batch loss: 1.2440948486328125 | avg loss: 1.102806352987522\n",
      "batch loss: 1.062354326248169 | avg loss: 1.1018432094937278\n",
      "batch loss: 1.0815515518188477 | avg loss: 1.101371310478033\n",
      "batch loss: 1.1066101789474487 | avg loss: 1.1014903756705197\n",
      "batch loss: 1.076486349105835 | avg loss: 1.1009347306357489\n",
      "batch loss: 1.1933804750442505 | avg loss: 1.102944420731586\n",
      "batch loss: 1.0891790390014648 | avg loss: 1.102651540269243\n",
      "batch loss: 1.0329068899154663 | avg loss: 1.101198526720206\n",
      "batch loss: 1.0642447471618652 | avg loss: 1.1004443679537093\n",
      "batch loss: 1.0948569774627686 | avg loss: 1.1003326201438903\n",
      "batch loss: 1.1206045150756836 | avg loss: 1.100730108279808\n",
      "batch loss: 1.0537596940994263 | avg loss: 1.0998268310840313\n",
      "batch loss: 1.0622727870941162 | avg loss: 1.099118264216297\n",
      "batch loss: 1.0237857103347778 | avg loss: 1.0977232169221949\n",
      "batch loss: 0.9298136234283447 | avg loss: 1.0946703152223067\n",
      "batch loss: 0.9993346333503723 | avg loss: 1.0929678923317365\n",
      "batch loss: 1.099015712738037 | avg loss: 1.0930739944441277\n",
      "batch loss: 1.1070107221603394 | avg loss: 1.0933142828530278\n",
      "batch loss: 1.0893878936767578 | avg loss: 1.0932477338839386\n",
      "batch loss: 1.1318780183792114 | avg loss: 1.0938915719588598\n",
      "batch loss: 1.0847783088684082 | avg loss: 1.0937421742032787\n",
      "batch loss: 1.1216399669647217 | avg loss: 1.0941921386026567\n",
      "batch loss: 1.0406599044799805 | avg loss: 1.0933424206007094\n",
      "batch loss: 1.0870319604873657 | avg loss: 1.0932438196614385\n",
      "batch loss: 1.048425555229187 | avg loss: 1.0925543079009423\n",
      "batch loss: 1.128039836883545 | avg loss: 1.0930919674309818\n",
      "batch loss: 1.0968670845031738 | avg loss: 1.09314831246191\n",
      "batch loss: 1.20899498462677 | avg loss: 1.0948519399937462\n",
      "batch loss: 1.0785062313079834 | avg loss: 1.094615045664967\n",
      "batch loss: 1.0574133396148682 | avg loss: 1.094083592721394\n",
      "batch loss: 1.0492281913757324 | avg loss: 1.093451826505258\n",
      "batch loss: 1.1294375658035278 | avg loss: 1.0939516284399562\n",
      "batch loss: 1.1471227407455444 | avg loss: 1.0946799998414027\n",
      "batch loss: 1.1149561405181885 | avg loss: 1.0949540017424404\n",
      "batch loss: 1.053433895111084 | avg loss: 1.0944004003206889\n",
      "batch loss: 1.040289044380188 | avg loss: 1.093688408795156\n",
      "batch loss: 1.0465196371078491 | avg loss: 1.0930758273446715\n",
      "batch loss: 1.1212503910064697 | avg loss: 1.0934370396993098\n",
      "batch loss: 1.092406153678894 | avg loss: 1.093423990509178\n",
      "batch loss: 1.1054078340530396 | avg loss: 1.0935737885534764\n",
      "batch loss: 1.0288121700286865 | avg loss: 1.0927742623988492\n",
      "batch loss: 0.9373066425323486 | avg loss: 1.0908783158151114\n",
      "batch loss: 1.0424126386642456 | avg loss: 1.0902943919940167\n",
      "batch loss: 1.0641118288040161 | avg loss: 1.0899826948131834\n",
      "batch loss: 1.1485093832015991 | avg loss: 1.0906712440883413\n",
      "batch loss: 1.2354062795639038 | avg loss: 1.0923542096171268\n",
      "batch loss: 1.112520456314087 | avg loss: 1.0925860055561722\n",
      "batch loss: 1.1382445096969604 | avg loss: 1.0931048521941358\n",
      "batch loss: 0.9688171744346619 | avg loss: 1.0917083614327934\n",
      "batch loss: 0.9753244519233704 | avg loss: 1.0904152068826887\n",
      "batch loss: 1.2150216102600098 | avg loss: 1.0917845080187032\n",
      "batch loss: 1.0075668096542358 | avg loss: 1.090869098253872\n",
      "batch loss: 1.0555917024612427 | avg loss: 1.0904897714173922\n",
      "batch loss: 1.0129575729370117 | avg loss: 1.0896649607952604\n",
      "batch loss: 0.9499298334121704 | avg loss: 1.0881940647175437\n",
      "batch loss: 0.971368134021759 | avg loss: 1.0869771279394627\n",
      "batch loss: 1.1252317428588867 | avg loss: 1.0873715054128588\n",
      "batch loss: 1.142350673675537 | avg loss: 1.0879325173339065\n",
      "batch loss: 1.204853892326355 | avg loss: 1.0891135413237292\n",
      "batch loss: 0.9069177508354187 | avg loss: 1.0872915834188461\n",
      "batch loss: 0.9139739274978638 | avg loss: 1.085575567023589\n",
      "batch loss: 1.2374727725982666 | avg loss: 1.0870647553135366\n",
      "batch loss: 1.106141448020935 | avg loss: 1.0872499659223465\n",
      "batch loss: 0.9576551914215088 | avg loss: 1.0860038623213768\n",
      "batch loss: 1.0157161951065063 | avg loss: 1.0853344559669496\n",
      "batch loss: 0.9457100629806519 | avg loss: 1.0840172447123617\n",
      "batch loss: 1.001372218132019 | avg loss: 1.0832448612863772\n",
      "batch loss: 1.0990521907806396 | avg loss: 1.083391225448361\n",
      "batch loss: 0.986944317817688 | avg loss: 1.0825063914334008\n",
      "batch loss: 1.0005242824554443 | avg loss: 1.0817610995336013\n",
      "batch loss: 0.9245810508728027 | avg loss: 1.0803450630591795\n",
      "batch loss: 1.0637977123260498 | avg loss: 1.0801973188562053\n",
      "batch loss: 0.9737248420715332 | avg loss: 1.0792550845483764\n",
      "batch loss: 1.116837739944458 | avg loss: 1.0795847569641315\n",
      "batch loss: 1.1366829872131348 | avg loss: 1.0800812633141228\n",
      "batch loss: 0.8545608520507812 | avg loss: 1.0781371218377147\n",
      "batch loss: 0.941944420337677 | avg loss: 1.0769730816539536\n",
      "batch loss: 1.2352439165115356 | avg loss: 1.0783143599154585\n",
      "batch loss: 1.227409839630127 | avg loss: 1.0795672631063382\n",
      "batch loss: 1.0897353887557983 | avg loss: 1.0796519974867502\n",
      "batch loss: 0.9896954894065857 | avg loss: 1.0789085552712117\n",
      "batch loss: 1.071400761604309 | avg loss: 1.0788470159788601\n",
      "batch loss: 1.0230247974395752 | avg loss: 1.078393176803744\n",
      "batch loss: 1.1672250032424927 | avg loss: 1.0791095625008307\n",
      "batch loss: 0.9089035391807556 | avg loss: 1.07774791431427\n",
      "batch loss: 1.1882678270339966 | avg loss: 1.0786250564787123\n",
      "batch loss: 1.037867784500122 | avg loss: 1.078304133077306\n",
      "batch loss: 1.025644063949585 | avg loss: 1.0778927262872458\n",
      "batch loss: 0.9782187342643738 | avg loss: 1.0771200596824173\n",
      "batch loss: 1.0497112274169922 | avg loss: 1.0769092225111447\n",
      "batch loss: 0.9646109938621521 | avg loss: 1.0760519841245113\n",
      "batch loss: 0.8886358737945557 | avg loss: 1.07463216510686\n",
      "batch loss: 0.9376657605171204 | avg loss: 1.0736023425159598\n",
      "batch loss: 1.104043960571289 | avg loss: 1.0738295187701041\n",
      "batch loss: 1.0496102571487427 | avg loss: 1.073650116832168\n",
      "batch loss: 1.2897151708602905 | avg loss: 1.0752388304647278\n",
      "batch loss: 0.866428554058075 | avg loss: 1.0737146678632192\n",
      "batch loss: 1.0926460027694702 | avg loss: 1.0738518514494966\n",
      "batch loss: 1.1627628803253174 | avg loss: 1.0744914991392507\n",
      "batch loss: 1.0052471160888672 | avg loss: 1.0739968964031765\n",
      "batch loss: 1.1339137554168701 | avg loss: 1.074421838665685\n",
      "batch loss: 1.0752201080322266 | avg loss: 1.0744274602809423\n",
      "batch loss: 1.1901655197143555 | avg loss: 1.0752368173399172\n",
      "batch loss: 1.2100508213043213 | avg loss: 1.076173025700781\n",
      "batch loss: 1.0700013637542725 | avg loss: 1.076130462514943\n",
      "batch loss: 1.0392112731933594 | avg loss: 1.075877591355206\n",
      "batch loss: 1.0209271907806396 | avg loss: 1.0755037791063997\n",
      "batch loss: 1.115993618965149 | avg loss: 1.0757773591054451\n",
      "batch loss: 1.0497392416000366 | avg loss: 1.075602606638966\n",
      "batch loss: 1.0371218919754028 | avg loss: 1.0753460685412088\n",
      "batch loss: 1.0294289588928223 | avg loss: 1.0750419817223453\n",
      "batch loss: 1.0142786502838135 | avg loss: 1.0746422229628814\n",
      "batch loss: 1.0069447755813599 | avg loss: 1.0741997559865315\n",
      "batch loss: 1.0928796529769897 | avg loss: 1.0743210540189372\n",
      "batch loss: 0.9830144643783569 | avg loss: 1.0737319792470625\n",
      "batch loss: 1.209018349647522 | avg loss: 1.0745991995701423\n",
      "batch loss: 1.2463058233261108 | avg loss: 1.075692872332919\n",
      "batch loss: 1.1375454664230347 | avg loss: 1.0760843444474135\n",
      "batch loss: 0.9803961515426636 | avg loss: 1.0754825319134214\n",
      "batch loss: 0.95900559425354 | avg loss: 1.0747545510530472\n",
      "batch loss: 0.9579926133155823 | avg loss: 1.0740293216261063\n",
      "batch loss: 1.3371144533157349 | avg loss: 1.0756533039204867\n",
      "batch loss: 0.9983395338058472 | avg loss: 1.0751789863124215\n",
      "batch loss: 1.1207152605056763 | avg loss: 1.075456646520917\n",
      "batch loss: 1.046169400215149 | avg loss: 1.0752791480584578\n",
      "batch loss: 0.9615940451622009 | avg loss: 1.0745942980410106\n",
      "batch loss: 1.0619546175003052 | avg loss: 1.074518611331186\n",
      "batch loss: 1.200474739074707 | avg loss: 1.0752683501868021\n",
      "batch loss: 1.064477801322937 | avg loss: 1.0752045007852407\n",
      "batch loss: 0.9831644892692566 | avg loss: 1.0746630889527937\n",
      "batch loss: 1.1091028451919556 | avg loss: 1.0748644910360639\n",
      "batch loss: 0.9124434590339661 | avg loss: 1.07392018271047\n",
      "batch loss: 1.0914520025253296 | avg loss: 1.0740215227093999\n",
      "batch loss: 1.1565110683441162 | avg loss: 1.0744956005578754\n",
      "batch loss: 0.9328576326370239 | avg loss: 1.0736862407411847\n",
      "batch loss: 1.101859450340271 | avg loss: 1.073846315795725\n",
      "batch loss: 0.9071155786514282 | avg loss: 1.072904334229938\n",
      "batch loss: 1.2422517538070679 | avg loss: 1.0738557242275624\n",
      "batch loss: 1.0079703330993652 | avg loss: 1.0734876494167904\n",
      "batch loss: 0.9477722644805908 | avg loss: 1.0727892306115892\n",
      "batch loss: 1.205597162246704 | avg loss: 1.0735229760902363\n",
      "batch loss: 1.1299564838409424 | avg loss: 1.0738330503086468\n",
      "batch loss: 0.8292178511619568 | avg loss: 1.0724963552313425\n",
      "batch loss: 0.9938741326332092 | avg loss: 1.0720690605433092\n",
      "batch loss: 1.130419135093689 | avg loss: 1.0723844663516895\n",
      "batch loss: 1.119181513786316 | avg loss: 1.072636063380908\n",
      "batch loss: 0.9318686723709106 | avg loss: 1.071883296584063\n",
      "batch loss: 1.067339539527893 | avg loss: 1.0718591276635514\n",
      "batch loss: 0.9522213935852051 | avg loss: 1.071226123779539\n",
      "batch loss: 0.9486454129219055 | avg loss: 1.0705809621434463\n",
      "batch loss: 1.0071403980255127 | avg loss: 1.0702488125930907\n",
      "batch loss: 1.0097074508666992 | avg loss: 1.0699334930007656\n",
      "batch loss: 0.8809738159179688 | avg loss: 1.068954427316399\n",
      "batch loss: 1.183030128479004 | avg loss: 1.0695424463945566\n",
      "batch loss: 0.8969706296920776 | avg loss: 1.0686574627191592\n",
      "batch loss: 0.9445984363555908 | avg loss: 1.0680245085030187\n",
      "batch loss: 0.9606378078460693 | avg loss: 1.0674793983473996\n",
      "batch loss: 1.0233598947525024 | avg loss: 1.0672565725716678\n",
      "batch loss: 1.053052306175232 | avg loss: 1.0671851943485702\n",
      "batch loss: 1.100832462310791 | avg loss: 1.0673534306883812\n",
      "batch loss: 1.1842327117919922 | avg loss: 1.0679349196490957\n",
      "batch loss: 1.091037631034851 | avg loss: 1.068049289507441\n",
      "batch loss: 1.2312517166137695 | avg loss: 1.0688532423503294\n",
      "batch loss: 0.9803584814071655 | avg loss: 1.0684194445025688\n",
      "batch loss: 0.9825586676597595 | avg loss: 1.0680006114447989\n",
      "batch loss: 0.9301782250404358 | avg loss: 1.067331570734098\n",
      "batch loss: 1.0013874769210815 | avg loss: 1.067013000232586\n",
      "batch loss: 0.9948020577430725 | avg loss: 1.066665832239848\n",
      "batch loss: 1.020926594734192 | avg loss: 1.0664469842135051\n",
      "batch loss: 0.966218888759613 | avg loss: 1.0659697075684866\n",
      "batch loss: 0.9561717510223389 | avg loss: 1.0654493381061825\n",
      "batch loss: 0.9842507839202881 | avg loss: 1.065066326058136\n",
      "batch loss: 0.8225693106651306 | avg loss: 1.0639278424177931\n",
      "batch loss: 0.8777824640274048 | avg loss: 1.0630580042010156\n",
      "batch loss: 1.0339751243591309 | avg loss: 1.0629227349924486\n",
      "batch loss: 0.9472440481185913 | avg loss: 1.0623871855161808\n",
      "batch loss: 0.9751019477844238 | avg loss: 1.0619849493976012\n",
      "batch loss: 1.0495390892028809 | avg loss: 1.0619278582957907\n",
      "batch loss: 1.141175627708435 | avg loss: 1.062289720256579\n",
      "batch loss: 1.0692839622497559 | avg loss: 1.0623215122656389\n",
      "batch loss: 1.260406255722046 | avg loss: 1.0632178233220027\n",
      "batch loss: 1.0483542680740356 | avg loss: 1.0631508703704353\n",
      "batch loss: 1.3137271404266357 | avg loss: 1.0642745307742747\n",
      "batch loss: 1.0765063762664795 | avg loss: 1.0643291372273649\n",
      "batch loss: 0.9511440396308899 | avg loss: 1.0638260923491585\n",
      "batch loss: 0.9096380472183228 | avg loss: 1.0631438443618537\n",
      "batch loss: 0.9898240566253662 | avg loss: 1.0628208497022218\n",
      "batch loss: 0.9968414902687073 | avg loss: 1.0625314665468115\n",
      "batch loss: 0.9316474199295044 | avg loss: 1.061959920491714\n",
      "batch loss: 0.9143861532211304 | avg loss: 1.0613182954166247\n",
      "batch loss: 0.9070273637771606 | avg loss: 1.0606503693056313\n",
      "batch loss: 0.9190586805343628 | avg loss: 1.0600400603023068\n",
      "batch loss: 1.1054558753967285 | avg loss: 1.0602349779636564\n",
      "batch loss: 0.9574289917945862 | avg loss: 1.059795636142421\n",
      "batch loss: 0.9913848042488098 | avg loss: 1.0595045262194693\n",
      "batch loss: 0.9454909563064575 | avg loss: 1.0590214178723805\n",
      "batch loss: 1.0271629095077515 | avg loss: 1.0588869937864538\n",
      "batch loss: 1.088626503944397 | avg loss: 1.0590119497114872\n",
      "batch loss: 1.1014835834503174 | avg loss: 1.0591896552919842\n",
      "batch loss: 0.9896470308303833 | avg loss: 1.0588998943567276\n",
      "batch loss: 0.8699744343757629 | avg loss: 1.058115971286267\n",
      "batch loss: 0.8726792335510254 | avg loss: 1.0573497037749646\n",
      "batch loss: 0.9571715593338013 | avg loss: 1.0569374480365235\n",
      "batch loss: 0.9546051025390625 | avg loss: 1.0565180531779275\n",
      "batch loss: 0.8730325102806091 | avg loss: 1.055769132594673\n",
      "batch loss: 1.226261019706726 | avg loss: 1.056462189046348\n",
      "batch loss: 1.0135937929153442 | avg loss: 1.0562886327867083\n",
      "batch loss: 1.1455894708633423 | avg loss: 1.0566487168112109\n",
      "batch loss: 1.0698853731155396 | avg loss: 1.0567018760734772\n",
      "batch loss: 1.1839066743850708 | avg loss: 1.0572106952667237\n",
      "batch loss: 0.8752948045730591 | avg loss: 1.056485930761968\n",
      "batch loss: 1.1588495969772339 | avg loss: 1.0568921357866317\n",
      "batch loss: 1.2587273120880127 | avg loss: 1.057689903281894\n",
      "batch loss: 0.9709674715995789 | avg loss: 1.057348476385507\n",
      "batch loss: 1.0380622148513794 | avg loss: 1.057272843987334\n",
      "batch loss: 1.0783332586288452 | avg loss: 1.0573551112320274\n",
      "batch loss: 1.1667869091033936 | avg loss: 1.0577809158930054\n",
      "batch loss: 0.9464364647865295 | avg loss: 1.0573493482530578\n",
      "batch loss: 1.1274062395095825 | avg loss: 1.0576198381806894\n",
      "batch loss: 0.9924927949905396 | avg loss: 1.057369349553035\n",
      "batch loss: 1.0967251062393188 | avg loss: 1.057520137892829\n",
      "batch loss: 0.9802107810974121 | avg loss: 1.0572250640119305\n",
      "batch loss: 0.9923579692840576 | avg loss: 1.0569784210661972\n",
      "batch loss: 1.07144296169281 | avg loss: 1.057033210992813\n",
      "batch loss: 1.06374990940094 | avg loss: 1.0570585570245419\n",
      "batch loss: 0.9788898229598999 | avg loss: 1.0567646896032463\n",
      "batch loss: 0.9910221099853516 | avg loss: 1.0565184627132915\n",
      "batch loss: 0.9292019605636597 | avg loss: 1.0560434011381064\n",
      "batch loss: 0.9672691226005554 | avg loss: 1.0557133852327623\n",
      "batch loss: 1.0292119979858398 | avg loss: 1.0556152319466625\n",
      "batch loss: 0.9844164848327637 | avg loss: 1.0553525059425524\n",
      "batch loss: 1.0514750480651855 | avg loss: 1.0553382505827091\n",
      "batch loss: 0.7594016790390015 | avg loss: 1.0542542338371277\n",
      "batch loss: 1.2788245677947998 | avg loss: 1.0550738335960972\n",
      "batch loss: 1.081567645072937 | avg loss: 1.0551701747287403\n",
      "batch loss: 1.0311245918273926 | avg loss: 1.0550830530515616\n",
      "batch loss: 1.144559621810913 | avg loss: 1.0554060731553858\n",
      "batch loss: 0.9494370222091675 | avg loss: 1.0550248895188887\n",
      "batch loss: 1.0042716264724731 | avg loss: 1.0548429781818047\n",
      "batch loss: 0.9813128709793091 | avg loss: 1.0545803706560817\n",
      "batch loss: 1.0134094953536987 | avg loss: 1.0544338550856105\n",
      "batch loss: 0.8031977415084839 | avg loss: 1.0535429468814363\n",
      "batch loss: 0.9686537981033325 | avg loss: 1.0532429852249765\n",
      "batch loss: 1.0899468660354614 | avg loss: 1.0533722242419148\n",
      "batch loss: 1.030898094177246 | avg loss: 1.0532933676451968\n",
      "batch loss: 1.1560794115066528 | avg loss: 1.05365275940695\n",
      "batch loss: 1.2139930725097656 | avg loss: 1.0542114364560888\n",
      "batch loss: 0.9984716773033142 | avg loss: 1.0540178956256971\n",
      "batch loss: 0.8930009007453918 | avg loss: 1.0534607433942775\n",
      "batch loss: 1.04430091381073 | avg loss: 1.0534291577750239\n",
      "batch loss: 1.1274961233139038 | avg loss: 1.0536836834297967\n",
      "batch loss: 1.013330101966858 | avg loss: 1.0535454862330058\n",
      "batch loss: 0.9053797721862793 | avg loss: 1.0530398012021296\n",
      "batch loss: 1.094032883644104 | avg loss: 1.0531792334553336\n",
      "batch loss: 0.8656608462333679 | avg loss: 1.0525435779054286\n",
      "batch loss: 1.1045989990234375 | avg loss: 1.052719440814611\n",
      "batch loss: 1.086230754852295 | avg loss: 1.0528322735218087\n",
      "batch loss: 0.9742262363433838 | avg loss: 1.052568494873559\n",
      "batch loss: 0.939304769039154 | avg loss: 1.0521896864259521\n",
      "batch loss: 0.9751524329185486 | avg loss: 1.0519328955809275\n",
      "batch loss: 0.9410924315452576 | avg loss: 1.0515646548366229\n",
      "batch loss: 1.0338022708892822 | avg loss: 1.0515058389957377\n",
      "batch loss: 0.9437596797943115 | avg loss: 1.0511502411105844\n",
      "batch loss: 0.8953588008880615 | avg loss: 1.0506377692677473\n",
      "batch loss: 1.133165717124939 | avg loss: 1.0509083527033447\n",
      "batch loss: 0.7678748965263367 | avg loss: 1.0499834067681257\n",
      "batch loss: 1.0144113302230835 | avg loss: 1.0498675368119528\n",
      "batch loss: 1.0035847425460815 | avg loss: 1.0497172679994013\n",
      "batch loss: 0.8525205850601196 | avg loss: 1.0490790910319603\n",
      "batch loss: 0.9304173588752747 | avg loss: 1.0486963112508096\n",
      "batch loss: 0.7824200391769409 | avg loss: 1.047840117449929\n",
      "batch loss: 1.0497486591339111 | avg loss: 1.0478462345707111\n",
      "batch loss: 0.8943726420402527 | avg loss: 1.0473559036041602\n",
      "batch loss: 0.9781996011734009 | avg loss: 1.0471356606027882\n",
      "batch loss: 0.8429763913154602 | avg loss: 1.046487535938384\n",
      "batch loss: 0.7034642100334167 | avg loss: 1.0454020190842543\n",
      "batch loss: 0.9176705479621887 | avg loss: 1.0449990806895475\n",
      "batch loss: 0.9904807209968567 | avg loss: 1.044827639306866\n",
      "batch loss: 0.9064914584159851 | avg loss: 1.0443939835673963\n",
      "batch loss: 1.0169360637664795 | avg loss: 1.0443081775680185\n",
      "batch loss: 0.8977530598640442 | avg loss: 1.0438516195689407\n",
      "batch loss: 0.9665988683700562 | avg loss: 1.0436117041925466\n",
      "batch loss: 0.9208535552024841 | avg loss: 1.043231648003723\n",
      "batch loss: 0.8738760352134705 | avg loss: 1.042708945495111\n",
      "batch loss: 0.7597640156745911 | avg loss: 1.0418383457110478\n",
      "batch loss: 1.1881393194198608 | avg loss: 1.0422871217040197\n",
      "batch loss: 0.7536417245864868 | avg loss: 1.0414044140675747\n",
      "batch loss: 0.9644436836242676 | avg loss: 1.0411697776942719\n",
      "batch loss: 1.0827006101608276 | avg loss: 1.0412960112276048\n",
      "batch loss: 1.1899421215057373 | avg loss: 1.0417464539860235\n",
      "batch loss: 1.063783884048462 | avg loss: 1.0418130323245807\n",
      "batch loss: 1.1139100790023804 | avg loss: 1.0420301921037307\n",
      "batch loss: 1.3508578538894653 | avg loss: 1.0429576024994835\n",
      "batch loss: 0.8445061445236206 | avg loss: 1.0423634364576397\n",
      "batch loss: 1.105304479598999 | avg loss: 1.0425513201685095\n",
      "batch loss: 1.0130666494369507 | avg loss: 1.0424635681722845\n",
      "batch loss: 0.843764066696167 | avg loss: 1.0418739554082606\n",
      "batch loss: 1.018560528755188 | avg loss: 1.0418049807731922\n",
      "batch loss: 0.7480332851409912 | avg loss: 1.0409383976002358\n",
      "batch loss: 1.157429575920105 | avg loss: 1.0412810187129413\n",
      "batch loss: 1.1227691173553467 | avg loss: 1.0415199867441507\n",
      "batch loss: 1.057285189628601 | avg loss: 1.0415660838286083\n",
      "batch loss: 1.1134884357452393 | avg loss: 1.0417757699857995\n",
      "batch loss: 0.8740657567977905 | avg loss: 1.041288240877695\n",
      "batch loss: 0.8023837208747864 | avg loss: 1.0405957640081211\n",
      "batch loss: 0.8185214996337891 | avg loss: 1.0399539308740913\n",
      "batch loss: 1.0281648635864258 | avg loss: 1.0399199566167783\n",
      "batch loss: 1.028042197227478 | avg loss: 1.0398858251242802\n",
      "batch loss: 1.0056120157241821 | avg loss: 1.0397876193666868\n",
      "batch loss: 0.9449043869972229 | avg loss: 1.0395165244170597\n",
      "batch loss: 1.0505727529525757 | avg loss: 1.0395480236436567\n",
      "batch loss: 1.0365537405014038 | avg loss: 1.039539517157457\n",
      "batch loss: 0.8960993885993958 | avg loss: 1.039133171184205\n",
      "batch loss: 0.9212431311607361 | avg loss: 1.0388001484722742\n",
      "batch loss: 1.0850958824157715 | avg loss: 1.0389305589904247\n",
      "batch loss: 0.8638466000556946 | avg loss: 1.0384387501170127\n",
      "batch loss: 0.9420419931411743 | avg loss: 1.0381687311899095\n",
      "batch loss: 1.086961269378662 | avg loss: 1.0383050231960234\n",
      "batch loss: 1.439450740814209 | avg loss: 1.0394224207381353\n",
      "batch loss: 0.8884584903717041 | avg loss: 1.0390030764871174\n",
      "batch loss: 1.039316177368164 | avg loss: 1.0390039438025773\n",
      "batch loss: 1.1072373390197754 | avg loss: 1.0391924338446137\n",
      "\n",
      "  Average training loss: 1.04\n",
      "  Training epoch took: 0:32:26\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.53\n",
      "  Validation took: 0:01:57\n",
      "\n",
      "======= Epoch 2 / 5 =======\n",
      "batch loss: 1.0467464923858643 | avg loss: 1.0467464923858643\n",
      "batch loss: 0.9623393416404724 | avg loss: 1.0045429170131683\n",
      "batch loss: 1.1088591814041138 | avg loss: 1.0393150051434834\n",
      "batch loss: 1.0593011379241943 | avg loss: 1.0443115383386612\n",
      "batch loss: 0.961079478263855 | avg loss: 1.0276651263237\n",
      "batch loss: 0.95026695728302 | avg loss: 1.0147654314835866\n",
      "batch loss: 0.9197580218315125 | avg loss: 1.0011929443904333\n",
      "batch loss: 0.9549729228019714 | avg loss: 0.9954154416918755\n",
      "batch loss: 0.873171329498291 | avg loss: 0.981832762559255\n",
      "batch loss: 1.0270429849624634 | avg loss: 0.9863537847995758\n",
      "batch loss: 1.0125465393066406 | avg loss: 0.988734944300218\n",
      "batch loss: 0.8948442935943604 | avg loss: 0.9809107234080633\n",
      "batch loss: 1.0349419116973877 | avg loss: 0.9850669686610882\n",
      "batch loss: 0.9988740682601929 | avg loss: 0.9860531900610242\n",
      "batch loss: 1.2174807786941528 | avg loss: 1.0014816959698996\n",
      "batch loss: 0.8084418177604675 | avg loss: 0.98941670358181\n",
      "batch loss: 1.017565369606018 | avg loss: 0.991072507465587\n",
      "batch loss: 0.9262144565582275 | avg loss: 0.9874692824151781\n",
      "batch loss: 0.9337671399116516 | avg loss: 0.984642853862361\n",
      "batch loss: 1.1157065629959106 | avg loss: 0.9911960393190384\n",
      "batch loss: 0.8195465803146362 | avg loss: 0.983022255556924\n",
      "batch loss: 1.1238493919372559 | avg loss: 0.9894234890287573\n",
      "batch loss: 0.9888969659805298 | avg loss: 0.9894005967223126\n",
      "batch loss: 0.8984432220458984 | avg loss: 0.9856107061107954\n",
      "batch loss: 0.848118782043457 | avg loss: 0.9801110291481018\n",
      "batch loss: 0.9382613301277161 | avg loss: 0.9785014253396255\n",
      "batch loss: 0.9867101311683655 | avg loss: 0.9788054514814306\n",
      "batch loss: 0.9830100536346436 | avg loss: 0.9789556158440453\n",
      "batch loss: 0.8438783884048462 | avg loss: 0.9742977804151075\n",
      "batch loss: 1.0000871419906616 | avg loss: 0.9751574258009593\n",
      "batch loss: 0.9212248921394348 | avg loss: 0.9734176666505875\n",
      "batch loss: 0.8740589022636414 | avg loss: 0.9703127052634954\n",
      "batch loss: 0.9153885245323181 | avg loss: 0.9686483361504294\n",
      "batch loss: 1.1135069131851196 | avg loss: 0.9729088825338027\n",
      "batch loss: 1.0095950365066528 | avg loss: 0.9739570583615984\n",
      "batch loss: 0.9166942238807678 | avg loss: 0.9723664240704643\n",
      "batch loss: 1.17422616481781 | avg loss: 0.9778220927393114\n",
      "batch loss: 0.9539565443992615 | avg loss: 0.9771940519935206\n",
      "batch loss: 1.118036150932312 | avg loss: 0.980805387863746\n",
      "batch loss: 1.0462205410003662 | avg loss: 0.9824407666921615\n",
      "batch loss: 1.0977505445480347 | avg loss: 0.9852532002984024\n",
      "batch loss: 0.7037121057510376 | avg loss: 0.9785498409044175\n",
      "batch loss: 1.0922471284866333 | avg loss: 0.9811939638714458\n",
      "batch loss: 0.7615994811058044 | avg loss: 0.9762031801722266\n",
      "batch loss: 0.7421088218688965 | avg loss: 0.9710010833210415\n",
      "batch loss: 0.923892617225647 | avg loss: 0.9699769862320112\n",
      "batch loss: 1.1908315420150757 | avg loss: 0.9746760193337786\n",
      "batch loss: 1.129922866821289 | avg loss: 0.977910328656435\n",
      "batch loss: 1.0050734281539917 | avg loss: 0.9784646776257729\n",
      "batch loss: 0.9842668771743774 | avg loss: 0.978580721616745\n",
      "batch loss: 0.8893236517906189 | avg loss: 0.9768305829927033\n",
      "batch loss: 0.9846827387809753 | avg loss: 0.9769815859886316\n",
      "batch loss: 0.819974422454834 | avg loss: 0.9740191866766732\n",
      "batch loss: 0.7878274321556091 | avg loss: 0.9705711912225794\n",
      "batch loss: 0.9186887145042419 | avg loss: 0.9696278734640642\n",
      "batch loss: 0.8579741716384888 | avg loss: 0.967634057360036\n",
      "batch loss: 0.8456116318702698 | avg loss: 0.9654933130531981\n",
      "batch loss: 0.9422587156295776 | avg loss: 0.9650927165458942\n",
      "batch loss: 0.6937978863716125 | avg loss: 0.9604944990853131\n",
      "batch loss: 1.2717068195343018 | avg loss: 0.9656813710927963\n",
      "batch loss: 0.9661828875541687 | avg loss: 0.9656895926741303\n",
      "batch loss: 1.0767673254013062 | avg loss: 0.9674811690084396\n",
      "batch loss: 1.0326259136199951 | avg loss: 0.9685152125737023\n",
      "batch loss: 1.126442790031433 | avg loss: 0.9709828309714794\n",
      "batch loss: 0.703191876411438 | avg loss: 0.9668629701320942\n",
      "batch loss: 1.2071610689163208 | avg loss: 0.9705038504167036\n",
      "batch loss: 0.8105670213699341 | avg loss: 0.9681167335652593\n",
      "batch loss: 1.1285277605056763 | avg loss: 0.9704757192555595\n",
      "batch loss: 0.8925530910491943 | avg loss: 0.9693464058032935\n",
      "batch loss: 1.0660017728805542 | avg loss: 0.97072719676154\n",
      "batch loss: 1.025099277496338 | avg loss: 0.9714930007155512\n",
      "batch loss: 1.3471853733062744 | avg loss: 0.9767109503348669\n",
      "batch loss: 0.9741714000701904 | avg loss: 0.9766761619750768\n",
      "batch loss: 0.9272845983505249 | avg loss: 0.9760087084125828\n",
      "batch loss: 1.0507372617721558 | avg loss: 0.9770050891240438\n",
      "batch loss: 0.9337500333786011 | avg loss: 0.976435943653709\n",
      "batch loss: 0.89427250623703 | avg loss: 0.975368886024921\n",
      "batch loss: 0.9747965335845947 | avg loss: 0.9753615481731219\n",
      "batch loss: 1.151580810546875 | avg loss: 0.9775921717474733\n",
      "batch loss: 0.9878616333007812 | avg loss: 0.9777205400168896\n",
      "batch loss: 0.902073323726654 | avg loss: 0.9767866237663928\n",
      "batch loss: 0.7907881736755371 | avg loss: 0.974518349984797\n",
      "batch loss: 1.1981192827224731 | avg loss: 0.9772123371262148\n",
      "batch loss: 0.8172802925109863 | avg loss: 0.9753083842141288\n",
      "batch loss: 0.7443224787712097 | avg loss: 0.9725909029736238\n",
      "batch loss: 1.3645473718643188 | avg loss: 0.977148536332818\n",
      "batch loss: 0.92665696144104 | avg loss: 0.9765681734030274\n",
      "batch loss: 0.9973729848861694 | avg loss: 0.9768045917153358\n",
      "batch loss: 0.7312900424003601 | avg loss: 0.9740460012735945\n",
      "batch loss: 0.8283789157867432 | avg loss: 0.9724274781015184\n",
      "batch loss: 0.9392232894897461 | avg loss: 0.9720625969079825\n",
      "batch loss: 1.11516535282135 | avg loss: 0.9736180616461713\n",
      "batch loss: 0.9379933476448059 | avg loss: 0.9732350002052963\n",
      "batch loss: 0.8557109832763672 | avg loss: 0.9719847447060525\n",
      "batch loss: 0.8167133927345276 | avg loss: 0.9703503094221416\n",
      "batch loss: 0.743837833404541 | avg loss: 0.967990804463625\n",
      "batch loss: 1.0722355842590332 | avg loss: 0.969065492915124\n",
      "batch loss: 0.8129485845565796 | avg loss: 0.967472463237996\n",
      "batch loss: 0.9780083894729614 | avg loss: 0.9675788867353189\n",
      "batch loss: 0.612165093421936 | avg loss: 0.964024748802185\n",
      "batch loss: 0.7212966084480286 | avg loss: 0.9616214998877874\n",
      "batch loss: 0.9052321314811707 | avg loss: 0.9610686629426246\n",
      "batch loss: 1.0747125148773193 | avg loss: 0.9621720013109226\n",
      "batch loss: 0.7649317979812622 | avg loss: 0.9602754608942912\n",
      "batch loss: 0.6919729113578796 | avg loss: 0.9577201985177539\n",
      "batch loss: 0.8222029805183411 | avg loss: 0.9564417341970047\n",
      "batch loss: 0.7950989007949829 | avg loss: 0.9549338572493223\n",
      "batch loss: 0.8018152117729187 | avg loss: 0.9535160920134297\n",
      "batch loss: 0.9653322100639343 | avg loss: 0.9536244967661867\n",
      "batch loss: 1.0475349426269531 | avg loss: 0.9544782280921936\n",
      "batch loss: 0.656963586807251 | avg loss: 0.9517979160085455\n",
      "batch loss: 1.0883610248565674 | avg loss: 0.9530172294804028\n",
      "batch loss: 0.7222453355789185 | avg loss: 0.9509750003308322\n",
      "batch loss: 0.8274219036102295 | avg loss: 0.9498912012367918\n",
      "batch loss: 1.046274185180664 | avg loss: 0.9507293141406515\n",
      "batch loss: 0.7465487122535706 | avg loss: 0.9489691365381767\n",
      "batch loss: 0.8107308745384216 | avg loss: 0.9477876129313412\n",
      "batch loss: 1.022870659828186 | avg loss: 0.9484239099389415\n",
      "batch loss: 1.0656344890594482 | avg loss: 0.9494088727886937\n",
      "batch loss: 0.806627631187439 | avg loss: 0.9482190291086833\n",
      "batch loss: 1.0495240688323975 | avg loss: 0.9490562608419371\n",
      "batch loss: 0.9801700711250305 | avg loss: 0.9493112920737657\n",
      "batch loss: 0.633450448513031 | avg loss: 0.9467433177358736\n",
      "batch loss: 0.8641122579574585 | avg loss: 0.9460769382215315\n",
      "batch loss: 0.7833224534988403 | avg loss: 0.94477490234375\n",
      "batch loss: 0.9823923110961914 | avg loss: 0.9450734532068646\n",
      "batch loss: 0.8402223587036133 | avg loss: 0.9442478540375476\n",
      "batch loss: 0.8179498314857483 | avg loss: 0.9432611507363617\n",
      "batch loss: 0.8900250792503357 | avg loss: 0.9428484680116639\n",
      "batch loss: 0.9975242614746094 | avg loss: 0.9432690510383019\n",
      "batch loss: 0.8323960900306702 | avg loss: 0.9424226925573276\n",
      "batch loss: 0.7535703182220459 | avg loss: 0.9409919927517573\n",
      "batch loss: 0.9068458080291748 | avg loss: 0.9407352545207605\n",
      "batch loss: 1.0492684841156006 | avg loss: 0.9415452039953488\n",
      "batch loss: 0.8894979953765869 | avg loss: 0.9411596691166914\n",
      "batch loss: 1.0256712436676025 | avg loss: 0.941781077753095\n",
      "batch loss: 0.7622631788253784 | avg loss: 0.9404707281258855\n",
      "batch loss: 1.0515252351760864 | avg loss: 0.941275470930597\n",
      "batch loss: 0.8467442393302917 | avg loss: 0.9405953901277172\n",
      "batch loss: 0.8280854225158691 | avg loss: 0.9397917475019183\n",
      "batch loss: 1.0536751747131348 | avg loss: 0.9405994313828488\n",
      "batch loss: 0.9433882236480713 | avg loss: 0.9406190707649983\n",
      "batch loss: 1.0331192016601562 | avg loss: 0.9412659248272022\n",
      "batch loss: 0.9889998435974121 | avg loss: 0.9415974103742175\n",
      "batch loss: 1.0084645748138428 | avg loss: 0.9420585632324219\n",
      "batch loss: 0.9011340141296387 | avg loss: 0.9417782581015809\n",
      "batch loss: 0.8684953451156616 | avg loss: 0.9412797348839896\n",
      "batch loss: 0.8736415505409241 | avg loss: 0.9408227201249149\n",
      "batch loss: 0.8615207672119141 | avg loss: 0.9402904922530155\n",
      "batch loss: 0.9003087878227234 | avg loss: 0.9400239475568135\n",
      "batch loss: 0.9197551608085632 | avg loss: 0.9398897171809973\n",
      "batch loss: 0.822499692440033 | avg loss: 0.9391174143866489\n",
      "batch loss: 0.8071652054786682 | avg loss: 0.9382549816486883\n",
      "batch loss: 0.6528862714767456 | avg loss: 0.9364019380761431\n",
      "batch loss: 1.0386916399002075 | avg loss: 0.9370618716362984\n",
      "batch loss: 0.9374097585678101 | avg loss: 0.9370641016807312\n",
      "batch loss: 1.1389418840408325 | avg loss: 0.9383499474282477\n",
      "batch loss: 0.9350974559783936 | avg loss: 0.9383293620393246\n",
      "batch loss: 0.8846457600593567 | avg loss: 0.9379917293224694\n",
      "batch loss: 0.7182710766792297 | avg loss: 0.9366184752434492\n",
      "batch loss: 0.7689211964607239 | avg loss: 0.9355768772385875\n",
      "batch loss: 0.9904139637947083 | avg loss: 0.9359153777728846\n",
      "batch loss: 0.5900717973709106 | avg loss: 0.9337936380158173\n",
      "batch loss: 1.247530221939087 | avg loss: 0.9357066659665689\n",
      "batch loss: 0.6828069686889648 | avg loss: 0.9341739405285229\n",
      "batch loss: 0.6587235927581787 | avg loss: 0.9325146010841232\n",
      "batch loss: 0.6693540811538696 | avg loss: 0.9309387895875348\n",
      "batch loss: 1.0369231700897217 | avg loss: 0.9315696489952859\n",
      "batch loss: 0.829645037651062 | avg loss: 0.9309665447861485\n",
      "batch loss: 0.7082351446151733 | avg loss: 0.9296563600792604\n",
      "batch loss: 1.2076525688171387 | avg loss: 0.9312820689022889\n",
      "batch loss: 0.7498421669006348 | avg loss: 0.9302271857511165\n",
      "batch loss: 1.241601586341858 | avg loss: 0.9320270377776526\n",
      "batch loss: 1.0359108448028564 | avg loss: 0.9326240711513607\n",
      "batch loss: 0.6575743556022644 | avg loss: 0.9310523584910801\n",
      "batch loss: 1.08869206905365 | avg loss: 0.9319480386647311\n",
      "batch loss: 0.6552081108093262 | avg loss: 0.930384536247469\n",
      "batch loss: 1.3821322917938232 | avg loss: 0.9329224449864933\n",
      "batch loss: 0.5910958647727966 | avg loss: 0.9310127992869756\n",
      "batch loss: 0.9329504370689392 | avg loss: 0.9310235639413198\n",
      "batch loss: 1.3253984451293945 | avg loss: 0.9332024306882152\n",
      "batch loss: 0.953230082988739 | avg loss: 0.9333124727338225\n",
      "batch loss: 0.5461582541465759 | avg loss: 0.9311968759109414\n",
      "batch loss: 0.8514424562454224 | avg loss: 0.9307634279779766\n",
      "batch loss: 0.8654599189758301 | avg loss: 0.9304104360374245\n",
      "batch loss: 0.8776682019233704 | avg loss: 0.9301268756389618\n",
      "batch loss: 0.6472876071929932 | avg loss: 0.9286143661820315\n",
      "batch loss: 1.0219568014144897 | avg loss: 0.9291108684970978\n",
      "batch loss: 0.8218370079994202 | avg loss: 0.928543281933618\n",
      "batch loss: 0.8087058067321777 | avg loss: 0.9279125583799261\n",
      "batch loss: 1.0601948499679565 | avg loss: 0.9286051358227955\n",
      "batch loss: 0.8323403000831604 | avg loss: 0.9281037564699849\n",
      "batch loss: 0.7633211612701416 | avg loss: 0.9272499606399338\n",
      "batch loss: 0.8734821081161499 | avg loss: 0.9269728067609453\n",
      "batch loss: 0.7639881372451782 | avg loss: 0.9261369879429157\n",
      "batch loss: 0.9552674293518066 | avg loss: 0.9262856126439815\n",
      "batch loss: 0.8227225542068481 | avg loss: 0.925759911839732\n",
      "batch loss: 0.7898709774017334 | avg loss: 0.9250736040900452\n",
      "batch loss: 0.921431303024292 | avg loss: 0.9250553010696143\n",
      "batch loss: 1.0129369497299194 | avg loss: 0.9254947093129158\n",
      "batch loss: 1.0915385484695435 | avg loss: 0.9263207980649388\n",
      "batch loss: 0.7420855164527893 | avg loss: 0.9254087422153737\n",
      "batch loss: 1.0341565608978271 | avg loss: 0.9259444457556814\n",
      "batch loss: 0.9509446024894714 | avg loss: 0.9260669955435921\n",
      "batch loss: 0.8069582581520081 | avg loss: 0.9254859773124137\n",
      "batch loss: 0.8085383176803589 | avg loss: 0.9249182702268212\n",
      "batch loss: 0.9333798289299011 | avg loss: 0.9249591473220051\n",
      "batch loss: 0.7414807677268982 | avg loss: 0.9240770397277979\n",
      "batch loss: 1.195290446281433 | avg loss: 0.9253747115294899\n",
      "batch loss: 0.7254955768585205 | avg loss: 0.9244229061262949\n",
      "batch loss: 0.8420391082763672 | avg loss: 0.9240324615867217\n",
      "batch loss: 0.832482099533081 | avg loss: 0.92360062025628\n",
      "batch loss: 0.73972088098526 | avg loss: 0.9227373350953831\n",
      "batch loss: 0.9739977121353149 | avg loss: 0.9229768695675324\n",
      "batch loss: 0.9001685976982117 | avg loss: 0.9228707845820937\n",
      "batch loss: 0.7743366360664368 | avg loss: 0.9221831264871138\n",
      "batch loss: 0.6292601227760315 | avg loss: 0.9208332508939752\n",
      "batch loss: 0.7417875528335571 | avg loss: 0.9200119403524136\n",
      "batch loss: 0.7348530888557434 | avg loss: 0.9191664661446662\n",
      "batch loss: 0.7654685974121094 | avg loss: 0.9184678394686092\n",
      "batch loss: 1.1279693841934204 | avg loss: 0.9194158102592193\n",
      "batch loss: 0.7951071262359619 | avg loss: 0.9188558612320874\n",
      "batch loss: 1.4225744009017944 | avg loss: 0.9211146887642385\n",
      "batch loss: 0.9315816164016724 | avg loss: 0.9211614161197629\n",
      "batch loss: 0.8865330219268799 | avg loss: 0.9210075121455722\n",
      "batch loss: 0.6868923306465149 | avg loss: 0.9199716042628331\n",
      "batch loss: 0.9724624752998352 | avg loss: 0.9202028415801766\n",
      "batch loss: 0.8184633851051331 | avg loss: 0.9197566158938826\n",
      "batch loss: 0.7289782762527466 | avg loss: 0.9189235227076767\n",
      "batch loss: 0.8427579402923584 | avg loss: 0.9185923680015232\n",
      "batch loss: 0.8719039559364319 | avg loss: 0.9183902536635791\n",
      "batch loss: 0.7405231595039368 | avg loss: 0.9176235851542703\n",
      "batch loss: 1.0111567974090576 | avg loss: 0.9180250152497844\n",
      "batch loss: 0.9007082581520081 | avg loss: 0.9179510120143238\n",
      "batch loss: 0.8991290330886841 | avg loss: 0.9178709184869807\n",
      "batch loss: 0.7357279658317566 | avg loss: 0.9170991263147128\n",
      "batch loss: 0.9428072571754456 | avg loss: 0.9172075994407074\n",
      "batch loss: 1.0734981298446655 | avg loss: 0.9178642823415644\n",
      "batch loss: 0.8890868425369263 | avg loss: 0.917743874643637\n",
      "batch loss: 0.7702178955078125 | avg loss: 0.9171291830639045\n",
      "batch loss: 0.7070190906524658 | avg loss: 0.9162573569543134\n",
      "batch loss: 0.8252135515213013 | avg loss: 0.9158811428822762\n",
      "batch loss: 0.8149460554122925 | avg loss: 0.915465772151947\n",
      "batch loss: 0.9704840183258057 | avg loss: 0.9156912567674137\n",
      "batch loss: 0.7329773306846619 | avg loss: 0.9149454856405452\n",
      "batch loss: 1.1034154891967773 | avg loss: 0.9157116238663836\n",
      "batch loss: 0.6308636665344238 | avg loss: 0.914558393269898\n",
      "batch loss: 0.9533889293670654 | avg loss: 0.9147149680122253\n",
      "batch loss: 0.7396190762519836 | avg loss: 0.9140117716597744\n",
      "batch loss: 1.0510525703430176 | avg loss: 0.9145599348545075\n",
      "batch loss: 0.7353538870811462 | avg loss: 0.9138459665366853\n",
      "batch loss: 0.8832027912139893 | avg loss: 0.9137243666346111\n",
      "batch loss: 1.0234742164611816 | avg loss: 0.9141581605074434\n",
      "batch loss: 0.9342438578605652 | avg loss: 0.9142372380560777\n",
      "batch loss: 0.9858357310295105 | avg loss: 0.9145180164598952\n",
      "batch loss: 0.9985556602478027 | avg loss: 0.9148462885059416\n",
      "batch loss: 1.395760416984558 | avg loss: 0.9167175497062475\n",
      "batch loss: 1.0859575271606445 | avg loss: 0.9173735186111095\n",
      "batch loss: 0.7261383533477783 | avg loss: 0.9166351588996681\n",
      "batch loss: 1.0045589208602905 | avg loss: 0.9169733272149012\n",
      "batch loss: 0.9527059197425842 | avg loss: 0.9171102336996817\n",
      "batch loss: 1.0980274677276611 | avg loss: 0.9178007574936816\n",
      "batch loss: 0.8948792219161987 | avg loss: 0.9177136033660105\n",
      "batch loss: 0.7913010120391846 | avg loss: 0.9172347677928029\n",
      "batch loss: 0.8434278964996338 | avg loss: 0.9169562512973569\n",
      "batch loss: 0.9150431156158447 | avg loss: 0.9169490590579528\n",
      "batch loss: 0.9029197692871094 | avg loss: 0.9168965149015077\n",
      "batch loss: 0.7579342722892761 | avg loss: 0.9163033722051933\n",
      "batch loss: 0.7925006747245789 | avg loss: 0.9158431391290572\n",
      "batch loss: 0.8431020975112915 | avg loss: 0.9155737278638063\n",
      "batch loss: 0.846262514591217 | avg loss: 0.9153179669292211\n",
      "batch loss: 1.134299397468567 | avg loss: 0.9161230457179687\n",
      "batch loss: 0.9220980405807495 | avg loss: 0.9161449321460374\n",
      "batch loss: 0.9617890119552612 | avg loss: 0.9163115163789178\n",
      "batch loss: 0.7317594289779663 | avg loss: 0.915640417879278\n",
      "batch loss: 0.8434051275253296 | avg loss: 0.9153786958127782\n",
      "batch loss: 0.9347630143165588 | avg loss: 0.9154486753019615\n",
      "batch loss: 0.8679777979850769 | avg loss: 0.9152779167504619\n",
      "batch loss: 0.9149635434150696 | avg loss: 0.9152767899643136\n",
      "batch loss: 1.086913824081421 | avg loss: 0.9158897793718747\n",
      "batch loss: 0.8397417664527893 | avg loss: 0.9156187900020558\n",
      "batch loss: 0.8460200428962708 | avg loss: 0.9153719859343048\n",
      "batch loss: 0.6936070322990417 | avg loss: 0.9145883641900107\n",
      "batch loss: 0.9521218538284302 | avg loss: 0.9147205243647938\n",
      "batch loss: 0.8241798877716064 | avg loss: 0.9144028379206072\n",
      "batch loss: 1.1006205081939697 | avg loss: 0.9150539486558287\n",
      "batch loss: 1.0961370468139648 | avg loss: 0.9156849002173554\n",
      "batch loss: 0.8066768050193787 | avg loss: 0.9153063998868068\n",
      "batch loss: 0.8583390116691589 | avg loss: 0.9151092808964343\n",
      "batch loss: 0.8948987722396851 | avg loss: 0.9150395894872732\n",
      "batch loss: 0.9419358372688293 | avg loss: 0.9151320164555946\n",
      "batch loss: 0.9526044726371765 | avg loss: 0.9152603467849836\n",
      "batch loss: 0.7825829982757568 | avg loss: 0.9148075230699351\n",
      "batch loss: 0.9078332781791687 | avg loss: 0.9147838011485379\n",
      "batch loss: 0.7717999219894409 | avg loss: 0.9142991100327443\n",
      "batch loss: 1.0865669250488281 | avg loss: 0.9148810958942851\n",
      "batch loss: 1.028487205505371 | avg loss: 0.9152636080478578\n",
      "batch loss: 0.8625097274780273 | avg loss: 0.9150865816029926\n",
      "batch loss: 0.8006204962730408 | avg loss: 0.91470375188617\n",
      "batch loss: 0.8301365971565247 | avg loss: 0.9144218613704046\n",
      "batch loss: 0.7740788459777832 | avg loss: 0.9139556055053129\n",
      "batch loss: 0.7962262630462646 | avg loss: 0.9135657732455146\n",
      "batch loss: 0.7844312191009521 | avg loss: 0.9131395865981728\n",
      "batch loss: 0.6974533796310425 | avg loss: 0.9124300924963072\n",
      "batch loss: 0.9901533126831055 | avg loss: 0.9126849227264279\n",
      "batch loss: 0.6605861783027649 | avg loss: 0.9118610706204683\n",
      "batch loss: 0.8133388161659241 | avg loss: 0.9115401512248509\n",
      "batch loss: 0.7388647198677063 | avg loss: 0.9109795167074575\n",
      "batch loss: 0.5611443519592285 | avg loss: 0.9098473640707319\n",
      "batch loss: 0.8293559551239014 | avg loss: 0.9095877143644517\n",
      "batch loss: 0.31137922406196594 | avg loss: 0.9076642143956335\n",
      "batch loss: 0.8307848572731018 | avg loss: 0.9074178061997279\n",
      "batch loss: 0.6593028903007507 | avg loss: 0.9066251067879101\n",
      "batch loss: 0.5492367148399353 | avg loss: 0.9054869271957191\n",
      "batch loss: 0.579944372177124 | avg loss: 0.9044534587670886\n",
      "batch loss: 0.4547467529773712 | avg loss: 0.9030303362804123\n",
      "batch loss: 0.6790966987609863 | avg loss: 0.9023239210200986\n",
      "batch loss: 0.7364671230316162 | avg loss: 0.9018023587622732\n",
      "batch loss: 0.8039259910583496 | avg loss: 0.901495536292982\n",
      "batch loss: 1.1625216007232666 | avg loss: 0.9023112427443266\n",
      "batch loss: 0.6935443878173828 | avg loss: 0.9016608787102862\n",
      "batch loss: 0.6256335973739624 | avg loss: 0.9008036511284965\n",
      "batch loss: 0.5724216103553772 | avg loss: 0.8997869884635642\n",
      "batch loss: 0.7024768590927124 | avg loss: 0.8991780065827899\n",
      "batch loss: 0.6167029142379761 | avg loss: 0.8983088524524983\n",
      "batch loss: 0.6071609258651733 | avg loss: 0.8974157606531505\n",
      "batch loss: 0.5936766862869263 | avg loss: 0.8964868949823058\n",
      "batch loss: 1.0613386631011963 | avg loss: 0.8969894918363269\n",
      "batch loss: 1.0515798330307007 | avg loss: 0.8974593712928447\n",
      "batch loss: 1.0132873058319092 | avg loss: 0.8978103650338722\n",
      "batch loss: 0.8788433074951172 | avg loss: 0.8977530627452355\n",
      "batch loss: 0.7141790390014648 | avg loss: 0.8972001289387783\n",
      "batch loss: 1.120185375213623 | avg loss: 0.897869754302967\n",
      "batch loss: 0.6598981618881226 | avg loss: 0.8971572645053179\n",
      "batch loss: 1.0820084810256958 | avg loss: 0.897709059181498\n",
      "batch loss: 0.7858362793922424 | avg loss: 0.8973761044797444\n",
      "batch loss: 0.5526342391967773 | avg loss: 0.8963531315857296\n",
      "batch loss: 1.1255838871002197 | avg loss: 0.8970313290872517\n",
      "batch loss: 0.48898693919181824 | avg loss: 0.8958276583205985\n",
      "batch loss: 1.1275843381881714 | avg loss: 0.8965092956143267\n",
      "batch loss: 1.079620361328125 | avg loss: 0.8970462782117279\n",
      "batch loss: 0.920634388923645 | avg loss: 0.8971152492956809\n",
      "batch loss: 1.1711969375610352 | avg loss: 0.8979143212731309\n",
      "batch loss: 0.6515533924102783 | avg loss: 0.8971981557822505\n",
      "batch loss: 0.6185849905014038 | avg loss: 0.8963905813901321\n",
      "batch loss: 0.5835739970207214 | avg loss: 0.895486487215654\n",
      "batch loss: 1.0113099813461304 | avg loss: 0.8958202725013327\n",
      "batch loss: 0.9273011088371277 | avg loss: 0.8959107346747114\n",
      "batch loss: 0.8067071437835693 | avg loss: 0.8956551369930749\n",
      "batch loss: 0.7020565271377563 | avg loss: 0.8951019981077739\n",
      "batch loss: 0.9185042977333069 | avg loss: 0.8951686713260802\n",
      "batch loss: 0.8353421688079834 | avg loss: 0.8949987096711993\n",
      "batch loss: 0.8346492052078247 | avg loss: 0.8948277479021812\n",
      "batch loss: 0.8053383231163025 | avg loss: 0.8945749529169105\n",
      "batch loss: 0.8242685794830322 | avg loss: 0.8943769067945615\n",
      "batch loss: 0.7706108093261719 | avg loss: 0.8940292492174031\n",
      "batch loss: 0.8120865225791931 | avg loss: 0.8937997177702373\n",
      "batch loss: 0.8941143155097961 | avg loss: 0.8938005965348729\n",
      "batch loss: 0.9780568480491638 | avg loss: 0.8940352936142999\n",
      "batch loss: 0.7115103602409363 | avg loss: 0.893528279910485\n",
      "batch loss: 0.72444087266922 | avg loss: 0.8930598937408416\n",
      "batch loss: 0.5314134359359741 | avg loss: 0.8920608703767398\n",
      "\n",
      "  Average training loss: 0.89\n",
      "  Training epoch took: 0:43:00\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.54\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "======= Epoch 3 / 5 =======\n",
      "batch loss: 0.6887166500091553 | avg loss: 0.6887166500091553\n",
      "batch loss: 0.6946896314620972 | avg loss: 0.6917031407356262\n",
      "batch loss: 1.1621402502059937 | avg loss: 0.848515510559082\n",
      "batch loss: 0.8535908460617065 | avg loss: 0.8497843444347382\n",
      "batch loss: 0.7307901382446289 | avg loss: 0.8259855031967163\n",
      "batch loss: 0.8267397880554199 | avg loss: 0.8261112173398336\n",
      "batch loss: 0.737342119216919 | avg loss: 0.8134299176079887\n",
      "batch loss: 0.7747252583503723 | avg loss: 0.8085918352007866\n",
      "batch loss: 0.7331604361534119 | avg loss: 0.8002105686399672\n",
      "batch loss: 0.8167529702186584 | avg loss: 0.8018648087978363\n",
      "batch loss: 1.0004922151565552 | avg loss: 0.819921845739538\n",
      "batch loss: 0.6688931584358215 | avg loss: 0.8073361217975616\n",
      "batch loss: 0.8949229121208191 | avg loss: 0.8140735672070429\n",
      "batch loss: 0.9767442345619202 | avg loss: 0.8256929005895343\n",
      "batch loss: 1.1521992683410645 | avg loss: 0.8474599917729696\n",
      "batch loss: 0.499798983335495 | avg loss: 0.8257311787456274\n",
      "batch loss: 0.938551127910614 | avg loss: 0.8323676463435677\n",
      "batch loss: 0.7188954949378967 | avg loss: 0.8260636379321417\n",
      "batch loss: 0.7192596197128296 | avg loss: 0.8204423738153357\n",
      "batch loss: 0.8431442975997925 | avg loss: 0.8215774700045586\n",
      "batch loss: 0.7975718975067139 | avg loss: 0.8204343475046612\n",
      "batch loss: 1.146098256111145 | avg loss: 0.8352372524413195\n",
      "batch loss: 0.7776147127151489 | avg loss: 0.8327319246271382\n",
      "batch loss: 0.701046347618103 | avg loss: 0.827245025585095\n",
      "batch loss: 0.7972480058670044 | avg loss: 0.8260451447963715\n",
      "batch loss: 0.8262028694152832 | avg loss: 0.8260512111278681\n",
      "batch loss: 0.7649574279785156 | avg loss: 0.8237884784186328\n",
      "batch loss: 0.7830272912979126 | avg loss: 0.8223327217357499\n",
      "batch loss: 0.7543113231658936 | avg loss: 0.8199871562678238\n",
      "batch loss: 0.7400189638137817 | avg loss: 0.8173215498526891\n",
      "batch loss: 0.8059251308441162 | avg loss: 0.8169539234330577\n",
      "batch loss: 0.6702386140823364 | avg loss: 0.8123690700158477\n",
      "batch loss: 0.8279496431350708 | avg loss: 0.8128412085952181\n",
      "batch loss: 0.9294779300689697 | avg loss: 0.8162717004032696\n",
      "batch loss: 0.8732456564903259 | avg loss: 0.8178995277200426\n",
      "batch loss: 0.618724524974823 | avg loss: 0.8123668887548976\n",
      "batch loss: 0.9131032228469849 | avg loss: 0.8150894923790081\n",
      "batch loss: 0.6750887632369995 | avg loss: 0.8114052626647448\n",
      "batch loss: 1.0554273128509521 | avg loss: 0.817662238310545\n",
      "batch loss: 0.8749353885650635 | avg loss: 0.8190940670669079\n",
      "batch loss: 1.142241358757019 | avg loss: 0.8269757083276423\n",
      "batch loss: 0.667236328125 | avg loss: 0.823172389751389\n",
      "batch loss: 1.1265735626220703 | avg loss: 0.8302282309809397\n",
      "batch loss: 0.5651649236679077 | avg loss: 0.8242040649056435\n",
      "batch loss: 0.4867405891418457 | avg loss: 0.8167048765553369\n",
      "batch loss: 0.6579872369766235 | avg loss: 0.8132544930862344\n",
      "batch loss: 0.9103622436523438 | avg loss: 0.8153206154387048\n",
      "batch loss: 0.8306227326393127 | avg loss: 0.8156394095470508\n",
      "batch loss: 0.8265015482902527 | avg loss: 0.8158610858479325\n",
      "batch loss: 0.7400656938552856 | avg loss: 0.8143451780080795\n",
      "batch loss: 0.7493724226951599 | avg loss: 0.8130712024137086\n",
      "batch loss: 0.7365109324455261 | avg loss: 0.811598889529705\n",
      "batch loss: 0.6869080662727356 | avg loss: 0.8092462324871207\n",
      "batch loss: 0.6256780028343201 | avg loss: 0.8058468208268836\n",
      "batch loss: 0.594937264919281 | avg loss: 0.8020121016285636\n",
      "batch loss: 0.5647012591362 | avg loss: 0.7977744080126286\n",
      "batch loss: 0.6576040387153625 | avg loss: 0.7953152787267116\n",
      "batch loss: 0.8661153316497803 | avg loss: 0.7965359692943508\n",
      "batch loss: 0.40167784690856934 | avg loss: 0.7898434587454392\n",
      "batch loss: 1.0698753595352173 | avg loss: 0.7945106570919355\n",
      "batch loss: 0.7738761901855469 | avg loss: 0.7941723871426504\n",
      "batch loss: 1.0439743995666504 | avg loss: 0.7982014518591666\n",
      "batch loss: 0.7807809114456177 | avg loss: 0.7979249353446658\n",
      "batch loss: 1.34890878200531 | avg loss: 0.8065340579487383\n",
      "batch loss: 0.5469804406166077 | avg loss: 0.8025409253743979\n",
      "batch loss: 1.0614819526672363 | avg loss: 0.8064642742727742\n",
      "batch loss: 0.6992886662483215 | avg loss: 0.8048646383321107\n",
      "batch loss: 0.9115631580352783 | avg loss: 0.8064337342100985\n",
      "batch loss: 0.7069783806800842 | avg loss: 0.8049923522748809\n",
      "batch loss: 0.9914036393165588 | avg loss: 0.8076553706611905\n",
      "batch loss: 0.9177405834197998 | avg loss: 0.8092058666155372\n",
      "batch loss: 1.2204232215881348 | avg loss: 0.8149172187679343\n",
      "batch loss: 0.8422265648841858 | avg loss: 0.8152913193996638\n",
      "batch loss: 0.9117504954338074 | avg loss: 0.8165948217785036\n",
      "batch loss: 0.9213188886642456 | avg loss: 0.8179911426703135\n",
      "batch loss: 0.6324485540390015 | avg loss: 0.8155497928199015\n",
      "batch loss: 0.9165544509887695 | avg loss: 0.8168615416272894\n",
      "batch loss: 0.8036937713623047 | avg loss: 0.8166927240597897\n",
      "batch loss: 1.102729082107544 | avg loss: 0.8203134374527992\n",
      "batch loss: 0.6246287226676941 | avg loss: 0.8178673785179853\n",
      "batch loss: 0.8932572603225708 | avg loss: 0.8187981177995234\n",
      "batch loss: 0.5979474186897278 | avg loss: 0.8161048165908674\n",
      "batch loss: 0.8600187301635742 | avg loss: 0.8166338998869241\n",
      "batch loss: 0.6408513784408569 | avg loss: 0.81454125082209\n",
      "batch loss: 0.6356936097145081 | avg loss: 0.8124371609267067\n",
      "batch loss: 1.1609752178192139 | avg loss: 0.8164899290301079\n",
      "batch loss: 0.7153910398483276 | avg loss: 0.8153278728326162\n",
      "batch loss: 0.9237263798713684 | avg loss: 0.8165596740489657\n",
      "batch loss: 0.5717360377311707 | avg loss: 0.8138088466746084\n",
      "batch loss: 0.6698774099349976 | avg loss: 0.8122096084886127\n",
      "batch loss: 0.672178328037262 | avg loss: 0.8106708032089275\n",
      "batch loss: 1.0342838764190674 | avg loss: 0.8131013800916465\n",
      "batch loss: 0.7568299770355225 | avg loss: 0.8124963112415806\n",
      "batch loss: 0.5446585416793823 | avg loss: 0.8096469732675147\n",
      "batch loss: 0.5388993620872498 | avg loss: 0.8067969984129856\n",
      "batch loss: 0.537045955657959 | avg loss: 0.8039870917176207\n",
      "batch loss: 0.9930759072303772 | avg loss: 0.8059364609497109\n",
      "batch loss: 0.6335943341255188 | avg loss: 0.8041778678188518\n",
      "batch loss: 0.7882802486419678 | avg loss: 0.8040172858069642\n",
      "batch loss: 0.509545624256134 | avg loss: 0.8010725691914559\n",
      "batch loss: 0.49809789657592773 | avg loss: 0.7980728199576387\n",
      "batch loss: 0.6082010269165039 | avg loss: 0.7962113317905688\n",
      "batch loss: 0.8421441316604614 | avg loss: 0.7966572813038687\n",
      "batch loss: 0.5293842554092407 | avg loss: 0.7940873483625742\n",
      "batch loss: 0.5690996646881104 | avg loss: 0.7919446085180555\n",
      "batch loss: 0.5957453846931458 | avg loss: 0.7900936724442356\n",
      "batch loss: 0.5769413709640503 | avg loss: 0.7881015948603086\n",
      "batch loss: 0.6746363639831543 | avg loss: 0.7870509908707054\n",
      "batch loss: 0.6386141777038574 | avg loss: 0.7856891852453214\n",
      "batch loss: 0.7921088933944702 | avg loss: 0.7857475462284955\n",
      "batch loss: 0.33501023054122925 | avg loss: 0.7816868496907724\n",
      "batch loss: 0.8423575758934021 | avg loss: 0.7822285526032958\n",
      "batch loss: 0.31376639008522034 | avg loss: 0.7780828697491536\n",
      "batch loss: 0.6102403998374939 | avg loss: 0.7766105673815075\n",
      "batch loss: 1.1116327047348022 | avg loss: 0.7795238033584927\n",
      "batch loss: 0.557769775390625 | avg loss: 0.7776121307035972\n",
      "batch loss: 0.552309513092041 | avg loss: 0.7756864673052078\n",
      "batch loss: 1.0156079530715942 | avg loss: 0.7777197002354315\n",
      "batch loss: 0.9347221851348877 | avg loss: 0.779039048848032\n",
      "batch loss: 0.5821128487586975 | avg loss: 0.7773979971806209\n",
      "batch loss: 0.7725728154182434 | avg loss: 0.7773581196453946\n",
      "batch loss: 0.9948716759681702 | avg loss: 0.7791410176480402\n",
      "batch loss: 0.5193623304367065 | avg loss: 0.7770289958007937\n",
      "batch loss: 0.8750079870223999 | avg loss: 0.7778191489558066\n",
      "batch loss: 0.5755352973937988 | avg loss: 0.7762008781433105\n",
      "batch loss: 1.0425227880477905 | avg loss: 0.7783145440949334\n",
      "batch loss: 0.659371018409729 | avg loss: 0.7773779809005618\n",
      "batch loss: 0.6878402233123779 | avg loss: 0.776678467169404\n",
      "batch loss: 0.9658202528953552 | avg loss: 0.7781446825626285\n",
      "batch loss: 0.8087822198867798 | avg loss: 0.7783803559266604\n",
      "batch loss: 0.6273882389068604 | avg loss: 0.7772277443463566\n",
      "batch loss: 0.5771234631538391 | avg loss: 0.7757118028221708\n",
      "batch loss: 0.7127044200897217 | avg loss: 0.775238063102378\n",
      "batch loss: 0.7860276103019714 | avg loss: 0.7753185821113302\n",
      "batch loss: 0.8521185517311096 | avg loss: 0.7758874707751804\n",
      "batch loss: 0.8807960748672485 | avg loss: 0.776658857569975\n",
      "batch loss: 0.588976263999939 | avg loss: 0.7752889116315076\n",
      "batch loss: 0.9325653314590454 | avg loss: 0.7764285958331564\n",
      "batch loss: 0.8075910210609436 | avg loss: 0.7766527859427088\n",
      "batch loss: 0.6665047407150269 | avg loss: 0.7758660141910826\n",
      "batch loss: 1.0289021730422974 | avg loss: 0.7776605968779706\n",
      "batch loss: 0.7311464548110962 | avg loss: 0.777333032497218\n",
      "batch loss: 0.8436537384986877 | avg loss: 0.7777968136580674\n",
      "batch loss: 0.7864782810211182 | avg loss: 0.7778571016258664\n",
      "batch loss: 0.9080814123153687 | avg loss: 0.7787552003202767\n",
      "batch loss: 0.8671362996101379 | avg loss: 0.7793605503154127\n",
      "batch loss: 0.589747428894043 | avg loss: 0.7780706651356756\n",
      "batch loss: 0.5935841202735901 | avg loss: 0.7768241344271479\n",
      "batch loss: 0.5721358060836792 | avg loss: 0.7754503872570575\n",
      "batch loss: 0.6795128583908081 | avg loss: 0.7748108037312825\n",
      "batch loss: 0.7792640328407288 | avg loss: 0.7748402953147888\n",
      "batch loss: 0.6750070452690125 | avg loss: 0.7741834976171192\n",
      "batch loss: 0.6906874775886536 | avg loss: 0.7736377719960181\n",
      "batch loss: 0.5177925825119019 | avg loss: 0.7719764395967706\n",
      "batch loss: 0.7088238000869751 | avg loss: 0.7715690032128365\n",
      "batch loss: 0.6601777076721191 | avg loss: 0.7708549564465498\n",
      "batch loss: 0.801841139793396 | avg loss: 0.7710523206716889\n",
      "batch loss: 0.8037069439888 | avg loss: 0.77125899550281\n",
      "batch loss: 0.6947860717773438 | avg loss: 0.7707780337183731\n",
      "batch loss: 0.6124526858329773 | avg loss: 0.7697885002940893\n",
      "batch loss: 0.618811309337616 | avg loss: 0.7688507537664093\n",
      "batch loss: 1.0121123790740967 | avg loss: 0.7703523687374445\n",
      "batch loss: 0.40925198793411255 | avg loss: 0.7681370289779148\n",
      "batch loss: 0.8301624655723572 | avg loss: 0.7685152328595882\n",
      "batch loss: 0.4900436997413635 | avg loss: 0.7668275265982657\n",
      "batch loss: 0.5671749114990234 | avg loss: 0.7656248000012823\n",
      "batch loss: 0.6521539092063904 | avg loss: 0.7649453335893368\n",
      "batch loss: 1.1220040321350098 | avg loss: 0.767070682985442\n",
      "batch loss: 0.6815757155418396 | avg loss: 0.7665647956041189\n",
      "batch loss: 0.5473889708518982 | avg loss: 0.765275526046753\n",
      "batch loss: 0.8299083709716797 | avg loss: 0.7656534959001151\n",
      "batch loss: 0.575692892074585 | avg loss: 0.7645490737848504\n",
      "batch loss: 1.133030652999878 | avg loss: 0.7666790251097927\n",
      "batch loss: 0.7340940237045288 | avg loss: 0.7664917549867739\n",
      "batch loss: 0.48159483075141907 | avg loss: 0.764863772562572\n",
      "batch loss: 1.162672758102417 | avg loss: 0.7671240508895029\n",
      "batch loss: 0.417966365814209 | avg loss: 0.7651514086009419\n",
      "batch loss: 1.1800329685211182 | avg loss: 0.7674822038813923\n",
      "batch loss: 0.3284127712249756 | avg loss: 0.7650293020229766\n",
      "batch loss: 0.6687855124473572 | avg loss: 0.764494614303112\n",
      "batch loss: 1.2381818294525146 | avg loss: 0.7671116707404015\n",
      "batch loss: 0.7650125026702881 | avg loss: 0.7671001368499064\n",
      "batch loss: 0.29038485884666443 | avg loss: 0.764495135330763\n",
      "batch loss: 0.6378234624862671 | avg loss: 0.7638067023261733\n",
      "batch loss: 0.7547459602355957 | avg loss: 0.7637577253419\n",
      "batch loss: 0.8691883087158203 | avg loss: 0.7643245564353082\n",
      "batch loss: 0.5574794411659241 | avg loss: 0.763218432824242\n",
      "batch loss: 0.8449419736862183 | avg loss: 0.7636531325096779\n",
      "batch loss: 0.7386571168899536 | avg loss: 0.7635208784587799\n",
      "batch loss: 0.6485862135887146 | avg loss: 0.7629159591699901\n",
      "batch loss: 0.855033814907074 | avg loss: 0.7633982516084042\n",
      "batch loss: 0.7567914724349976 | avg loss: 0.7633638413002094\n",
      "batch loss: 0.6545584797859192 | avg loss: 0.7628000829503944\n",
      "batch loss: 0.766487717628479 | avg loss: 0.7628190913765701\n",
      "batch loss: 0.4895113408565521 | avg loss: 0.7614175131687752\n",
      "batch loss: 0.7834569811820984 | avg loss: 0.7615299594341493\n",
      "batch loss: 0.6067178845405579 | avg loss: 0.7607441113382427\n",
      "batch loss: 0.5134727358818054 | avg loss: 0.7594952660076546\n",
      "batch loss: 0.8624640107154846 | avg loss: 0.7600126968855834\n",
      "batch loss: 0.9503720998764038 | avg loss: 0.7609644939005374\n",
      "batch loss: 0.9732513427734375 | avg loss: 0.7620206473775171\n",
      "batch loss: 0.6556294560432434 | avg loss: 0.7614939583115058\n",
      "batch loss: 0.83685302734375 | avg loss: 0.7618651852525514\n",
      "batch loss: 0.8311235904693604 | avg loss: 0.7622046872389083\n",
      "batch loss: 0.6580899357795715 | avg loss: 0.7616968104025212\n",
      "batch loss: 0.5060955286026001 | avg loss: 0.7604560274811625\n",
      "batch loss: 0.7200967073440552 | avg loss: 0.7602610549201136\n",
      "batch loss: 0.7502016425132751 | avg loss: 0.7602126923604653\n",
      "batch loss: 0.9156917333602905 | avg loss: 0.7609566112169238\n",
      "batch loss: 0.43729498982429504 | avg loss: 0.7594153654007685\n",
      "batch loss: 0.734720766544342 | avg loss: 0.7592983293872309\n",
      "batch loss: 0.6398369669914246 | avg loss: 0.7587348323947979\n",
      "batch loss: 0.7173347473144531 | avg loss: 0.7585404657981765\n",
      "batch loss: 0.7570307850837708 | avg loss: 0.758533411215399\n",
      "batch loss: 0.7938852906227112 | avg loss: 0.7586978385614794\n",
      "batch loss: 0.9039500951766968 | avg loss: 0.7593703027124759\n",
      "batch loss: 0.5640907287597656 | avg loss: 0.758470396841726\n",
      "batch loss: 0.7346853017807007 | avg loss: 0.7583612909010791\n",
      "batch loss: 0.6806527972221375 | avg loss: 0.758006457596609\n",
      "batch loss: 0.7437042593955994 | avg loss: 0.7579414476047862\n",
      "batch loss: 1.0665035247802734 | avg loss: 0.759337656098793\n",
      "batch loss: 0.5793952941894531 | avg loss: 0.7585271049190212\n",
      "batch loss: 1.1997891664505005 | avg loss: 0.7605058585581758\n",
      "batch loss: 0.8683111667633057 | avg loss: 0.7609871322555202\n",
      "batch loss: 0.8295317888259888 | avg loss: 0.7612917751736111\n",
      "batch loss: 0.5153490900993347 | avg loss: 0.7602035332042559\n",
      "batch loss: 0.6702620387077332 | avg loss: 0.7598073151668263\n",
      "batch loss: 0.7964101433753967 | avg loss: 0.7599678538870394\n",
      "batch loss: 0.6285200715065002 | avg loss: 0.7593938461037182\n",
      "batch loss: 0.6528773307800293 | avg loss: 0.7589307308197022\n",
      "batch loss: 0.7729100584983826 | avg loss: 0.7589912473897398\n",
      "batch loss: 0.6338761448860168 | avg loss: 0.7584519581548099\n",
      "batch loss: 0.8653618097305298 | avg loss: 0.7589107987195125\n",
      "batch loss: 0.5640013217926025 | avg loss: 0.7580778522369189\n",
      "batch loss: 0.636167585849762 | avg loss: 0.7575590851459097\n",
      "batch loss: 0.5969492793083191 | avg loss: 0.7568785351211742\n",
      "batch loss: 0.797253429889679 | avg loss: 0.7570488933269485\n",
      "batch loss: 0.9363692998886108 | avg loss: 0.757802340413342\n",
      "batch loss: 0.6988513469696045 | avg loss: 0.7575556835370084\n",
      "batch loss: 0.5813776254653931 | avg loss: 0.7568216082950433\n",
      "batch loss: 0.587807297706604 | avg loss: 0.7561203041017303\n",
      "batch loss: 0.5319855809211731 | avg loss: 0.7551941275596619\n",
      "batch loss: 0.6371312141418457 | avg loss: 0.754708271948889\n",
      "batch loss: 0.8443072438240051 | avg loss: 0.7550754808500165\n",
      "batch loss: 0.5682387351989746 | avg loss: 0.7543128818881755\n",
      "batch loss: 1.0925073623657227 | avg loss: 0.7556876561990599\n",
      "batch loss: 0.49794721603393555 | avg loss: 0.7546441726356383\n",
      "batch loss: 0.7996407747268677 | avg loss: 0.7548256105472965\n",
      "batch loss: 0.4699738025665283 | avg loss: 0.753681627382715\n",
      "batch loss: 0.8801426291465759 | avg loss: 0.7541874713897705\n",
      "batch loss: 0.40238654613494873 | avg loss: 0.7527858740779984\n",
      "batch loss: 0.6295133233070374 | avg loss: 0.7522966972892247\n",
      "batch loss: 1.024868130683899 | avg loss: 0.7533740547334724\n",
      "batch loss: 1.047852635383606 | avg loss: 0.7545334192242209\n",
      "batch loss: 1.1069823503494263 | avg loss: 0.7559155718953001\n",
      "batch loss: 0.7585411667823792 | avg loss: 0.7559258281253278\n",
      "batch loss: 1.513525366783142 | avg loss: 0.758873686252401\n",
      "batch loss: 0.7964086532592773 | avg loss: 0.7590191706206447\n",
      "batch loss: 0.7364979982376099 | avg loss: 0.7589322162871195\n",
      "batch loss: 0.7205376625061035 | avg loss: 0.7587845449264233\n",
      "batch loss: 0.6213157773017883 | avg loss: 0.758257844667325\n",
      "batch loss: 0.8327312469482422 | avg loss: 0.7585420942943515\n",
      "batch loss: 0.5685678124427795 | avg loss: 0.7578197586219121\n",
      "batch loss: 0.6391955018043518 | avg loss: 0.7573704243157849\n",
      "batch loss: 0.6073001027107239 | avg loss: 0.7568041212153884\n",
      "batch loss: 0.7995030879974365 | avg loss: 0.7569646436469\n",
      "batch loss: 0.6651321053504944 | avg loss: 0.7566207015559021\n",
      "batch loss: 0.5135366916656494 | avg loss: 0.755713671668252\n",
      "batch loss: 0.6378577351570129 | avg loss: 0.7552755455102176\n",
      "batch loss: 0.6172916293144226 | avg loss: 0.7547644939687517\n",
      "batch loss: 0.6076956987380981 | avg loss: 0.7542218046874578\n",
      "batch loss: 0.9249270558357239 | avg loss: 0.7548493975225616\n",
      "batch loss: 0.7860355377197266 | avg loss: 0.7549636324683389\n",
      "batch loss: 0.8828241229057312 | avg loss: 0.7554302765940227\n",
      "batch loss: 0.5927626490592957 | avg loss: 0.7548387579484419\n",
      "batch loss: 0.9295806884765625 | avg loss: 0.7554718808851381\n",
      "batch loss: 0.7167472243309021 | avg loss: 0.7553320806809711\n",
      "batch loss: 0.7602211236953735 | avg loss: 0.7553496671666344\n",
      "batch loss: 0.6866203546524048 | avg loss: 0.7551033255447196\n",
      "batch loss: 1.1559609174728394 | avg loss: 0.7565349598016058\n",
      "batch loss: 0.752981424331665 | avg loss: 0.7565223137679049\n",
      "batch loss: 0.5232120156288147 | avg loss: 0.7556949722851422\n",
      "batch loss: 0.5882390737533569 | avg loss: 0.7551032553291995\n",
      "batch loss: 0.6384027600288391 | avg loss: 0.7546923380922264\n",
      "batch loss: 0.5427695512771606 | avg loss: 0.7539487493665595\n",
      "batch loss: 0.6951517462730408 | avg loss: 0.753743165439659\n",
      "batch loss: 0.8893792033195496 | avg loss: 0.7542157648747806\n",
      "batch loss: 1.0411255359649658 | avg loss: 0.7552119793577327\n",
      "batch loss: 0.6130454540252686 | avg loss: 0.7547200536645408\n",
      "batch loss: 0.7762022614479065 | avg loss: 0.754794130243104\n",
      "batch loss: 0.9146845936775208 | avg loss: 0.7553435820074835\n",
      "batch loss: 0.8063192367553711 | avg loss: 0.755518156167579\n",
      "batch loss: 0.4024159610271454 | avg loss: 0.7543130292217073\n",
      "batch loss: 0.6038306951522827 | avg loss: 0.7538011845480017\n",
      "batch loss: 0.5718845129013062 | avg loss: 0.7531845178644536\n",
      "batch loss: 0.9053196907043457 | avg loss: 0.7536984880429667\n",
      "batch loss: 0.9827815890312195 | avg loss: 0.7544698116153178\n",
      "batch loss: 0.641119122505188 | avg loss: 0.7540894401753508\n",
      "batch loss: 0.5024959444999695 | avg loss: 0.7532479903570386\n",
      "batch loss: 0.8401883244514465 | avg loss: 0.7535377914706866\n",
      "batch loss: 0.6957242488861084 | avg loss: 0.7533457199006381\n",
      "batch loss: 0.6879917979240417 | avg loss: 0.7531293161854838\n",
      "batch loss: 0.6133496761322021 | avg loss: 0.7526679972414136\n",
      "batch loss: 0.480755478143692 | avg loss: 0.7517735481654343\n",
      "batch loss: 0.7285455465316772 | avg loss: 0.7516973907830286\n",
      "batch loss: 0.5845466256141663 | avg loss: 0.7511511464524113\n",
      "batch loss: 0.6796085834503174 | avg loss: 0.7509181087879094\n",
      "batch loss: 0.5695207118988037 | avg loss: 0.7503291562006071\n",
      "batch loss: 0.44487640261650085 | avg loss: 0.7493406359624708\n",
      "batch loss: 0.6109759211540222 | avg loss: 0.7488942981727662\n",
      "batch loss: 0.20336702466011047 | avg loss: 0.7471401911839795\n",
      "batch loss: 0.6508591175079346 | avg loss: 0.7468315979991204\n",
      "batch loss: 0.5667369961738586 | avg loss: 0.7462562158846626\n",
      "batch loss: 0.4856513440608978 | avg loss: 0.7454262640635678\n",
      "batch loss: 0.6134648323059082 | avg loss: 0.7450073388833848\n",
      "batch loss: 0.3290518820285797 | avg loss: 0.7436910241465026\n",
      "batch loss: 0.4188382625579834 | avg loss: 0.7426662520279268\n",
      "batch loss: 0.6496227383613586 | avg loss: 0.7423736623623086\n",
      "batch loss: 0.42560890316963196 | avg loss: 0.741380669386783\n",
      "batch loss: 0.8846039772033691 | avg loss: 0.7418282422237098\n",
      "batch loss: 0.35401424765586853 | avg loss: 0.7406200989384517\n",
      "batch loss: 0.590051531791687 | avg loss: 0.7401524946926543\n",
      "batch loss: 0.6438955068588257 | avg loss: 0.7398544854423948\n",
      "batch loss: 0.4621346592903137 | avg loss: 0.7389973254851353\n",
      "batch loss: 0.602668821811676 | avg loss: 0.73857785316614\n",
      "batch loss: 0.6045108437538147 | avg loss: 0.7381666046710103\n",
      "batch loss: 0.2912939488887787 | avg loss: 0.7368000216258046\n",
      "batch loss: 0.782153844833374 | avg loss: 0.7369382954770471\n",
      "batch loss: 0.9792788624763489 | avg loss: 0.7376748929451302\n",
      "batch loss: 0.8856198191642761 | avg loss: 0.73812321090337\n",
      "batch loss: 1.2040619850158691 | avg loss: 0.7395308809157944\n",
      "batch loss: 0.8504814505577087 | avg loss: 0.7398650693785713\n",
      "batch loss: 0.9934985041618347 | avg loss: 0.7406267313448874\n",
      "batch loss: 0.7463026642799377 | avg loss: 0.7406437251560702\n",
      "batch loss: 0.7759924530982971 | avg loss: 0.7407492437469425\n",
      "batch loss: 0.7378386855125427 | avg loss: 0.7407405813712449\n",
      "batch loss: 0.4801967740058899 | avg loss: 0.739967454346422\n",
      "batch loss: 1.0661357641220093 | avg loss: 0.740932449345758\n",
      "batch loss: 0.43719014525413513 | avg loss: 0.7400364543484376\n",
      "batch loss: 1.0256218910217285 | avg loss: 0.7408764115151237\n",
      "batch loss: 1.0766007900238037 | avg loss: 0.7418609404843574\n",
      "batch loss: 0.7535591721534729 | avg loss: 0.7418951458401151\n",
      "batch loss: 0.8264012336730957 | avg loss: 0.7421415192740304\n",
      "batch loss: 0.4521821439266205 | avg loss: 0.7412986141131368\n",
      "batch loss: 0.42098015546798706 | avg loss: 0.7403701548127161\n",
      "batch loss: 0.36072981357574463 | avg loss: 0.739272928392956\n",
      "batch loss: 0.996862530708313 | avg loss: 0.7400152615408389\n",
      "batch loss: 0.7673168778419495 | avg loss: 0.7400937144612444\n",
      "batch loss: 0.7159029245376587 | avg loss: 0.7400243998769361\n",
      "batch loss: 0.5477725267410278 | avg loss: 0.7394751088108336\n",
      "batch loss: 0.765229344367981 | avg loss: 0.7395484827013097\n",
      "batch loss: 0.7866042852401733 | avg loss: 0.7396821639585224\n",
      "batch loss: 0.6491937041282654 | avg loss: 0.7394258227125443\n",
      "batch loss: 0.6269341707229614 | avg loss: 0.7391080496843252\n",
      "batch loss: 0.8323256969451904 | avg loss: 0.7393706346061868\n",
      "batch loss: 0.538751482963562 | avg loss: 0.7388070976633704\n",
      "batch loss: 0.7885767817497253 | avg loss: 0.7389465085431641\n",
      "batch loss: 0.7123027443885803 | avg loss: 0.7388720846209447\n",
      "batch loss: 0.7899611592292786 | avg loss: 0.7390143940209678\n",
      "batch loss: 0.6983250975608826 | avg loss: 0.7389013681974675\n",
      "batch loss: 0.5913483500480652 | avg loss: 0.7384926340751701\n",
      "batch loss: 0.4881380796432495 | avg loss: 0.7378010469082311\n",
      "\n",
      "  Average training loss: 0.74\n",
      "  Training epoch took: 0:30:35\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.55\n",
      "  Validation took: 0:01:49\n",
      "\n",
      "======= Epoch 4 / 5 =======\n",
      "batch loss: 0.4148852229118347 | avg loss: 0.4148852229118347\n",
      "batch loss: 0.4374050498008728 | avg loss: 0.42614513635635376\n",
      "batch loss: 1.0373581647872925 | avg loss: 0.6298828125\n",
      "batch loss: 0.7282155752182007 | avg loss: 0.6544660031795502\n",
      "batch loss: 0.5607794523239136 | avg loss: 0.6357286930084228\n",
      "batch loss: 0.7293422818183899 | avg loss: 0.651330957810084\n",
      "batch loss: 0.5631726384162903 | avg loss: 0.6387369121823993\n",
      "batch loss: 0.5179682970046997 | avg loss: 0.6236408352851868\n",
      "batch loss: 0.49130260944366455 | avg loss: 0.608936587969462\n",
      "batch loss: 0.739484429359436 | avg loss: 0.6219913721084595\n",
      "batch loss: 0.7905836701393127 | avg loss: 0.6373179446567189\n",
      "batch loss: 0.5629149675369263 | avg loss: 0.6311176965634028\n",
      "batch loss: 0.8448396325111389 | avg loss: 0.6475578454824594\n",
      "batch loss: 0.9435796737670898 | avg loss: 0.6687022617885044\n",
      "batch loss: 1.1147124767303467 | avg loss: 0.6984362761179607\n",
      "batch loss: 0.3277095854282379 | avg loss: 0.6752658579498529\n",
      "batch loss: 0.7734031081199646 | avg loss: 0.6810386373716242\n",
      "batch loss: 0.5341393351554871 | avg loss: 0.6728775650262833\n",
      "batch loss: 0.27058976888656616 | avg loss: 0.6517045231241929\n",
      "batch loss: 0.6633480191230774 | avg loss: 0.6522866979241371\n",
      "batch loss: 0.4671133756637573 | avg loss: 0.6434689206736428\n",
      "batch loss: 1.029160737991333 | avg loss: 0.661000366915356\n",
      "batch loss: 0.552006721496582 | avg loss: 0.6562615127667136\n",
      "batch loss: 0.501635730266571 | avg loss: 0.6498187718292078\n",
      "batch loss: 0.6060531735420227 | avg loss: 0.6480681478977204\n",
      "batch loss: 0.46971961855888367 | avg loss: 0.6412085890769958\n",
      "batch loss: 0.6714604496955872 | avg loss: 0.6423290283591659\n",
      "batch loss: 0.5687163472175598 | avg loss: 0.63970000403268\n",
      "batch loss: 0.47452661395072937 | avg loss: 0.6340043698919231\n",
      "batch loss: 0.426078736782074 | avg loss: 0.6270735154549281\n",
      "batch loss: 0.6207168698310852 | avg loss: 0.626868462370288\n",
      "batch loss: 0.4514527916908264 | avg loss: 0.6213867226615548\n",
      "batch loss: 0.5479932427406311 | avg loss: 0.6191626778154662\n",
      "batch loss: 0.6945226192474365 | avg loss: 0.6213791466811124\n",
      "batch loss: 0.8227819204330444 | avg loss: 0.6271335116454533\n",
      "batch loss: 0.4770860970020294 | avg loss: 0.6229655279053582\n",
      "batch loss: 0.6772957444190979 | avg loss: 0.6244339121354593\n",
      "batch loss: 0.4090334475040436 | avg loss: 0.6187654788556852\n",
      "batch loss: 0.8864292502403259 | avg loss: 0.6256286524809324\n",
      "batch loss: 0.6391435861587524 | avg loss: 0.6259665258228779\n",
      "batch loss: 0.760515034198761 | avg loss: 0.629248196758875\n",
      "batch loss: 0.6360686421394348 | avg loss: 0.6294105883155551\n",
      "batch loss: 0.8548075556755066 | avg loss: 0.6346523782541585\n",
      "batch loss: 0.3648549020290375 | avg loss: 0.6285206174308603\n",
      "batch loss: 0.2538596987724304 | avg loss: 0.6201948192384508\n",
      "batch loss: 0.4621722102165222 | avg loss: 0.6167595451292784\n",
      "batch loss: 0.6806415319442749 | avg loss: 0.6181187363381081\n",
      "batch loss: 0.3771078288555145 | avg loss: 0.6130976757655541\n",
      "batch loss: 0.7508294582366943 | avg loss: 0.6159085284690468\n",
      "batch loss: 0.5737709403038025 | avg loss: 0.6150657767057419\n",
      "batch loss: 0.5514233112335205 | avg loss: 0.6138178852258944\n",
      "batch loss: 0.5163623094558716 | avg loss: 0.6119437395380094\n",
      "batch loss: 0.44456371665000916 | avg loss: 0.6087856258986131\n",
      "batch loss: 0.3443973660469055 | avg loss: 0.6038895470124704\n",
      "batch loss: 0.3108007311820984 | avg loss: 0.5985606594519182\n",
      "batch loss: 0.3313849866390228 | avg loss: 0.593789665294545\n",
      "batch loss: 0.42850014567375183 | avg loss: 0.5908898491608469\n",
      "batch loss: 0.6710150241851807 | avg loss: 0.5922713176957493\n",
      "batch loss: 0.19376106560230255 | avg loss: 0.5855169066433179\n",
      "batch loss: 1.1489096879959106 | avg loss: 0.5949067863325278\n",
      "batch loss: 0.5982224941253662 | avg loss: 0.5949611421979841\n",
      "batch loss: 0.6482136845588684 | avg loss: 0.5958200541715468\n",
      "batch loss: 0.4944652318954468 | avg loss: 0.5942112474687515\n",
      "batch loss: 1.1118249893188477 | avg loss: 0.6022989621851593\n",
      "batch loss: 0.48484212160110474 | avg loss: 0.6004919338684815\n",
      "batch loss: 0.9714312553405762 | avg loss: 0.6061122266180587\n",
      "batch loss: 0.40616971254348755 | avg loss: 0.6031280099900801\n",
      "batch loss: 0.6061258316040039 | avg loss: 0.6031720956020495\n",
      "batch loss: 0.696354329586029 | avg loss: 0.6045225627612376\n",
      "batch loss: 0.9311161041259766 | avg loss: 0.6091881847807339\n",
      "batch loss: 0.9041823148727417 | avg loss: 0.6133430316834383\n",
      "batch loss: 0.722697913646698 | avg loss: 0.6148618494884835\n",
      "batch loss: 0.8431487679481506 | avg loss: 0.6179890675495748\n",
      "batch loss: 0.6433930397033691 | avg loss: 0.618332364470572\n",
      "batch loss: 1.0474529266357422 | avg loss: 0.6240539719661077\n",
      "batch loss: 0.606763482093811 | avg loss: 0.6238264655204195\n",
      "batch loss: 0.9198865294456482 | avg loss: 0.6276714014155524\n",
      "batch loss: 0.4610210955142975 | avg loss: 0.625534859032203\n",
      "batch loss: 0.9131505489349365 | avg loss: 0.6291755639676806\n",
      "batch loss: 0.3282845616340637 | avg loss: 0.6254144264385104\n",
      "batch loss: 0.5789094567298889 | avg loss: 0.6248402910100089\n",
      "batch loss: 0.5939648151397705 | avg loss: 0.6244637608164694\n",
      "batch loss: 0.47094324231147766 | avg loss: 0.6226141160152044\n",
      "batch loss: 0.33830681443214417 | avg loss: 0.6192295052820728\n",
      "batch loss: 0.3639759123325348 | avg loss: 0.6162265218356077\n",
      "batch loss: 1.0710768699645996 | avg loss: 0.6215154793719913\n",
      "batch loss: 0.40322640538215637 | avg loss: 0.6190064095560162\n",
      "batch loss: 0.7184156775474548 | avg loss: 0.6201360603286461\n",
      "batch loss: 0.4158003032207489 | avg loss: 0.6178401529454114\n",
      "batch loss: 0.5265592336654663 | avg loss: 0.6168259205089675\n",
      "batch loss: 0.42612728476524353 | avg loss: 0.6147303311051903\n",
      "batch loss: 0.696173906326294 | avg loss: 0.6156155873575936\n",
      "batch loss: 0.6343029737472534 | avg loss: 0.6158165269961922\n",
      "batch loss: 0.21236425638198853 | avg loss: 0.6115244815641261\n",
      "batch loss: 0.381819486618042 | avg loss: 0.6091065342489042\n",
      "batch loss: 0.3199993073940277 | avg loss: 0.6060950006358325\n",
      "batch loss: 0.8733200430870056 | avg loss: 0.60884989798069\n",
      "batch loss: 0.4498338997364044 | avg loss: 0.6072272857537075\n",
      "batch loss: 0.8109627962112427 | avg loss: 0.6092852202027735\n",
      "batch loss: 0.4416448175907135 | avg loss: 0.6076088161766529\n",
      "batch loss: 0.37872475385665894 | avg loss: 0.6053426373418015\n",
      "batch loss: 0.4874945878982544 | avg loss: 0.6041872643080413\n",
      "batch loss: 0.6424041390419006 | avg loss: 0.6045583019268166\n",
      "batch loss: 0.31329941749572754 | avg loss: 0.6017577357303637\n",
      "batch loss: 0.4070807099342346 | avg loss: 0.5999036688180197\n",
      "batch loss: 0.4721852242946625 | avg loss: 0.5986987778319502\n",
      "batch loss: 0.3902466595172882 | avg loss: 0.5967506271934955\n",
      "batch loss: 0.21841180324554443 | avg loss: 0.5932474899347182\n",
      "batch loss: 0.6758339405059814 | avg loss: 0.5940051637931701\n",
      "batch loss: 0.8479180335998535 | avg loss: 0.5963134626095945\n",
      "batch loss: 0.30986255407333374 | avg loss: 0.5937328237939525\n",
      "batch loss: 0.6523625254631042 | avg loss: 0.5942563032731414\n",
      "batch loss: 0.475739061832428 | avg loss: 0.5932074781276483\n",
      "batch loss: 0.46541932225227356 | avg loss: 0.5920865293918994\n",
      "batch loss: 1.0090667009353638 | avg loss: 0.59571244392706\n",
      "batch loss: 0.5203973650932312 | avg loss: 0.5950631760060787\n",
      "batch loss: 0.30953019857406616 | avg loss: 0.5926227232075145\n",
      "batch loss: 0.7592789530754089 | avg loss: 0.5940350641385984\n",
      "batch loss: 0.9245280027389526 | avg loss: 0.5968123157234753\n",
      "batch loss: 0.49341681599617004 | avg loss: 0.5959506865590811\n",
      "batch loss: 0.8521151542663574 | avg loss: 0.5980677482756701\n",
      "batch loss: 0.7939023971557617 | avg loss: 0.5996729503156709\n",
      "batch loss: 0.2629970610141754 | avg loss: 0.5969357479636263\n",
      "batch loss: 0.29196155071258545 | avg loss: 0.5944762786309565\n",
      "batch loss: 0.1905289888381958 | avg loss: 0.5912447003126144\n",
      "batch loss: 1.008225440979004 | avg loss: 0.5945540712702841\n",
      "batch loss: 0.43354663252830505 | avg loss: 0.5932862961620796\n",
      "batch loss: 0.4993443787097931 | avg loss: 0.5925523749319836\n",
      "batch loss: 1.2876781225204468 | avg loss: 0.5979409466187159\n",
      "batch loss: 0.6636633276939392 | avg loss: 0.5984465033962176\n",
      "batch loss: 0.5452694892883301 | avg loss: 0.598040571990814\n",
      "batch loss: 0.8172195553779602 | avg loss: 0.599701018834656\n",
      "batch loss: 0.726250171661377 | avg loss: 0.6006525162243306\n",
      "batch loss: 0.535750687122345 | avg loss: 0.6001681742161068\n",
      "batch loss: 0.9931849837303162 | avg loss: 0.6030794098421379\n",
      "batch loss: 0.7113333344459534 | avg loss: 0.6038753945818719\n",
      "batch loss: 1.0871015787124634 | avg loss: 0.6074025930061827\n",
      "batch loss: 0.8653653860092163 | avg loss: 0.609271888607654\n",
      "batch loss: 0.6971142292022705 | avg loss: 0.6099038478925074\n",
      "batch loss: 0.72489994764328 | avg loss: 0.6107252486050129\n",
      "batch loss: 0.9164853096008301 | avg loss: 0.6128937596759052\n",
      "batch loss: 0.5035528540611267 | avg loss: 0.6121237532983363\n",
      "batch loss: 0.8340741395950317 | avg loss: 0.6136758539018098\n",
      "batch loss: 0.6039279699325562 | avg loss: 0.6136081602631344\n",
      "batch loss: 0.7187042832374573 | avg loss: 0.6143329611112331\n",
      "batch loss: 0.9138579964637756 | avg loss: 0.6163845024492642\n",
      "batch loss: 0.4699042737483978 | avg loss: 0.615388038308442\n",
      "batch loss: 0.6321262717247009 | avg loss: 0.6155011344801735\n",
      "batch loss: 0.38879311084747314 | avg loss: 0.6139796041202226\n",
      "batch loss: 0.583871066570282 | avg loss: 0.6137788805365563\n",
      "batch loss: 0.4651317894458771 | avg loss: 0.6127944627147637\n",
      "batch loss: 0.46343615651130676 | avg loss: 0.6118118422792146\n",
      "batch loss: 0.757344126701355 | avg loss: 0.6127630336806665\n",
      "batch loss: 0.4407035708427429 | avg loss: 0.6116457644414592\n",
      "batch loss: 0.269896924495697 | avg loss: 0.6094409332160027\n",
      "batch loss: 0.8134032487869263 | avg loss: 0.610748383956842\n",
      "batch loss: 0.424966037273407 | avg loss: 0.6095650569079029\n",
      "batch loss: 0.8013665676116943 | avg loss: 0.6107789905199522\n",
      "batch loss: 0.5449926853179932 | avg loss: 0.61036524017277\n",
      "batch loss: 0.25727659463882446 | avg loss: 0.6081584361381829\n",
      "batch loss: 0.3863840699195862 | avg loss: 0.6067809556026637\n",
      "batch loss: 0.8254826068878174 | avg loss: 0.6081309657957819\n",
      "batch loss: 0.4059602916240692 | avg loss: 0.6068906549112928\n",
      "batch loss: 0.9908624291419983 | avg loss: 0.6092319462175776\n",
      "batch loss: 0.5087063312530518 | avg loss: 0.6086227000662775\n",
      "batch loss: 0.597204327583313 | avg loss: 0.6085539146898741\n",
      "batch loss: 0.787894606590271 | avg loss: 0.6096278110485591\n",
      "batch loss: 0.9347769021987915 | avg loss: 0.6115632223054057\n",
      "batch loss: 0.731326162815094 | avg loss: 0.6122718787581257\n",
      "batch loss: 0.6195325255393982 | avg loss: 0.6123145884450745\n",
      "batch loss: 0.8703031539916992 | avg loss: 0.6138232935067506\n",
      "batch loss: 0.5707162022590637 | avg loss: 0.6135726708832175\n",
      "batch loss: 0.8717662692070007 | avg loss: 0.6150651194284417\n",
      "batch loss: 0.5949273109436035 | avg loss: 0.6149493848969196\n",
      "batch loss: 0.25583770871162415 | avg loss: 0.6128973181758608\n",
      "batch loss: 1.0545369386672974 | avg loss: 0.6154066342013803\n",
      "batch loss: 0.4515262842178345 | avg loss: 0.6144807565178575\n",
      "batch loss: 1.1682102680206299 | avg loss: 0.6175915964701203\n",
      "batch loss: 0.26988160610198975 | avg loss: 0.6156490825574491\n",
      "batch loss: 0.7074778079986572 | avg loss: 0.6161592421432336\n",
      "batch loss: 1.07221519947052 | avg loss: 0.6186788883163126\n",
      "batch loss: 0.9714967608451843 | avg loss: 0.6206174480554821\n",
      "batch loss: 0.3468918800354004 | avg loss: 0.6191216799242248\n",
      "batch loss: 0.680547297000885 | avg loss: 0.6194555147996416\n",
      "batch loss: 0.5161076784133911 | avg loss: 0.6188968778462023\n",
      "batch loss: 0.7541772723197937 | avg loss: 0.6196241917949851\n",
      "batch loss: 0.2759920656681061 | avg loss: 0.6177865868424349\n",
      "batch loss: 0.7617771029472351 | avg loss: 0.6185524938429924\n",
      "batch loss: 0.6776657104492188 | avg loss: 0.6188652621848243\n",
      "batch loss: 0.39980602264404297 | avg loss: 0.6177123188188202\n",
      "batch loss: 0.8714100122451782 | avg loss: 0.6190405789938273\n",
      "batch loss: 0.6369243264198303 | avg loss: 0.6191337235116711\n",
      "batch loss: 0.4082338213920593 | avg loss: 0.6180409779048337\n",
      "batch loss: 0.6983135938644409 | avg loss: 0.6184547542757595\n",
      "batch loss: 0.49431076645851135 | avg loss: 0.6178181184407993\n",
      "batch loss: 0.7793555855751038 | avg loss: 0.6186422891914845\n",
      "batch loss: 0.3951268792152405 | avg loss: 0.6175076932017574\n",
      "batch loss: 0.3174211084842682 | avg loss: 0.6159921043900528\n",
      "batch loss: 0.6113235354423523 | avg loss: 0.615968644244587\n",
      "batch loss: 0.8273112773895264 | avg loss: 0.6170253574103117\n",
      "batch loss: 0.9690032005310059 | avg loss: 0.6187764909581759\n",
      "batch loss: 0.49830541014671326 | avg loss: 0.6181800994690102\n",
      "batch loss: 0.7058568596839905 | avg loss: 0.618612004691744\n",
      "batch loss: 0.5983560085296631 | avg loss: 0.6185127105929104\n",
      "batch loss: 0.5735574960708618 | avg loss: 0.6182934168635346\n",
      "batch loss: 0.3831396996974945 | avg loss: 0.6171518939646702\n",
      "batch loss: 0.6103095412254333 | avg loss: 0.6171188391205193\n",
      "batch loss: 0.6190975904464722 | avg loss: 0.617128352348048\n",
      "batch loss: 0.7008997201919556 | avg loss: 0.6175291722898848\n",
      "batch loss: 0.31576550006866455 | avg loss: 0.6160922024221648\n",
      "batch loss: 0.49695587158203125 | avg loss: 0.6155275752617849\n",
      "batch loss: 0.41590118408203125 | avg loss: 0.6145859413411258\n",
      "batch loss: 0.5341382622718811 | avg loss: 0.6142082527069979\n",
      "batch loss: 0.49342405796051025 | avg loss: 0.6136438405820143\n",
      "batch loss: 0.5233173370361328 | avg loss: 0.6132237173097078\n",
      "batch loss: 0.35023123025894165 | avg loss: 0.6120061594992876\n",
      "batch loss: 0.30989980697631836 | avg loss: 0.6106139643263707\n",
      "batch loss: 0.41435104608535767 | avg loss: 0.6097136757105862\n",
      "batch loss: 0.5585015416145325 | avg loss: 0.609479830349417\n",
      "batch loss: 0.6640865206718445 | avg loss: 0.6097280425781554\n",
      "batch loss: 0.7687524557113647 | avg loss: 0.6104476100583961\n",
      "batch loss: 0.3815534710884094 | avg loss: 0.6094165553783512\n",
      "batch loss: 0.8155191540718079 | avg loss: 0.6103407822783218\n",
      "batch loss: 0.5509803891181946 | avg loss: 0.6100757805231426\n",
      "batch loss: 0.8060995936393738 | avg loss: 0.6109469974703259\n",
      "batch loss: 0.22526851296424866 | avg loss: 0.6092404555034848\n",
      "batch loss: 0.4230360686779022 | avg loss: 0.6084201718610814\n",
      "batch loss: 0.47404590249061584 | avg loss: 0.6078308110305092\n",
      "batch loss: 0.5062614679336548 | avg loss: 0.607387276781178\n",
      "batch loss: 0.5330267548561096 | avg loss: 0.6070639701641124\n",
      "batch loss: 0.6372972130775452 | avg loss: 0.6071948500035645\n",
      "batch loss: 0.518442690372467 | avg loss: 0.6068122975913615\n",
      "batch loss: 0.5602091550827026 | avg loss: 0.6066122841041999\n",
      "batch loss: 0.31271201372146606 | avg loss: 0.6053563000427352\n",
      "batch loss: 0.47089776396751404 | avg loss: 0.6047841360594364\n",
      "batch loss: 0.2509259283542633 | avg loss: 0.603284736874245\n",
      "batch loss: 0.7172664999961853 | avg loss: 0.6037656725836202\n",
      "batch loss: 0.6056933403015137 | avg loss: 0.6037737720278131\n",
      "batch loss: 0.4272193908691406 | avg loss: 0.603035050767735\n",
      "batch loss: 0.4447779059410095 | avg loss: 0.6023756459976236\n",
      "batch loss: 0.29689669609069824 | avg loss: 0.6011080984876364\n",
      "batch loss: 0.2792823314666748 | avg loss: 0.5997782399462275\n",
      "batch loss: 0.4754570424556732 | avg loss: 0.5992666300800111\n",
      "batch loss: 0.825057864189148 | avg loss: 0.6001920039902945\n",
      "batch loss: 0.29389700293540955 | avg loss: 0.5989418203125194\n",
      "batch loss: 1.202707052230835 | avg loss: 0.6013961505235695\n",
      "batch loss: 0.40580809116363525 | avg loss: 0.6006042960322338\n",
      "batch loss: 0.7278361320495605 | avg loss: 0.6011173276290778\n",
      "batch loss: 0.308272123336792 | avg loss: 0.5999412424712774\n",
      "batch loss: 0.7258893847465515 | avg loss: 0.6004450350403786\n",
      "batch loss: 0.2785335183143616 | avg loss: 0.5991625190374861\n",
      "batch loss: 0.4964412748813629 | avg loss: 0.5987548950527396\n",
      "batch loss: 0.9200072288513184 | avg loss: 0.6000246671230897\n",
      "batch loss: 0.822023868560791 | avg loss: 0.6008986797271751\n",
      "batch loss: 1.0866315364837646 | avg loss: 0.6028035144595539\n",
      "batch loss: 0.7557790875434875 | avg loss: 0.603401075291913\n",
      "batch loss: 1.141662359237671 | avg loss: 0.6054954771749704\n",
      "batch loss: 0.5551095008850098 | avg loss: 0.6053001826932264\n",
      "batch loss: 0.527594804763794 | avg loss: 0.6050001619290201\n",
      "batch loss: 0.46681106090545654 | avg loss: 0.6044686653866218\n",
      "batch loss: 0.36996087431907654 | avg loss: 0.603570168102838\n",
      "batch loss: 0.71464604139328 | avg loss: 0.6039941218176871\n",
      "batch loss: 0.40538567304611206 | avg loss: 0.6032389566132325\n",
      "batch loss: 0.5065359473228455 | avg loss: 0.6028726573356173\n",
      "batch loss: 0.4391217529773712 | avg loss: 0.6022547293946429\n",
      "batch loss: 0.723733127117157 | avg loss: 0.6027114151003665\n",
      "batch loss: 0.3295271694660187 | avg loss: 0.6016882531316986\n",
      "batch loss: 0.26234495639801025 | avg loss: 0.6004220468006027\n",
      "batch loss: 0.4151175022125244 | avg loss: 0.5997331823225801\n",
      "batch loss: 0.5879285931587219 | avg loss: 0.5996894616219732\n",
      "batch loss: 0.35602912306785583 | avg loss: 0.5987903459815521\n",
      "batch loss: 0.7803705334663391 | avg loss: 0.5994579202002462\n",
      "batch loss: 0.6072158217430115 | avg loss: 0.5994863374220146\n",
      "batch loss: 0.8229476809501648 | avg loss: 0.6003018897706575\n",
      "batch loss: 0.2785913050174713 | avg loss: 0.5991320330988277\n",
      "batch loss: 0.762328565120697 | avg loss: 0.5997233248815157\n",
      "batch loss: 0.5354406237602234 | avg loss: 0.5994912573684423\n",
      "batch loss: 0.5270678400993347 | avg loss: 0.5992307414789851\n",
      "batch loss: 0.6337457299232483 | avg loss: 0.5993544511149861\n",
      "batch loss: 0.8431729078292847 | avg loss: 0.6002252313175371\n",
      "batch loss: 0.9056981205940247 | avg loss: 0.6013123234501937\n",
      "batch loss: 0.5792023539543152 | avg loss: 0.6012339193030452\n",
      "batch loss: 0.5873916149139404 | avg loss: 0.6011850065666879\n",
      "batch loss: 0.42843684554100037 | avg loss: 0.6005767383940623\n",
      "batch loss: 0.3832860291004181 | avg loss: 0.5998143148526811\n",
      "batch loss: 0.5867525339126587 | avg loss: 0.5997686442899537\n",
      "batch loss: 0.6731734275817871 | avg loss: 0.6000244100853956\n",
      "batch loss: 0.7097403407096863 | avg loss: 0.6004053681778411\n",
      "batch loss: 0.5237623453140259 | avg loss: 0.6001401674066861\n",
      "batch loss: 0.7309261560440063 | avg loss: 0.600591153574401\n",
      "batch loss: 0.49438169598579407 | avg loss: 0.6002261726204882\n",
      "batch loss: 0.5356185436248779 | avg loss: 0.6000049136170785\n",
      "batch loss: 0.34747007489204407 | avg loss: 0.5991430199695529\n",
      "batch loss: 0.4453131854534149 | avg loss: 0.5986197892399061\n",
      "batch loss: 0.4891127347946167 | avg loss: 0.5982485788858543\n",
      "batch loss: 0.560071587562561 | avg loss: 0.5981196025638161\n",
      "batch loss: 0.7090403437614441 | avg loss: 0.5984930730728991\n",
      "batch loss: 0.4079432487487793 | avg loss: 0.5978536441322141\n",
      "batch loss: 0.421985000371933 | avg loss: 0.5972654546881998\n",
      "batch loss: 0.4486809968948364 | avg loss: 0.5967701731622219\n",
      "batch loss: 0.4054875373840332 | avg loss: 0.596134682677909\n",
      "batch loss: 0.4180077016353607 | avg loss: 0.5955448582373708\n",
      "batch loss: 0.5266251564025879 | avg loss: 0.5953174004755398\n",
      "batch loss: 0.49285250902175903 | avg loss: 0.594980344911547\n",
      "batch loss: 0.6405940055847168 | avg loss: 0.5951298978973607\n",
      "batch loss: 0.24961335957050323 | avg loss: 0.5940007588832207\n",
      "batch loss: 0.325562059879303 | avg loss: 0.5931263657268562\n",
      "batch loss: 0.3883681297302246 | avg loss: 0.5924615662593347\n",
      "batch loss: 0.2216241955757141 | avg loss: 0.5912614453186109\n",
      "batch loss: 0.4298863112926483 | avg loss: 0.59074088037014\n",
      "batch loss: 0.08706951141357422 | avg loss: 0.589121358283463\n",
      "batch loss: 0.5429232120513916 | avg loss: 0.58897328730195\n",
      "batch loss: 0.35317105054855347 | avg loss: 0.5882199255231851\n",
      "batch loss: 0.3325880169868469 | avg loss: 0.5874058111647892\n",
      "batch loss: 0.5135568976402283 | avg loss: 0.5871713701694731\n",
      "batch loss: 0.21269391477108002 | avg loss: 0.5859863149308706\n",
      "batch loss: 0.2971535325050354 | avg loss: 0.5850751705068143\n",
      "batch loss: 0.24237926304340363 | avg loss: 0.5839975104204513\n",
      "batch loss: 0.495536208152771 | avg loss: 0.5837202022628725\n",
      "batch loss: 0.5523785948753357 | avg loss: 0.5836222597397864\n",
      "batch loss: 0.21439027786254883 | avg loss: 0.5824720043445302\n",
      "batch loss: 0.5421708822250366 | avg loss: 0.5823468455801839\n",
      "batch loss: 0.41462865471839905 | avg loss: 0.5818275942152867\n",
      "batch loss: 0.4423797130584717 | avg loss: 0.5813971995203583\n",
      "batch loss: 0.23445551097393036 | avg loss: 0.5803296866325232\n",
      "batch loss: 0.49086636304855347 | avg loss: 0.5800552592595662\n",
      "batch loss: 0.1475406289100647 | avg loss: 0.5787325845490173\n",
      "batch loss: 0.37716415524482727 | avg loss: 0.5781180466547972\n",
      "batch loss: 0.6751896142959595 | avg loss: 0.5784130970123691\n",
      "batch loss: 0.9972007274627686 | avg loss: 0.5796821504379763\n",
      "batch loss: 0.6997808814048767 | avg loss: 0.5800449864831936\n",
      "batch loss: 0.4512743651866913 | avg loss: 0.5796571231660355\n",
      "batch loss: 0.8938408493995667 | avg loss: 0.5806006178394094\n",
      "batch loss: 0.6360553503036499 | avg loss: 0.5807666499725359\n",
      "batch loss: 0.5464657545089722 | avg loss: 0.5806642592398088\n",
      "batch loss: 0.7843625545501709 | avg loss: 0.5812705041663278\n",
      "batch loss: 0.3518144488334656 | avg loss: 0.5805896256638564\n",
      "batch loss: 0.8578425049781799 | avg loss: 0.5814099004547272\n",
      "batch loss: 0.6970563530921936 | avg loss: 0.5817510404330087\n",
      "batch loss: 0.9702852368354797 | avg loss: 0.5828937880694867\n",
      "batch loss: 0.9706065654754639 | avg loss: 0.584030775686513\n",
      "batch loss: 0.6849489808082581 | avg loss: 0.5843258581576292\n",
      "batch loss: 0.8684154748916626 | avg loss: 0.5851541077690986\n",
      "batch loss: 0.2697514593601227 | avg loss: 0.5842372396051191\n",
      "batch loss: 0.20702867209911346 | avg loss: 0.583143881438435\n",
      "batch loss: 0.3362794816493988 | avg loss: 0.5824304005141893\n",
      "batch loss: 0.9465600848197937 | avg loss: 0.5834797655986435\n",
      "batch loss: 0.6648967862129211 | avg loss: 0.5837137225544315\n",
      "batch loss: 0.4747134745121002 | avg loss: 0.5834014009268031\n",
      "batch loss: 0.3269177973270416 | avg loss: 0.5826685906308038\n",
      "batch loss: 0.5428240299224854 | avg loss: 0.5825550733638285\n",
      "batch loss: 0.4886564314365387 | avg loss: 0.5822883158583533\n",
      "batch loss: 0.5648952126502991 | avg loss: 0.582239043611305\n",
      "batch loss: 0.6367824077606201 | avg loss: 0.5823931209111618\n",
      "batch loss: 0.9947676062583923 | avg loss: 0.5835547391797455\n",
      "batch loss: 0.44971728324890137 | avg loss: 0.5831787912698274\n",
      "batch loss: 0.32162341475486755 | avg loss: 0.5824461431563401\n",
      "batch loss: 0.9003825783729553 | avg loss: 0.5833342337575038\n",
      "batch loss: 0.49227118492126465 | avg loss: 0.5830805762398542\n",
      "batch loss: 0.5074926614761353 | avg loss: 0.582870609809955\n",
      "batch loss: 0.6060910224914551 | avg loss: 0.582934932282757\n",
      "batch loss: 0.30507132411003113 | avg loss: 0.5821673532546554\n",
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epoch took: 0:30:28\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.54\n",
      "  Validation took: 0:01:51\n",
      "\n",
      "======= Epoch 5 / 5 =======\n",
      "batch loss: 0.37496501207351685 | avg loss: 0.37496501207351685\n",
      "batch loss: 0.2984371781349182 | avg loss: 0.33670109510421753\n",
      "batch loss: 0.9515740275382996 | avg loss: 0.5416587392489115\n",
      "batch loss: 0.384030282497406 | avg loss: 0.5022516250610352\n",
      "batch loss: 0.40801385045051575 | avg loss: 0.4834040701389313\n",
      "batch loss: 0.5344001054763794 | avg loss: 0.4919034093618393\n",
      "batch loss: 0.3169015944004059 | avg loss: 0.4669031500816345\n",
      "batch loss: 0.3557316064834595 | avg loss: 0.45300670713186264\n",
      "batch loss: 0.5883146524429321 | avg loss: 0.46804092327753705\n",
      "batch loss: 0.45940208435058594 | avg loss: 0.4671770393848419\n",
      "batch loss: 0.57884681224823 | avg loss: 0.4773288369178772\n",
      "batch loss: 0.5481178760528564 | avg loss: 0.4832279235124588\n",
      "batch loss: 0.47985583543777466 | avg loss: 0.4829685321220985\n",
      "batch loss: 0.7517650127410889 | avg loss: 0.5021682807377407\n",
      "batch loss: 0.9519245624542236 | avg loss: 0.5321520328521728\n",
      "batch loss: 0.2843793034553528 | avg loss: 0.5166662372648716\n",
      "batch loss: 0.6967002153396606 | avg loss: 0.527256471269271\n",
      "batch loss: 0.5697907209396362 | avg loss: 0.5296194851398468\n",
      "batch loss: 0.1718483567237854 | avg loss: 0.5107894257495278\n",
      "batch loss: 0.5072142481803894 | avg loss: 0.5106106668710708\n",
      "batch loss: 0.47960779070854187 | avg loss: 0.5091343394347599\n",
      "batch loss: 0.839280366897583 | avg loss: 0.5241409770467065\n",
      "batch loss: 0.4099195599555969 | avg loss: 0.5191748284775278\n",
      "batch loss: 0.4380948841571808 | avg loss: 0.51579649746418\n",
      "batch loss: 0.18195749819278717 | avg loss: 0.5024429374933242\n",
      "batch loss: 0.4615611732006073 | avg loss: 0.5008705619436044\n",
      "batch loss: 0.3561016321182251 | avg loss: 0.4955087497278496\n",
      "batch loss: 0.575759768486023 | avg loss: 0.49837485754064154\n",
      "batch loss: 0.30060014128685 | avg loss: 0.4915550397387866\n",
      "batch loss: 0.3831287920475006 | avg loss: 0.4879408314824104\n",
      "batch loss: 0.34919285774230957 | avg loss: 0.4834650903940201\n",
      "batch loss: 0.40025338530540466 | avg loss: 0.48086472461000085\n",
      "batch loss: 0.39193764328956604 | avg loss: 0.47816996456998767\n",
      "batch loss: 0.7304475903511047 | avg loss: 0.4855898947400205\n",
      "batch loss: 0.7492651343345642 | avg loss: 0.4931234730141503\n",
      "batch loss: 0.24593929946422577 | avg loss: 0.4862572459710969\n",
      "batch loss: 0.23457691073417664 | avg loss: 0.47945507474847743\n",
      "batch loss: 0.3925401270389557 | avg loss: 0.4771678392824374\n",
      "batch loss: 0.9199362993240356 | avg loss: 0.4885208767194014\n",
      "batch loss: 0.8179872035980225 | avg loss: 0.496757534891367\n",
      "batch loss: 0.3536226451396942 | avg loss: 0.4932664400193749\n",
      "batch loss: 0.26762646436691284 | avg loss: 0.4878940596466973\n",
      "batch loss: 0.6882422566413879 | avg loss: 0.49255332004192265\n",
      "batch loss: 0.13784322142601013 | avg loss: 0.484491726891561\n",
      "batch loss: 0.0865597203373909 | avg loss: 0.47564879341257943\n",
      "batch loss: 0.38616764545440674 | avg loss: 0.47370355106566264\n",
      "batch loss: 0.3430187404155731 | avg loss: 0.47092302317949053\n",
      "batch loss: 0.2722407281398773 | avg loss: 0.4667838086994986\n",
      "batch loss: 0.5994469523429871 | avg loss: 0.46949121979426367\n",
      "batch loss: 0.4795549511909485 | avg loss: 0.46969249442219735\n",
      "batch loss: 0.328225314617157 | avg loss: 0.46691862815151025\n",
      "batch loss: 0.44046494364738464 | avg loss: 0.46640990344950783\n",
      "batch loss: 0.4483000636100769 | avg loss: 0.46606820835819784\n",
      "batch loss: 0.16237616539001465 | avg loss: 0.4604442816365648\n",
      "batch loss: 0.17386412620544434 | avg loss: 0.455233733355999\n",
      "batch loss: 0.44867831468582153 | avg loss: 0.45511667230831726\n",
      "batch loss: 0.49879035353660583 | avg loss: 0.4558828772421469\n",
      "batch loss: 0.5669330358505249 | avg loss: 0.45779753514918786\n",
      "batch loss: 0.09617997705936432 | avg loss: 0.4516684239951231\n",
      "batch loss: 0.6656091213226318 | avg loss: 0.45523410228391487\n",
      "batch loss: 0.6085678935050964 | avg loss: 0.45774777099245884\n",
      "batch loss: 0.506401777267456 | avg loss: 0.4585325130291523\n",
      "batch loss: 0.627859890460968 | avg loss: 0.46122024917886373\n",
      "batch loss: 0.6990459561347961 | avg loss: 0.46493627585005015\n",
      "batch loss: 0.4304496645927429 | avg loss: 0.46440571259993774\n",
      "batch loss: 0.8441053032875061 | avg loss: 0.4701587367012645\n",
      "batch loss: 0.2999463677406311 | avg loss: 0.46761825358244913\n",
      "batch loss: 0.39004796743392944 | avg loss: 0.466477514080265\n",
      "batch loss: 0.6628398895263672 | avg loss: 0.4693233456084694\n",
      "batch loss: 0.658893346786499 | avg loss: 0.4720314884824412\n",
      "batch loss: 0.5754427909851074 | avg loss: 0.47348798570078865\n",
      "batch loss: 0.6806541681289673 | avg loss: 0.47636529379006887\n",
      "batch loss: 0.7602720856666565 | avg loss: 0.4802544279253646\n",
      "batch loss: 0.43923574686050415 | avg loss: 0.47970012142448815\n",
      "batch loss: 0.6513505578041077 | avg loss: 0.48198879390954974\n",
      "batch loss: 0.3259449899196625 | avg loss: 0.47993558596231434\n",
      "batch loss: 0.49378880858421326 | avg loss: 0.48011549794441694\n",
      "batch loss: 0.30985701084136963 | avg loss: 0.4779326968277112\n",
      "batch loss: 0.954942524433136 | avg loss: 0.4839707959113242\n",
      "batch loss: 0.2543047368526459 | avg loss: 0.4810999701730907\n",
      "batch loss: 0.27206605672836304 | avg loss: 0.47851930457500763\n",
      "batch loss: 0.32775911688804626 | avg loss: 0.4766807657007764\n",
      "batch loss: 0.3483266234397888 | avg loss: 0.47513433025184887\n",
      "batch loss: 0.19428640604019165 | avg loss: 0.47179090258266243\n",
      "batch loss: 0.24543549120426178 | avg loss: 0.4691278977429166\n",
      "batch loss: 1.2318317890167236 | avg loss: 0.4779965476414492\n",
      "batch loss: 0.16746123135089874 | avg loss: 0.47442717618983365\n",
      "batch loss: 0.5781591534614563 | avg loss: 0.4756059486588294\n",
      "batch loss: 0.21482114493846893 | avg loss: 0.47267578232489277\n",
      "batch loss: 0.18587371706962585 | avg loss: 0.46948909271094535\n",
      "batch loss: 0.26401960849761963 | avg loss: 0.4672311862910187\n",
      "batch loss: 0.48867085576057434 | avg loss: 0.4674642261765573\n",
      "batch loss: 0.45684945583343506 | avg loss: 0.4673500888610399\n",
      "batch loss: 0.1194438561797142 | avg loss: 0.46364895872613215\n",
      "batch loss: 0.17264625430107117 | avg loss: 0.4605857723637631\n",
      "batch loss: 0.32805997133255005 | avg loss: 0.45920529526968795\n",
      "batch loss: 0.6270716786384583 | avg loss: 0.46093587654153095\n",
      "batch loss: 0.3804115951061249 | avg loss: 0.4601142002003534\n",
      "batch loss: 0.6313649415969849 | avg loss: 0.4618440056690062\n",
      "batch loss: 0.282202810049057 | avg loss: 0.4600475937128067\n",
      "batch loss: 0.3027265667915344 | avg loss: 0.45848995978289314\n",
      "batch loss: 0.32839152216911316 | avg loss: 0.45721448490432665\n",
      "batch loss: 0.3160783648490906 | avg loss: 0.4558442313115574\n",
      "batch loss: 0.17401331663131714 | avg loss: 0.4531343186704012\n",
      "batch loss: 0.37885767221450806 | avg loss: 0.4524269220374879\n",
      "batch loss: 0.46123000979423523 | avg loss: 0.4525099700351931\n",
      "batch loss: 0.5024788975715637 | avg loss: 0.45297696935796294\n",
      "batch loss: 0.2537311017513275 | avg loss: 0.451132100213457\n",
      "batch loss: 0.5571961998939514 | avg loss: 0.4521051653481405\n",
      "batch loss: 0.49753403663635254 | avg loss: 0.45251815508712423\n",
      "batch loss: 0.22461190819740295 | avg loss: 0.4504649456556853\n",
      "batch loss: 0.4189169704914093 | avg loss: 0.4501832673060043\n",
      "batch loss: 0.18979693949222565 | avg loss: 0.4478789635200416\n",
      "batch loss: 0.39166802167892456 | avg loss: 0.44738588508283883\n",
      "batch loss: 0.8490018844604492 | avg loss: 0.450878198120905\n",
      "batch loss: 0.6652471423149109 | avg loss: 0.4527262062605085\n",
      "batch loss: 0.22713521122932434 | avg loss: 0.45079807809784883\n",
      "batch loss: 0.6059946417808533 | avg loss: 0.4521133032138065\n",
      "batch loss: 0.7647250890731812 | avg loss: 0.4547402930109441\n",
      "batch loss: 0.3493719696998596 | avg loss: 0.4538622236500184\n",
      "batch loss: 1.0199902057647705 | avg loss: 0.4585409673038593\n",
      "batch loss: 0.4580140709877014 | avg loss: 0.4585366484815957\n",
      "batch loss: 0.17017762362957 | avg loss: 0.45619226616572556\n",
      "batch loss: 0.42064034938812256 | avg loss: 0.4559055571594546\n",
      "batch loss: 0.22759021818637848 | avg loss: 0.45407903444767\n",
      "batch loss: 0.5245220065116882 | avg loss: 0.4546381056545273\n",
      "batch loss: 0.21081852912902832 | avg loss: 0.45271826646928714\n",
      "batch loss: 0.3817172944545746 | avg loss: 0.4521635713754222\n",
      "batch loss: 0.7240851521492004 | avg loss: 0.4542714906062267\n",
      "batch loss: 0.2808542251586914 | avg loss: 0.4529375116412456\n",
      "batch loss: 0.35620418190956116 | avg loss: 0.4521990892768816\n",
      "batch loss: 0.516218364238739 | avg loss: 0.4526840837841684\n",
      "batch loss: 0.5979110598564148 | avg loss: 0.45377601593508754\n",
      "batch loss: 0.3532930016517639 | avg loss: 0.45302614269416724\n",
      "batch loss: 0.3727760314941406 | avg loss: 0.4524316974260189\n",
      "batch loss: 0.8037826418876648 | avg loss: 0.45501516025294275\n",
      "batch loss: 0.5655825734138489 | avg loss: 0.4558222216628764\n",
      "batch loss: 0.6470520496368408 | avg loss: 0.45720794505399204\n",
      "batch loss: 0.5311867594718933 | avg loss: 0.45774016674045176\n",
      "batch loss: 0.6454135179519653 | avg loss: 0.4590806906776769\n",
      "batch loss: 1.0059987306594849 | avg loss: 0.4629595420250656\n",
      "batch loss: 0.36849603056907654 | avg loss: 0.46229430602889665\n",
      "batch loss: 0.9962589144706726 | avg loss: 0.4660283242697482\n",
      "batch loss: 0.5519279837608337 | avg loss: 0.46662484968288076\n",
      "batch loss: 0.35579073429107666 | avg loss: 0.4658604764732821\n",
      "batch loss: 0.8258371949195862 | avg loss: 0.4683260704352431\n",
      "batch loss: 0.3059520721435547 | avg loss: 0.4672214854128507\n",
      "batch loss: 0.6427057981491089 | avg loss: 0.4684071902286362\n",
      "batch loss: 0.28483977913856506 | avg loss: 0.46717519418105186\n",
      "batch loss: 0.22658604383468628 | avg loss: 0.4655712665120761\n",
      "batch loss: 0.29150864481925964 | avg loss: 0.4644185339180839\n",
      "batch loss: 0.2658672034740448 | avg loss: 0.46311227516516257\n",
      "batch loss: 0.3554431200027466 | avg loss: 0.46240855519678076\n",
      "batch loss: 0.37391722202301025 | avg loss: 0.46183393615019785\n",
      "batch loss: 0.18037301301956177 | avg loss: 0.4600180592267744\n",
      "batch loss: 0.5152928829193115 | avg loss: 0.46037238501967526\n",
      "batch loss: 0.35255926847457886 | avg loss: 0.4596856772709804\n",
      "batch loss: 0.4948226809501648 | avg loss: 0.4599080633702157\n",
      "batch loss: 0.5960249304771423 | avg loss: 0.4607641442954165\n",
      "batch loss: 0.33684512972831726 | avg loss: 0.4599896504543722\n",
      "batch loss: 0.15911932289600372 | avg loss: 0.45812089065587297\n",
      "batch loss: 0.6284463405609131 | avg loss: 0.4591722823219535\n",
      "batch loss: 0.197152778506279 | avg loss: 0.4575648007034524\n",
      "batch loss: 0.5645719766616821 | avg loss: 0.4582172834836855\n",
      "batch loss: 0.32208767533302307 | avg loss: 0.45739225555549967\n",
      "batch loss: 0.3459184169769287 | avg loss: 0.456720726407436\n",
      "batch loss: 0.3523397445678711 | avg loss: 0.4560956905880374\n",
      "batch loss: 0.8344805836677551 | avg loss: 0.45834798161827384\n",
      "batch loss: 0.32278892397880554 | avg loss: 0.45754585701685685\n",
      "batch loss: 0.4155266284942627 | avg loss: 0.457298685084371\n",
      "batch loss: 0.6287121772766113 | avg loss: 0.45830110316736655\n",
      "batch loss: 0.4212401509284973 | avg loss: 0.458085632514815\n",
      "batch loss: 1.0601943731307983 | avg loss: 0.4615660298594161\n",
      "batch loss: 0.3937981426715851 | avg loss: 0.461176559243394\n",
      "batch loss: 0.10277107357978821 | avg loss: 0.45912852789674485\n",
      "batch loss: 0.6011946201324463 | avg loss: 0.4599357216026295\n",
      "batch loss: 0.39311838150024414 | avg loss: 0.45955822250600586\n",
      "batch loss: 0.9302809238433838 | avg loss: 0.462202732064081\n",
      "batch loss: 0.24082136154174805 | avg loss: 0.4609659646309954\n",
      "batch loss: 0.280428946018219 | avg loss: 0.4599629811942577\n",
      "batch loss: 0.9065520167350769 | avg loss: 0.4624303239320523\n",
      "batch loss: 0.5043649077415466 | avg loss: 0.4626607337332034\n",
      "batch loss: 0.07919667661190033 | avg loss: 0.46056530172707605\n",
      "batch loss: 0.1907913088798523 | avg loss: 0.45909913872247154\n",
      "batch loss: 0.19610676169395447 | avg loss: 0.4576775583061012\n",
      "batch loss: 0.39097580313682556 | avg loss: 0.4573189467191696\n",
      "batch loss: 0.23186030983924866 | avg loss: 0.45611328555938396\n",
      "batch loss: 0.6580411791801453 | avg loss: 0.4571873700999199\n",
      "batch loss: 0.632666289806366 | avg loss: 0.45811582999254663\n",
      "batch loss: 0.28533458709716797 | avg loss: 0.45720645502993934\n",
      "batch loss: 0.6795552968978882 | avg loss: 0.45837058509207523\n",
      "batch loss: 0.7391489744186401 | avg loss: 0.4598329725364844\n",
      "batch loss: 0.2066224217414856 | avg loss: 0.45852100077070723\n",
      "batch loss: 0.785414457321167 | avg loss: 0.4602060185879776\n",
      "batch loss: 0.1312345564365387 | avg loss: 0.4585189854487395\n",
      "batch loss: 0.5551058650016785 | avg loss: 0.4590117756505402\n",
      "batch loss: 0.21250389516353607 | avg loss: 0.45776046661253506\n",
      "batch loss: 0.2244502305984497 | avg loss: 0.45658213208721143\n",
      "batch loss: 0.5858184099197388 | avg loss: 0.45723156061903314\n",
      "batch loss: 0.8967451453208923 | avg loss: 0.4594291285425425\n",
      "batch loss: 0.8035194277763367 | avg loss: 0.46114102057853146\n",
      "batch loss: 0.5045150518417358 | avg loss: 0.46135574350557707\n",
      "batch loss: 0.5009172558784485 | avg loss: 0.46155062780298034\n",
      "batch loss: 0.9508500099182129 | avg loss: 0.46394915418589816\n",
      "batch loss: 0.4674249291419983 | avg loss: 0.46396610918568404\n",
      "batch loss: 0.18559153378009796 | avg loss: 0.4626147762953656\n",
      "batch loss: 0.6844770312309265 | avg loss: 0.4636865746283877\n",
      "batch loss: 0.39835551381111145 | avg loss: 0.46337248298984307\n",
      "batch loss: 0.6177089810371399 | avg loss: 0.46411093513361007\n",
      "batch loss: 0.39415445923805237 | avg loss: 0.46377780905791693\n",
      "batch loss: 0.21701882779598236 | avg loss: 0.4626083352130736\n",
      "batch loss: 0.30355194211006165 | avg loss: 0.46185806920787076\n",
      "batch loss: 0.28548410534858704 | avg loss: 0.46103002242918867\n",
      "batch loss: 0.29558151960372925 | avg loss: 0.4602568985842099\n",
      "batch loss: 0.40088871121406555 | avg loss: 0.4599807674801627\n",
      "batch loss: 0.2836984097957611 | avg loss: 0.459164645453846\n",
      "batch loss: 0.08919204026460648 | avg loss: 0.4574597025727896\n",
      "batch loss: 0.4208802580833435 | avg loss: 0.4572919069558655\n",
      "batch loss: 0.5188291072845459 | avg loss: 0.4575728987381883\n",
      "batch loss: 0.48608389496803284 | avg loss: 0.45770249417559666\n",
      "batch loss: 0.5601971745491028 | avg loss: 0.45816627100986596\n",
      "batch loss: 0.4239996373653412 | avg loss: 0.4580123672547104\n",
      "batch loss: 0.6977041959762573 | avg loss: 0.45908721850458284\n",
      "batch loss: 0.4017419219017029 | avg loss: 0.4588312127161771\n",
      "batch loss: 0.446607768535614 | avg loss: 0.45877688629759683\n",
      "batch loss: 0.15944266319274902 | avg loss: 0.4574523985847435\n",
      "batch loss: 0.2751561999320984 | avg loss: 0.45664933163032656\n",
      "batch loss: 0.36097246408462524 | avg loss: 0.4562296962463542\n",
      "batch loss: 0.1089945062994957 | avg loss: 0.4547133853732238\n",
      "batch loss: 0.43563640117645264 | avg loss: 0.45463044196367264\n",
      "batch loss: 0.8440146446228027 | avg loss: 0.4563160878626299\n",
      "batch loss: 0.32062065601348877 | avg loss: 0.4557311937598319\n",
      "batch loss: 0.35072585940361023 | avg loss: 0.45528052708877514\n",
      "batch loss: 0.23360705375671387 | avg loss: 0.45433320455316806\n",
      "batch loss: 0.34540730714797974 | avg loss: 0.4538696900961247\n",
      "batch loss: 0.10285257548093796 | avg loss: 0.4523823294409756\n",
      "batch loss: 0.45491206645965576 | avg loss: 0.45239300343683503\n",
      "batch loss: 0.32529425621032715 | avg loss: 0.4518589750871438\n",
      "batch loss: 0.31710126996040344 | avg loss: 0.45129513531673904\n",
      "batch loss: 0.2870042026042938 | avg loss: 0.4506105897637705\n",
      "batch loss: 0.15702201426029205 | avg loss: 0.4493923799069096\n",
      "batch loss: 0.17540055513381958 | avg loss: 0.4482601822838803\n",
      "batch loss: 0.27501922845840454 | avg loss: 0.447547256547973\n",
      "batch loss: 0.32050612568855286 | avg loss: 0.4470265961755983\n",
      "batch loss: 0.29899027943611145 | avg loss: 0.4464223663113555\n",
      "batch loss: 1.0645837783813477 | avg loss: 0.4489352175799327\n",
      "batch loss: 0.2552138566970825 | avg loss: 0.4481509205723098\n",
      "batch loss: 0.583774983882904 | avg loss: 0.4486977917953364\n",
      "batch loss: 0.20921379327774048 | avg loss: 0.44773600866875973\n",
      "batch loss: 0.7464191317558289 | avg loss: 0.44893074116110804\n",
      "batch loss: 0.13842587172985077 | avg loss: 0.4476936699681548\n",
      "batch loss: 0.163593590259552 | avg loss: 0.44656628869946985\n",
      "batch loss: 0.8400648832321167 | avg loss: 0.44812161911264237\n",
      "batch loss: 1.1044937372207642 | avg loss: 0.45070576130991846\n",
      "batch loss: 0.8295565843582153 | avg loss: 0.45219145081206863\n",
      "batch loss: 0.5723605751991272 | avg loss: 0.4526608614542056\n",
      "batch loss: 0.9040921926498413 | avg loss: 0.4544174035989357\n",
      "batch loss: 0.3448176980018616 | avg loss: 0.4539925985384819\n",
      "batch loss: 0.36661452054977417 | avg loss: 0.45365523144200043\n",
      "batch loss: 0.24339576065540314 | avg loss: 0.45284654116974427\n",
      "batch loss: 0.28441643714904785 | avg loss: 0.45220121510069944\n",
      "batch loss: 0.9113907217979431 | avg loss: 0.45395384680565076\n",
      "batch loss: 0.6182769536972046 | avg loss: 0.45457864949345134\n",
      "batch loss: 0.9431771636009216 | avg loss: 0.4564294014408281\n",
      "batch loss: 0.3884279727935791 | avg loss: 0.4561727922761215\n",
      "batch loss: 0.5343208312988281 | avg loss: 0.45646658189650763\n",
      "batch loss: 0.2644365727901459 | avg loss: 0.45574736837925534\n",
      "batch loss: 0.23353199660778046 | avg loss: 0.45491820654428716\n",
      "batch loss: 0.18609420955181122 | avg loss: 0.4539188608305605\n",
      "batch loss: 0.5235996246337891 | avg loss: 0.4541769377335354\n",
      "batch loss: 0.17880935966968536 | avg loss: 0.45316082120931456\n",
      "batch loss: 0.794672966003418 | avg loss: 0.45441638056517525\n",
      "batch loss: 0.5003613233566284 | avg loss: 0.4545846770589168\n",
      "batch loss: 0.5733320116996765 | avg loss: 0.4550180614919123\n",
      "batch loss: 0.1926114708185196 | avg loss: 0.4540638557076454\n",
      "batch loss: 0.7730911374092102 | avg loss: 0.4552197516558395\n",
      "batch loss: 0.2715957760810852 | avg loss: 0.45455684921694145\n",
      "batch loss: 0.2955259382724762 | avg loss: 0.4539847955804506\n",
      "batch loss: 0.6791843175888062 | avg loss: 0.4547919623260002\n",
      "batch loss: 0.2972767949104309 | avg loss: 0.4542294081566589\n",
      "batch loss: 0.5927062034606934 | avg loss: 0.4547222081399473\n",
      "batch loss: 0.4513446092605591 | avg loss: 0.45471023083895656\n",
      "batch loss: 0.34446632862091064 | avg loss: 0.4543206764141578\n",
      "batch loss: 0.15117309987545013 | avg loss: 0.45325325537000744\n",
      "batch loss: 0.3970133662223816 | avg loss: 0.4530559224256298\n",
      "batch loss: 0.39346033334732056 | avg loss: 0.4528475462400413\n",
      "batch loss: 0.8101048469543457 | avg loss: 0.454092345197234\n",
      "batch loss: 0.5890234708786011 | avg loss: 0.4545608560502943\n",
      "batch loss: 0.4191124439239502 | avg loss: 0.4544381971848052\n",
      "batch loss: 0.5654681921005249 | avg loss: 0.4548210592362387\n",
      "batch loss: 0.3440673053264618 | avg loss: 0.45444046214376527\n",
      "batch loss: 0.3813585042953491 | avg loss: 0.4541901814662022\n",
      "batch loss: 0.13339851796627045 | avg loss: 0.4530953293723458\n",
      "batch loss: 0.2536780834197998 | avg loss: 0.45241703942012623\n",
      "batch loss: 0.39838698506355286 | avg loss: 0.4522338866934938\n",
      "batch loss: 0.5623282194137573 | avg loss: 0.4526058270067379\n",
      "batch loss: 0.8035566806793213 | avg loss: 0.4537874797127062\n",
      "batch loss: 0.4303985834121704 | avg loss: 0.4537089934835098\n",
      "batch loss: 0.4437922239303589 | avg loss: 0.4536758270301548\n",
      "batch loss: 0.4397265911102295 | avg loss: 0.45362932957708835\n",
      "batch loss: 0.365601122379303 | avg loss: 0.45333687706148107\n",
      "batch loss: 0.5246776342391968 | avg loss: 0.453573104734255\n",
      "batch loss: 0.45375141501426697 | avg loss: 0.4535736932170273\n",
      "batch loss: 0.4487932324409485 | avg loss: 0.453557968017106\n",
      "batch loss: 0.24508357048034668 | avg loss: 0.4528744454022314\n",
      "batch loss: 0.2619040012359619 | avg loss: 0.45225035898338733\n",
      "batch loss: 0.36667773127555847 | avg loss: 0.4519716207823847\n",
      "batch loss: 0.2659146785736084 | avg loss: 0.4513675398011873\n",
      "batch loss: 0.13552561402320862 | avg loss: 0.45034539764656606\n",
      "batch loss: 0.38871926069259644 | avg loss: 0.4501466036563919\n",
      "batch loss: 0.04955928400158882 | avg loss: 0.4488585415353154\n",
      "batch loss: 0.4527439773082733 | avg loss: 0.4488709948551005\n",
      "batch loss: 0.31151941418647766 | avg loss: 0.44843217191366724\n",
      "batch loss: 0.17119547724723816 | avg loss: 0.44754925250390154\n",
      "batch loss: 0.2902405560016632 | avg loss: 0.44704985981659284\n",
      "batch loss: 0.27046945691108704 | avg loss: 0.446491061073221\n",
      "batch loss: 0.28579840064048767 | avg loss: 0.44598414416333854\n",
      "batch loss: 0.16310828924179077 | avg loss: 0.4450945974497488\n",
      "batch loss: 0.20297648012638092 | avg loss: 0.4443356064863526\n",
      "batch loss: 0.5135087966918945 | avg loss: 0.444551772705745\n",
      "batch loss: 0.10091054439544678 | avg loss: 0.44348123928421757\n",
      "batch loss: 0.24172647297382355 | avg loss: 0.4428546716869803\n",
      "batch loss: 0.21298515796661377 | avg loss: 0.4421430013658646\n",
      "batch loss: 0.39129698276519775 | avg loss: 0.4419860692096897\n",
      "batch loss: 0.2485184520483017 | avg loss: 0.4413907842338085\n",
      "batch loss: 0.3479474186897278 | avg loss: 0.4411041481431825\n",
      "batch loss: 0.11860920488834381 | avg loss: 0.44011792507512487\n",
      "batch loss: 0.401032954454422 | avg loss: 0.43999876357933004\n",
      "batch loss: 0.9926819801330566 | avg loss: 0.441678651775542\n",
      "batch loss: 0.6719918847084045 | avg loss: 0.44237657066321734\n",
      "batch loss: 0.6709687113761902 | avg loss: 0.4430671813602354\n",
      "batch loss: 0.3446264863014221 | avg loss: 0.44277067324258834\n",
      "batch loss: 0.6090208888053894 | avg loss: 0.44326992313917335\n",
      "batch loss: 0.47640085220336914 | avg loss: 0.44336911753756914\n",
      "batch loss: 0.5013757944107056 | avg loss: 0.44354227179689193\n",
      "batch loss: 0.41912734508514404 | avg loss: 0.4434696083245355\n",
      "batch loss: 0.3274293541908264 | avg loss: 0.4431252752262159\n",
      "batch loss: 0.696907639503479 | avg loss: 0.4438761106234859\n",
      "batch loss: 0.5237618684768677 | avg loss: 0.4441117618265933\n",
      "batch loss: 0.8791399002075195 | avg loss: 0.44539125635124305\n",
      "batch loss: 0.7596368193626404 | avg loss: 0.4463127975917457\n",
      "batch loss: 0.5303663611412048 | avg loss: 0.4465585682453991\n",
      "batch loss: 0.9393693804740906 | avg loss: 0.4479953344618092\n",
      "batch loss: 0.25448524951934814 | avg loss: 0.447432805145116\n",
      "batch loss: 0.4596727192401886 | avg loss: 0.44746828315698584\n",
      "batch loss: 0.37428706884384155 | avg loss: 0.44725677675723685\n",
      "batch loss: 0.43901821970939636 | avg loss: 0.4472330345179059\n",
      "batch loss: 0.7998765707015991 | avg loss: 0.44824637801268663\n",
      "batch loss: 0.49250274896621704 | avg loss: 0.44837318709851337\n",
      "batch loss: 0.27999329566955566 | avg loss: 0.4478921016944306\n",
      "batch loss: 0.36790114641189575 | avg loss: 0.4476642072349362\n",
      "batch loss: 0.5778089165687561 | avg loss: 0.44803393652281637\n",
      "batch loss: 0.4629969894886017 | avg loss: 0.4480763247748441\n",
      "batch loss: 0.3443344831466675 | avg loss: 0.447783268725047\n",
      "batch loss: 0.6139844655990601 | avg loss: 0.4482514411106076\n",
      "batch loss: 0.24664193391799927 | avg loss: 0.4476851222701789\n",
      "batch loss: 0.46375566720962524 | avg loss: 0.4477301378022222\n",
      "batch loss: 0.3909435570240021 | avg loss: 0.44757151606820483\n",
      "batch loss: 0.5549824237823486 | avg loss: 0.4478707107972136\n",
      "batch loss: 0.3772714138031006 | avg loss: 0.4476746016388966\n",
      "batch loss: 0.25255683064460754 | avg loss: 0.4471341091984692\n",
      "batch loss: 0.15720246732234955 | avg loss: 0.44633319306068986\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epoch took: 0:30:51\n",
      "\n",
      "======= Validation =======\n",
      "  Accuracy: 0.55\n",
      "  Validation took: 0:01:51\n",
      "\n",
      "Training complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.65      0.62       566\n",
      "           2       0.44      0.45      0.44       463\n",
      "           3       0.62      0.54      0.58       418\n",
      "\n",
      "    accuracy                           0.55      1447\n",
      "   macro avg       0.55      0.55      0.55      1447\n",
      "weighted avg       0.56      0.55      0.55      1447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Definimos la funcion para entrenar el modelo y entregar los resultados en el set de validación\n",
    "#Train model\n",
    "def training(n_epochs, training_dataloader, validation_dataloader):\n",
    "    for epoch_i in range(0, n_epochs):\n",
    "        print('======= Epoch {:} / {:} ======='.format(epoch_i + 1, epochs))\n",
    "        # Mira cuanto tiempo le cuesta entrenar un EPOCH.\n",
    "        t0 = time.time()\n",
    "        # Resetea la perdida para este EPOCH.\n",
    "        total_loss = 0\n",
    "        # Pone el modelo en modo entrenamiento.\n",
    "        model.train()\n",
    "        # Para cada batch en el training data\n",
    "        for step, batch in enumerate(training_dataloader):\n",
    "            batch_loss = 0\n",
    "            # Unpack this training batch from dataloader\n",
    "            #   [0]: input ids, [1]: attention masks, \n",
    "            #   [2]: labels\n",
    "            b_input_ids,b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "            # Limpia el gradiente calculado anteriormente\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Genera un paso adelante\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            # Saca el loss value fuera del output\n",
    "            loss = outputs[0]\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Genera un paso atras\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipea el los gradientes a 1.0.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                            1.0)\n",
    "\n",
    "            # Actualiza los parametros\n",
    "            # ¿take a step using the computed gradient?\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calcula el average loss sobre el training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        #Validación\n",
    "        # Despues de completar un entrenamiento genera un paso de validacion\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Pone el modelo en modo evaluación\n",
    "        model.eval()\n",
    "\n",
    "        # Trackea las variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        # Evalua el data para un epoch mas\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Add batch to device\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #   [0]: input ids, [1]: attention masks,\n",
    "            #   [2]: labels\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(\n",
    "                                t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "            # El modelo no computa los gradientes\n",
    "            with torch.no_grad():\n",
    "                # Paso adelante \n",
    "                # Devolvemos los loggits \n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # Los \"logits\" son el valor de salida\n",
    "            # Prioriza aplicar la funcion de activación\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Mueve los logits y labels a la CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Guarda los logits y labels del batch\n",
    "            # Utilizamos esto en la matriz de confusión\n",
    "            predict_labels = np.argmax(logits, axis=1).flatten()\n",
    "            all_logits.extend(predict_labels.tolist())\n",
    "            all_labels.extend(b_labels.tolist())\n",
    "\n",
    "            # Calcula la precision para este batch\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "            # Accumula la precisión total\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #Print la matriz de confussión\"\n",
    "    conf = confusion_matrix(all_labels, all_logits, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    print(classification_report(all_labels, all_logits, target_names=target_names))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Llamamos a la funcion para entrenar el modelo\n",
    "training(epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
